[
  {
    "job_id": null,
    "job_title": "Data Engineer - Consultant (Remote)",
    "employer_name": "Releady",
    "employer_logo": null,
    "employer_website": "https://www.releady.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/consultant-data-engineer_93b50b06-e8ad-4459-ac66-68a2c9229a93?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Pangian",
        "apply_link": "https://pangian.com/remote/job/data-engineer-consultant-remote-1o?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EarnBetter",
        "apply_link": "https://earnbetter.com/app/job/01JM148F98JPMEKGYAMF1HY8HZ/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "OVERVIEW\n\nThis Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team. You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies. Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure.\n• Duration: 6+ months contract\n• Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada. Working hours will be PST. Preference for California.\n• Rate: $70/hr - $85/hr DOE\n• **Must be able to work in the United States without sponsorship***\n\nRESPONSIBILITIES\n• Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)\n• Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability\n• Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling\n• Work with Collibra for data governance, quality assurance, and metadata management\n• Leverage Refuel.ai for data mastering and Striim for data validation processes\n• Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows\n• Collaborate with client stakeholders to translate requirements into effective data solutions\n• Support data visualization and reporting needs through Tableau\n• Implement CI/CD practices using Git repositories and modern DevOps tools\n• Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners\n\nQUALIFICATIONS\n• Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)\n• 10+ years of experience in data engineering or related roles\n• Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server\n• Hands-on experience with dbt Cloud for data transformation and pipeline development\n• Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse\n• Knowledge of Data Vault modeling principles and implementation techniques\n• Experience with data governance and data quality tools, particularly Collibra\n• Familiarity with data visualization platforms, especially Tableau\n• Understanding of version control systems (Git, Bitbucket) and CI/CD practices\n• Experience with scheduling systems like Tidal or Control-M\n• Working knowledge of Agile methodologies and DevOps principles applied to data pipelines\n• Preferred Skills:\n• Experience with data observability platforms and data quality monitoring\n• Knowledge of Python, R, KNIME, or Alteryx for data science applications\n• Experience with Refuel.ai and Striim technologies\n• Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms\n• Experience with enterprise scheduling tools like Tidal\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or other non-merit factor. We are committed to creating a diverse and inclusive environment for all employees.",
    "job_is_remote": null,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Sacramento, CA",
    "job_city": "Sacramento",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 38.5781342,
    "job_longitude": -121.4944209,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D00eCRolQcTBw_16xAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 70,
    "job_max_salary": 85,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "**Must be able to work in the United States without sponsorship***",
        "Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)",
        "10+ years of experience in data engineering or related roles",
        "Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server",
        "Hands-on experience with dbt Cloud for data transformation and pipeline development",
        "Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse",
        "Knowledge of Data Vault modeling principles and implementation techniques",
        "Experience with data governance and data quality tools, particularly Collibra",
        "Familiarity with data visualization platforms, especially Tableau",
        "Understanding of version control systems (Git, Bitbucket) and CI/CD practices",
        "Experience with scheduling systems like Tidal or Control-M",
        "Working knowledge of Agile methodologies and DevOps principles applied to data pipelines",
        "Experience with data observability platforms and data quality monitoring",
        "Knowledge of Python, R, KNIME, or Alteryx for data science applications",
        "Experience with Refuel.ai and Striim technologies",
        "Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms",
        "Experience with enterprise scheduling tools like Tidal"
      ],
      "Benefits": [
        "Rate: $70/hr - $85/hr DOE"
      ],
      "Responsibilities": [
        "This Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team",
        "You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies",
        "Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure",
        "Duration: 6+ months contract",
        "Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada",
        "Working hours will be PST. Preference for California",
        "Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)",
        "Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability",
        "Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling",
        "Work with Collibra for data governance, quality assurance, and metadata management",
        "Leverage Refuel.ai for data mastering and Striim for data validation processes",
        "Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows",
        "Collaborate with client stakeholders to translate requirements into effective data solutions",
        "Support data visualization and reporting needs through Tableau",
        "Implement CI/CD practices using Git repositories and modern DevOps tools",
        "Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "URGENT by 6/25:REMOTE:Certified Data Engineer-Databricks, PySpark/Scala, ADFactory, W2 Only",
    "employer_name": "Solitsys",
    "employer_logo": null,
    "employer_website": "http://www.solitsys.com",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time and Contractor",
    "job_employment_types": [
      "FULLTIME",
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "URGENTLY NEEDED: REMOTE DATA ENGINEER- DATA BRICKS , PySpark/Scala, AZURE DATA FACTORY [W2 ONLY, ALL Corp-to-Corp WILL BE REJECTED]\n\nWe cannot offer Corp-to-Corp arrangement. This position is being offered on W2 basis only (no Corp-to-Corp or 1099), we are NOT a head-hunting agency. Please respond ASAP with your detailed resume in Word format. Resume must address the minimum qualifications listed below.\n\nDATA ENGINEER - DATABRICKS SPECIALIST\n\nAre you a skilled Data Engineer with a passion for modernizing data solutions? We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity. This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity. Time-sensitive, project-based with defined deliverables. Apply now!\n\nYOUR RESUME MUST SHOW THESE MINIMUM QUALIFICATIONS:\n• 3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling.\n• 5+ years background: PySpark or Scala for data engineering tasks.\n• 5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning.\n• 5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF).\n• 3+ years of: Delta Lake architecture and implementation for data warehousing.\n• 3+ years of experience with: BI tools such as Power BI or Tableau for reporting.\n• Familiarity with: Azure Cloud environment (Blob Storage, ADLS).\n• Proficiency in: Python scripting for data manipulation and automation.\n\nEducation & Certifications:\n• Bachelor’s degree in Computer Science, Information Systems, Data Science, or related field.\n• Databricks Certified Data Engineer Associate or relevant industry certifications.\n• Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields.\n\nJob Types: Full-time, Contract\n\nPay: $60,000.00 per year\n\nCompensation Package:\n• Hourly pay\n\nSchedule:\n• 8 hour shift\n\nEducation:\n• Bachelor's (Required)\n\nExperience:\n• Databricks: 3 years (Required)\n• Python: 5 years (Required)\n• PySpark: 5 years (Required)\n• Scala: 5 years (Required)\n• Azure Data Lake: 3 years (Required)\n• Azure Data Factory: 3 years (Required)\n• Power BI: 5 years (Preferred)\n• Tableau: 5 years (Preferred)\n\nWork Location: Remote",
    "job_is_remote": null,
    "job_posted_at": "27 days ago",
    "job_posted_at_timestamp": 1750809600,
    "job_posted_at_datetime_utc": "2025-06-25T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuINRzsWvsKKLdrOtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Resume must address the minimum qualifications listed below",
        "3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling",
        "5+ years background: PySpark or Scala for data engineering tasks",
        "5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning",
        "5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF)",
        "3+ years of: Delta Lake architecture and implementation for data warehousing",
        "3+ years of experience with: BI tools such as Power BI or Tableau for reporting",
        "Familiarity with: Azure Cloud environment (Blob Storage, ADLS)",
        "Proficiency in: Python scripting for data manipulation and automation",
        "Bachelor’s degree in Computer Science, Information Systems, Data Science, or related field",
        "Databricks Certified Data Engineer Associate or relevant industry certifications",
        "Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields",
        "Bachelor's (Required)",
        "Databricks: 3 years (Required)",
        "Python: 5 years (Required)",
        "PySpark: 5 years (Required)",
        "Scala: 5 years (Required)",
        "Azure Data Lake: 3 years (Required)",
        "Azure Data Factory: 3 years (Required)"
      ],
      "Benefits": [
        "Pay: $60,000.00 per year",
        "Compensation Package:",
        "Hourly pay",
        "8 hour shift"
      ],
      "Responsibilities": [
        "We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity",
        "This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity",
        "Time-sensitive, project-based with defined deliverables"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Google Cloud Platform Data Engineer(Fulltime) for Remote",
    "employer_name": "Amaze Systems Inc",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ3j6-Rx9U3c7Gzc5qfkYkFeEVFSNq9iCBifDLC&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs™ - DentaQuest",
        "apply_link": "https://dev-cm.dentaquest.com/job/work-from-home-system-data-analyst-google-cloud-platform-data-fn8ov.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://jobxpedia.com/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://buzzcloud.in/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Fulltime opportunity\n\nRole: Google Cloud Platform Data Engineer\n\nLocation: Remote - USA\n\nDuration: FTE only\n\nJob Description:\n• 10+ years' proven experience as a Data Engineer with a focus on Google Cloud Platform services.\n• Strong proficiency in Google Cloud Platform services such as GCS, Dataflow with Apache Beam (Batch & Stream data processing), BigQuery, cloud Composer and Pub/Sub.\n• Proficiency in SQL and Python for data manipulation and analysis is mandatory.\n• Solid understanding of data warehousing concepts and ETL processes.\n\nThanks &Regards\n\nRahul Sharma | Lead Technical Recruiter\nAmaze Systems Inc\n\nE: |",
    "job_is_remote": null,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1752883200,
    "job_posted_at_datetime_utc": "2025-07-19T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVSDv3_uV9S8qF8byAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Senior/Lead Data Engineer , Remote, $63/HR",
    "employer_name": "FASTRA LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8iFPXl68j7QGKA_ta-TCHN1e0yurtlbKR__Xc&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Quality Contracts",
        "apply_link": "https://qualitycontracts.co.uk/contract-jobs/remote-senior-lead-data-engineer-remote-63-hr-in-usa-1752741389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior/Lead Data Engineer\n\nLocation: Remote\n\nDuration: 12 Months\n\nResponsibilities:\n• Collaborate with cross-functional stakeholders (including data scientists, analysts, and business teams) to define data requirements, processing needs, and analytics solutions.\n• Create and sustain scalable data models and efficient extraction processes.\n• Establish and monitor data quality checks and validation systems.\n• Construct and enhance business intelligence dashboards.\n• Produce comprehensive documentation for data models and processes.\n• Adapt to shifting priorities and ad-hoc requests.\n• Drive continuous improvement in data practices and visualization techniques.\n\nRequirements:\n• 5+ years of proven experience in database engineering and software development.\n• Advanced skills in SQL for complex query development and database management.\n• Strong Python programming skills for automation and data processing workflows.\n• Ability to handle large-scale data processing tasks using Spark.\n• Knowledge of data visualization tools, Qlik is preferred.\n• Familiarity with cloud platforms, particularly AWS (Glue and Athena desired).\n• Growth mindset with enthusiasm to learn new technologies.",
    "job_is_remote": null,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPrsQLoxA4Q6wEGlDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Growth Insights and Foundations",
    "employer_name": "Netflix",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQG6PlRytpbamOkfuuSsdXOdyiJlb7J7TCmmimh&s=0",
    "employer_website": "https://www.netflix.com/",
    "job_publisher": "Remote Rocketship",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Remote Rocketship",
        "apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description:\n• Partner closely with data scientists and other engineers to build low-latency data products\n• Ensure the availability of critical data to enhance \"in-the-moment\" experiences\n• Develop highly available and reliable distributed data systems and services\n• Optimize for the best customer experience with data insights\n• Ensure timely delivery of high-quality data for Netflix product\n\nRequirements:\n• Proficient in at least one major language preferably on the JVM stack (e.g., Java, Scala) and SQL (any variant)\n• Strive to write elegant and maintainable code\n• Comfortable with picking up new technologies\n• Have a product mindset and are curious to understand the business's needs\n• Naturally collaborative style to work with product management, data science, engineering, etc.\n• Strong data intuition and know how to apply analytical skills to support building high quality data products\n• Experience building applications that use large-scale distributed systems and data processing frameworks (batch and real-time)\n• Passionate about making data available for self-service and wider integration\n• Knowledge about transport protocols and building APIs/services and frameworks (e.g. Spring, gRPC)\n• Experience in supporting and maintaining products that run 24x7\n• Can craft scalable systems and solutions to realize a range of product and engineering goals\n• Strong operational awareness and design multi-tenant systems handling high-scale demands\n• Prioritize observability in designs with comprehensive monitoring, logging, and alerting\n• Own what you build and have a passion for quality\n• Comfortable working in agile environments with vague requirements\n• Nimble and can pivot easily when needed\n• Unafraid to take smart risks\n\nBenefits:\n• Health Plans\n• Mental Health support\n• 401(k) Retirement Plan with employer match\n• Stock Option Program\n• Disability Programs\n• Health Savings and Flexible Spending Accounts\n• Family-forming benefits\n• Life and Serious Injury Benefits\n• Paid leave of absence programs\n• Paid time off",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6UkieosGdj3DK2hqAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 170000,
    "job_max_salary": 720000,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Netflix",
        "score": 4,
        "num_stars": 4,
        "review_count": 820,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Netflix/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Netflix",
        "score": 4.2,
        "num_stars": 4,
        "review_count": 3404,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Netflix-Reviews-E11891.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Netflix",
        "score": 4,
        "num_stars": 4,
        "review_count": 43,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/netflix-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer (Full-Time, Remote, North Carolina Based)",
    "employer_name": "Alliance Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQc8LYAEYcIakIeMQVnhkBhH5cN4eBzxq9dkqiN&s=0",
    "employer_website": "https://www.alliancehealthplan.org",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Alliance/Job/Data-Engineer-(Full-Time,-Remote,-North-Carolina-Based)/-in-Morrisville,NC?jid=63c088f9feb31195&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-full-time-remote-north-carolina-based-alliance-health-JV_KO0,51_KE52,67.htm?jl=1009695680692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-full-time-remote-north-carolina-based-at-alliance-health-4256300763?refId=ZZ8HCMbeJKzEURTwY97AHQ%3D%3D&trackingId=Ty0o5ZD5tUD13LeFLBMwKA%3D%3D&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/685c8729de1ebd49c8be30eb-data-engineer-full-time-remote-north-carolina-based?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5274727037?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://vividhireio.com/job/606346?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kinetichires",
        "apply_link": "https://kinetichires.net/job/data-analyst-diversity-equity-inclusion-and-health-equity-full-time-remote-north-carolina-based-264200?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages. The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities.\n\nThis position is fulltime remote. Selected candidate must reside in North Carolina. Some travel for onsite meetings to the Home office at Morrisville may be required.\n\nResponsibilities & Duties\n• Analyze business and technical requirements for the design of data integration solutions\n• Define the overall data integration and dataflow architectures to support data integration projects\n• Design and develop SQL and SSIS processes to support data integration projects\n• Design and develop APIs to consume and distribute healthcare data\n• Design, develop and execute unit testing plans\n• Ensure data quality and integrity in all data integration projects\n• Develop technical and business process documentation for data integration projects\n• Maintain and continually improve data integration projects\n• Assist in establishing standards for the design, development, implementation and support of data integration projects\n• Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders\n• Any other tasks as reasonably required\n\nMinimum Requirements\n\nEducation & Experience:\n• Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position.\n• Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis.\n\nPreferred\n• Bachelor’s degree plus five (5) years of experience in a computer science related field including experience in a data integration or ETL development position including developing complex data integration software applications.\n• Microsoft Certified Solutions Expert, MuleSoft Certified Developer and/or HL7 Certifications.\n\nKnowledge, Skills, & Abilities\n• Expert programming in SQL\n• Proficient designing and developing ETL processes, preferably using SSIS\n• Proficient designing and developing APIs, preferably using .NET Framework\n• Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI\n• Experience working with API management and data integration platforms such as Apigee or MuleSoft\n• Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP\n• Experience working with HIEs and/or HISPs\n• Strong communication and organizational skills\n• Ability to access and analyze large data sets for completeness and quality\n• Ability to work independently and in a team setting\n\nEmployment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date.\n\nSalary Range\n\n$102,424-$130,591/Annually\n\nExact compensation will be determined based on the candidate's education, experience, external market data and consideration of internal equity.\n\nAn excellent fringe benefit package accompanies the salary, which includes:\n• Medical, Dental, Vision, Life, Long Term Disability\n• Generous retirement savings plan\n• Flexible work schedules including hybrid/remote options\n• Paid time off including vacation, sick leave, holiday, management leave\n• Dress flexibility\n\nEqual Opportunity Employer\nThis employer is required to notify all applicants of their rights pursuant to federal employment laws. For further information, please review the Know Your Rights notice from the Department of Labor.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "North Carolina",
    "job_city": null,
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.7595731,
    "job_longitude": -79.01929969999999,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dj0uGbWgrvId1fahdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 102424,
    "job_max_salary": 130591,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Selected candidate must reside in North Carolina",
        "Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position",
        "Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis",
        "Knowledge, Skills, & Abilities",
        "Expert programming in SQL",
        "Proficient designing and developing ETL processes, preferably using SSIS",
        "Proficient designing and developing APIs, preferably using .NET Framework",
        "Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI",
        "Experience working with API management and data integration platforms such as Apigee or MuleSoft",
        "Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP",
        "Experience working with HIEs and/or HISPs",
        "Strong communication and organizational skills",
        "Ability to access and analyze large data sets for completeness and quality",
        "Ability to work independently and in a team setting",
        "Employment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date"
      ],
      "Benefits": [
        "$102,424-$130,591/Annually",
        "An excellent fringe benefit package accompanies the salary, which includes:",
        "Medical, Dental, Vision, Life, Long Term Disability",
        "Generous retirement savings plan",
        "Flexible work schedules including hybrid/remote options",
        "Paid time off including vacation, sick leave, holiday, management leave",
        "Dress flexibility"
      ],
      "Responsibilities": [
        "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages",
        "The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities",
        "This position is fulltime remote",
        "Analyze business and technical requirements for the design of data integration solutions",
        "Define the overall data integration and dataflow architectures to support data integration projects",
        "Design and develop SQL and SSIS processes to support data integration projects",
        "Design and develop APIs to consume and distribute healthcare data",
        "Design, develop and execute unit testing plans",
        "Ensure data quality and integrity in all data integration projects",
        "Develop technical and business process documentation for data integration projects",
        "Maintain and continually improve data integration projects",
        "Assist in establishing standards for the design, development, implementation and support of data integration projects",
        "Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders",
        "Any other tasks as reasonably required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Junior Data Engineer, Entry Level, (Remote)",
    "employer_name": "Jobright.ai",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTpvmPhBSJqzn6uW7nKlM9EiPB60FRouKbgC-AC&s=0",
    "employer_website": "https://jobright.ai",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/687c8ddbfd9af3ce40360fa6-junior-data-engineer-entry-level-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/687c8ddbfd9af3ce40360fa6-junior-data-engineer-entry-level-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://prowlremote.com/job/131094?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Flexremote003.Kesug.com",
        "apply_link": "https://flexremote003.kesug.com/job/parttime-remote-specialist-data-entry-junior-remote/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: United States\n\nJob Summary:\n\nSegmed, Inc. is a fast-growing startup focused on revolutionizing healthcare research through a medical imaging data platform. They are seeking a Junior Data Engineer to streamline and scale data operations, ensuring data integrity and compliance while collaborating with various teams.\n\nResponsibilities:\n\n• Operating and supporting internal data workflows using a range of tools and platforms\n\n• Proactively monitoring and improving data operation pipelines as well as debugging and resolving data related issues\n\n• Collaborating with cross-functional teams — clinical, engineering, product, and commercial — to support project delivery\n\n• Performing quality checks and supporting de-identification efforts to ensure patient privacy\n\n• Following and reinforcing Segmed's compliance policies and data security protocols\n\n• Managing multiple projects and shifting priorities in a dynamic environment\n\nQualifications:\n\nRequired:\n\n• 1–4 years of experience in a data engineering, technical operations, or software support role\n\n• Proficiency in Python, SQL and familiarity with bash, Git, file formats, and data wrangling\n\n• Comfort working with healthcare data types (or interest in learning about DICOM, ECG, video, and pathology data)\n\n• Detail-oriented mindset, especially when working with sensitive or regulated data\n\n• A self-starter who takes ownership and thrives in a collaborative startup environment. A proactive, ownership-driven attitude — you like to take initiative and figure things out\n\n• Comfortable working in a fast-paced, startup-style environment with evolving processes\n\n• Strong communication and collaboration skills across technical and non-technical teams\n\nPreferred:\n\n• Experience with healthcare, imaging, or clinical data\n\n• Familiarity with privacy regulations (e.g., HIPAA) or de-identification practices\n\n• Exposure to cloud platforms like Google Cloud or AWS\n\n• Prior experience in a startup or fast-paced operational role\n\nCompany:\n\nSegmed provides at-scale access to novel clinical data globally, offering millions of diagnostic-grade medical images to accelerate innovation and mature the evidence needs of life sciences and technology industry. Founded in 2019, the company is headquartered in Palo Alto, California, USA, with a team of 11-50 employees. The company is currently Early Stage. Segmed, Inc. has a track record of offering H1B sponsorships.\n\nThis offer from \"Jobright.ai\" has been enriched by Jobgether.com and got a 72% flex score.",
    "job_is_remote": null,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DNEO12lrRiYzpcopxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "1–4 years of experience in a data engineering, technical operations, or software support role",
        "Proficiency in Python, SQL and familiarity with bash, Git, file formats, and data wrangling",
        "Comfort working with healthcare data types (or interest in learning about DICOM, ECG, video, and pathology data)",
        "Detail-oriented mindset, especially when working with sensitive or regulated data",
        "A self-starter who takes ownership and thrives in a collaborative startup environment",
        "A proactive, ownership-driven attitude — you like to take initiative and figure things out",
        "Comfortable working in a fast-paced, startup-style environment with evolving processes",
        "Strong communication and collaboration skills across technical and non-technical teams"
      ],
      "Responsibilities": [
        "They are seeking a Junior Data Engineer to streamline and scale data operations, ensuring data integrity and compliance while collaborating with various teams",
        "Operating and supporting internal data workflows using a range of tools and platforms",
        "Proactively monitoring and improving data operation pipelines as well as debugging and resolving data related issues",
        "Collaborating with cross-functional teams — clinical, engineering, product, and commercial — to support project delivery",
        "Performing quality checks and supporting de-identification efforts to ensure patient privacy",
        "Following and reinforcing Segmed's compliance policies and data security protocols",
        "Managing multiple projects and shifting priorities in a dynamic environment"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "W2 Contract || Title- Azure Data Engineer in 100% Remote || USC & GC only on W2 || 12 Years Exp.",
    "employer_name": "JS Consulting",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRE1gF8gajTg6maWTnVUROZLV3HcGESVQuhd_J8&s=0",
    "employer_website": "https://www.jsconsultingco.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Note: Candidate must be Work on W2 and USC & GC Visa only.\n\nTitle: Azure Data Engineer (12+ Years Experinece)\n\nLocation: Kansas City, MO 100% Remote\n\nDuration: 6-12+ Months (W2 Contract)\n\nInterview: Video\n\nVisa: USC/GC (must work on our w2) (need strong communication)\n\nJob Description\n\nMake sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager.\n\nDon't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes.\n\nCommunication must be flawless\n\nResume should not be more than 6 pages\n\nMust have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020.\n\nJob Description-\n\nMust have strong Azure, ADF and Databricks experience.\n\nThe purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks. It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes. This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files.\n\nEssential Duties And Responsibilities\n• Work with a highly dynamic team focused on Digital Transformation.\n• Understand the domain and business processes to implement successful data pipelines.\n• Provide work status and coordinate with Data Engineers.\n• Manage customer deliverables and regularly report the status via Weekly/Monthly reviews.\n• Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views\n• Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server.\n• Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot.\n• Design normalized database tables with proper indexing and constraints.\n• Perform SQL query tuning and performance optimization on complex and inefficient queries.\n• Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets.\n• Collaborate with DBA on database design and performance enhancements.\n• Leading in all phases of the software development life cycle in a team environment.\n• Debug existing code and troubleshoot for issues.\n• Design and provide a framework for maintaining existing data warehouse for reporting and data analytics.\n• Follow best practices, design, develop, test and document ETL processes.",
    "job_is_remote": null,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Kansas City, MO",
    "job_city": "Kansas City",
    "job_state": "Missouri",
    "job_country": "US",
    "job_latitude": 39.099726499999996,
    "job_longitude": -94.5785667,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3KUvi58ta8OiVWqPAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Note: Candidate must be Work on W2 and USC & GC Visa only",
        "Title: Azure Data Engineer (12+ Years Experinece)",
        "Visa: USC/GC (must work on our w2) (need strong communication)",
        "Communication must be flawless",
        "Resume should not be more than 6 pages",
        "Must have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020",
        "Must have strong Azure, ADF and Databricks experience",
        "Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server"
      ],
      "Responsibilities": [
        "Make sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager",
        "Don't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes",
        "The purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks",
        "It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes",
        "This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files",
        "Work with a highly dynamic team focused on Digital Transformation",
        "Understand the domain and business processes to implement successful data pipelines",
        "Provide work status and coordinate with Data Engineers",
        "Manage customer deliverables and regularly report the status via Weekly/Monthly reviews",
        "Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views",
        "Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot",
        "Design normalized database tables with proper indexing and constraints",
        "Perform SQL query tuning and performance optimization on complex and inefficient queries",
        "Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets",
        "Collaborate with DBA on database design and performance enhancements",
        "Leading in all phases of the software development life cycle in a team environment",
        "Debug existing code and troubleshoot for issues",
        "Design and provide a framework for maintaining existing data warehouse for reporting and data analytics",
        "Follow best practices, design, develop, test and document ETL processes"
      ]
    },
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer 5 - Playback",
    "employer_name": "Netflix",
    "employer_logo": null,
    "employer_website": "https://about.netflix.com",
    "job_publisher": "Careers At Netflix",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At Netflix",
        "apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://netflix.wd1.myworkdayjobs.com/netflix/job/usa---remote/data-engineer-5---playback_jr33081?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=949db8d2e2fdda48&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ShowbizJobs",
        "apply_link": "https://www.showbizjobs.com/jobs/netflix-data-engineer-games-in-los-gatos/jid-23obn2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-5-playback-netflix-JV_KO0,24_KE25,32.htm?jl=1009715302012&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/freelancer-data-engineer-developer-at-norconsulting-global-recruitment-4267468273?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6803ddfc9501a3a17fa1d905-data-engineer-5---playback?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Taro",
        "apply_link": "https://www.jointaro.com/jobs/netflix/data-engineer-5-playback/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\n\nWe are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.\n\nThe Playback pod within the Consumer Engineering Product Data Engineering team partners with client and edge systems to produce playback datasets, a source of truth for Title, Live and Ads performance, title popularity (Netflix top 10), and member quality of experience. This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP. It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network.\n\nThis role is focused on supporting Netflix’s core datasets, delivering high-quality business metrics, and building systems to process batch and real-time data at a large scale. Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques. Since data engineers are responsible for their pipelines at Netflix, this role requires engineers to take ownership of the operational excellence in their domain. In addition, the ideal candidate will have excellent data intuition and share our passion for continuously improving how we handle streaming data at Netflix.\n\nWho are you?\n• You have strong product ownership and good intuition on how data is used to drive business decisions\n• You strive to write elegant code and are comfortable with independently picking up new technologies.\n• You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL.\n• You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models.\n• You have a strong background in at least one of the following: distributed data processing or software engineering of data services.\n• You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets.\n• You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation.\n• At Netflix, you own your code, services, and pipelines. You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain.\n• You have an eye for detail, good data intuition, and a passion for data quality.\n• You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.\n• You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks.\n\nWhat you will do:\n• Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences.\n• Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data\n• Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets.\n• Maintain and rethink existing pipelines to improve scalability and maintainability.\n\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\n\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\n\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\n\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\nJob is open for no less than 7 days and will be removed when the position is filled.",
    "job_is_remote": null,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1752364800,
    "job_posted_at_datetime_utc": "2025-07-13T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DeCMK1KxoFvtFAEKKAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques",
        "You have strong product ownership and good intuition on how data is used to drive business decisions",
        "You strive to write elegant code and are comfortable with independently picking up new technologies",
        "You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL",
        "You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models",
        "You have a strong background in at least one of the following: distributed data processing or software engineering of data services",
        "You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets",
        "You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation",
        "At Netflix, you own your code, services, and pipelines",
        "You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain",
        "You have an eye for detail, good data intuition, and a passion for data quality",
        "You relate to and embody many of the aspects of the Netflix Culture",
        "You love working independently while also collaborating and giving/receiving candid feedback",
        "You are comfortable working in a rapidly changing environment with ambiguous requirements",
        "You are nimble and take intelligent risks"
      ],
      "Benefits": [
        "Our compensation structure consists solely of an annual salary; we do not have bonuses",
        "You choose each year how much of your compensation you want in salary versus stock options",
        "To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range",
        "The range for this role is $170,000 - $720,000",
        "Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits",
        "We also offer paid leave of absence programs",
        "Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off",
        "Full-time salaried employees are immediately entitled to flexible time off"
      ],
      "Responsibilities": [
        "This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP",
        "It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network",
        "Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences",
        "Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data",
        "Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets",
        "Maintain and rethink existing pipelines to improve scalability and maintainability"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - MLOps | Remote in the USA - 1725",
    "employer_name": "PlacingIT",
    "employer_logo": null,
    "employer_website": "https://www.placingit.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - MLOps | Remote in the USA - 1725\n\nLocation: Hybrid - Onsite 4 days a week in Dallas, TX\nEmployment Type 6-12+ months contract w/possible extensions\nInterview: In-Person interview is required for this role.\nResidency Requirements: Those authorized to work in the US are encouraged to apply.\n\nRequired Qualifications\n• 5+ years of software development experience building and delivering customer facing cloud-based analytic solutions.\n• 5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives.\n• 5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope.\n• 5+ years coding in Python/PySpark, SQL, and LLM modeling.\n• 5+ years in cloud-based platforms such as Azure, GCP and AWS.\n\nPreferred Qualifications\n• 5+ years of experience leading the design, development, and deployment of applications on Palantir Foundry Platform\n• Retail merchandising domain expertise including category management, assortment optimization, product clustering, product price sensitivity and promotion affinity evaluation, store clustering and localization.\n• Experience with Prompt Engineering developing solutions leveraging LLM models.\n\nResponsibilities include:\n• You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions.\n• You will solve complex problems and deliver decision support tools to improve customer experience.\n• Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses.\n• Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces.\n• Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses.\n• Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry.\n• Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle.\n• Provide technical guidance, mentorship, and support to junior developers and team members.\n• Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem.",
    "job_is_remote": null,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1752537600,
    "job_posted_at_datetime_utc": "2025-07-15T00:00:00.000Z",
    "job_location": "Dallas, TX",
    "job_city": "Dallas",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7766642,
    "job_longitude": -96.79698789999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRxeJao3yHoNlD18FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Residency Requirements: Those authorized to work in the US are encouraged to apply",
        "5+ years of software development experience building and delivering customer facing cloud-based analytic solutions",
        "5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives",
        "5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope",
        "5+ years coding in Python/PySpark, SQL, and LLM modeling",
        "5+ years in cloud-based platforms such as Azure, GCP and AWS"
      ],
      "Responsibilities": [
        "You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions",
        "You will solve complex problems and deliver decision support tools to improve customer experience",
        "Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses",
        "Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces",
        "Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses",
        "Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry",
        "Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle",
        "Provide technical guidance, mentorship, and support to junior developers and team members",
        "Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": null,
    "job_title": "Senior/Lead Data Engineer , Remote, $63/HR",
    "employer_name": "FASTRA LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8iFPXl68j7QGKA_ta-TCHN1e0yurtlbKR__Xc&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Quality Contracts",
        "apply_link": "https://qualitycontracts.co.uk/contract-jobs/remote-senior-lead-data-engineer-remote-63-hr-in-usa-1752741389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior/Lead Data Engineer\n\nLocation: Remote\n\nDuration: 12 Months\n\nResponsibilities:\n• Collaborate with cross-functional stakeholders (including data scientists, analysts, and business teams) to define data requirements, processing needs, and analytics solutions.\n• Create and sustain scalable data models and efficient extraction processes.\n• Establish and monitor data quality checks and validation systems.\n• Construct and enhance business intelligence dashboards.\n• Produce comprehensive documentation for data models and processes.\n• Adapt to shifting priorities and ad-hoc requests.\n• Drive continuous improvement in data practices and visualization techniques.\n\nRequirements:\n• 5+ years of proven experience in database engineering and software development.\n• Advanced skills in SQL for complex query development and database management.\n• Strong Python programming skills for automation and data processing workflows.\n• Ability to handle large-scale data processing tasks using Spark.\n• Knowledge of data visualization tools, Qlik is preferred.\n• Familiarity with cloud platforms, particularly AWS (Glue and Athena desired).\n• Growth mindset with enthusiasm to learn new technologies.",
    "job_is_remote": null,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPrsQLoxA4Q6wEGlDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer 5 - Playback",
    "employer_name": "Netflix",
    "employer_logo": null,
    "employer_website": "https://about.netflix.com",
    "job_publisher": "Careers At Netflix",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At Netflix",
        "apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://netflix.wd1.myworkdayjobs.com/netflix/job/usa---remote/data-engineer-5---playback_jr33081?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=949db8d2e2fdda48&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ShowbizJobs",
        "apply_link": "https://www.showbizjobs.com/jobs/netflix-data-engineer-games-in-los-gatos/jid-23obn2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-5-playback-netflix-JV_KO0,24_KE25,32.htm?jl=1009715302012&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/freelancer-data-engineer-developer-at-norconsulting-global-recruitment-4267468273?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6803ddfc9501a3a17fa1d905-data-engineer-5---playback?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Taro",
        "apply_link": "https://www.jointaro.com/jobs/netflix/data-engineer-5-playback/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\n\nWe are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.\n\nThe Playback pod within the Consumer Engineering Product Data Engineering team partners with client and edge systems to produce playback datasets, a source of truth for Title, Live and Ads performance, title popularity (Netflix top 10), and member quality of experience. This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP. It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network.\n\nThis role is focused on supporting Netflix’s core datasets, delivering high-quality business metrics, and building systems to process batch and real-time data at a large scale. Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques. Since data engineers are responsible for their pipelines at Netflix, this role requires engineers to take ownership of the operational excellence in their domain. In addition, the ideal candidate will have excellent data intuition and share our passion for continuously improving how we handle streaming data at Netflix.\n\nWho are you?\n• You have strong product ownership and good intuition on how data is used to drive business decisions\n• You strive to write elegant code and are comfortable with independently picking up new technologies.\n• You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL.\n• You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models.\n• You have a strong background in at least one of the following: distributed data processing or software engineering of data services.\n• You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets.\n• You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation.\n• At Netflix, you own your code, services, and pipelines. You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain.\n• You have an eye for detail, good data intuition, and a passion for data quality.\n• You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.\n• You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks.\n\nWhat you will do:\n• Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences.\n• Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data\n• Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets.\n• Maintain and rethink existing pipelines to improve scalability and maintainability.\n\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\n\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\n\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\n\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\nJob is open for no less than 7 days and will be removed when the position is filled.",
    "job_is_remote": null,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1752364800,
    "job_posted_at_datetime_utc": "2025-07-13T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DeCMK1KxoFvtFAEKKAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques",
        "You have strong product ownership and good intuition on how data is used to drive business decisions",
        "You strive to write elegant code and are comfortable with independently picking up new technologies",
        "You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL",
        "You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models",
        "You have a strong background in at least one of the following: distributed data processing or software engineering of data services",
        "You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets",
        "You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation",
        "At Netflix, you own your code, services, and pipelines",
        "You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain",
        "You have an eye for detail, good data intuition, and a passion for data quality",
        "You relate to and embody many of the aspects of the Netflix Culture",
        "You love working independently while also collaborating and giving/receiving candid feedback",
        "You are comfortable working in a rapidly changing environment with ambiguous requirements",
        "You are nimble and take intelligent risks"
      ],
      "Benefits": [
        "Our compensation structure consists solely of an annual salary; we do not have bonuses",
        "You choose each year how much of your compensation you want in salary versus stock options",
        "To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range",
        "The range for this role is $170,000 - $720,000",
        "Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits",
        "We also offer paid leave of absence programs",
        "Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off",
        "Full-time salaried employees are immediately entitled to flexible time off"
      ],
      "Responsibilities": [
        "This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP",
        "It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network",
        "Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences",
        "Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data",
        "Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets",
        "Maintain and rethink existing pipelines to improve scalability and maintainability"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - MLOps | Remote in the USA - 1725",
    "employer_name": "PlacingIT",
    "employer_logo": null,
    "employer_website": "https://www.placingit.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - MLOps | Remote in the USA - 1725\n\nLocation: Hybrid - Onsite 4 days a week in Dallas, TX\nEmployment Type 6-12+ months contract w/possible extensions\nInterview: In-Person interview is required for this role.\nResidency Requirements: Those authorized to work in the US are encouraged to apply.\n\nRequired Qualifications\n• 5+ years of software development experience building and delivering customer facing cloud-based analytic solutions.\n• 5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives.\n• 5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope.\n• 5+ years coding in Python/PySpark, SQL, and LLM modeling.\n• 5+ years in cloud-based platforms such as Azure, GCP and AWS.\n\nPreferred Qualifications\n• 5+ years of experience leading the design, development, and deployment of applications on Palantir Foundry Platform\n• Retail merchandising domain expertise including category management, assortment optimization, product clustering, product price sensitivity and promotion affinity evaluation, store clustering and localization.\n• Experience with Prompt Engineering developing solutions leveraging LLM models.\n\nResponsibilities include:\n• You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions.\n• You will solve complex problems and deliver decision support tools to improve customer experience.\n• Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses.\n• Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces.\n• Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses.\n• Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry.\n• Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle.\n• Provide technical guidance, mentorship, and support to junior developers and team members.\n• Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem.",
    "job_is_remote": null,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1752537600,
    "job_posted_at_datetime_utc": "2025-07-15T00:00:00.000Z",
    "job_location": "Dallas, TX",
    "job_city": "Dallas",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7766642,
    "job_longitude": -96.79698789999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRxeJao3yHoNlD18FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Residency Requirements: Those authorized to work in the US are encouraged to apply",
        "5+ years of software development experience building and delivering customer facing cloud-based analytic solutions",
        "5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives",
        "5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope",
        "5+ years coding in Python/PySpark, SQL, and LLM modeling",
        "5+ years in cloud-based platforms such as Azure, GCP and AWS"
      ],
      "Responsibilities": [
        "You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions",
        "You will solve complex problems and deliver decision support tools to improve customer experience",
        "Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses",
        "Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces",
        "Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses",
        "Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry",
        "Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle",
        "Provide technical guidance, mentorship, and support to junior developers and team members",
        "Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": null,
    "job_title": "Senior Data Engineer - Full remote",
    "employer_name": "All European Careers",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsGyayz3Bns7zN430bb2TN0tCgmnP5cORT61YZ&s=0",
    "employer_website": "https://www.all-european-careers.com",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JOBITT",
        "apply_link": "https://jobitt.com/job-openings/external/senior-engineer-full-stack-7870835690680623803?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: Europe\n\nThis is a remote position.\n\nFor an International Institution in Geneva, we are urgently looking for an experienced Senior Data Engineer preferably with Azure experience in Geneva or Full remote.As Senior Data Engineer, you work with the Cloud Architect, Data Architect, Solution Engineer and other technical professionals to act as technical focal point for the development of prototypes, providing advice and technical feasibility view, and other technical activities to ensure the Cloud data Platform architecture and technical configuration will address business and IT objectives.\n\nCandidates need to be fluent in English. This positions is long-term. Work permit not required.Candidates need to be based in Europe.\n\nTasks and Responsibilities:\n• Plan the activites and concrete milestones to deliver the new Cloud Data Platform Architecture;\n• Lead the development and technical implementation aligned to the Data Platform Architecture;\n• Provide expertise and technical advice in the development of conceptual, logical and physical data models, in support of interoperability protocols, API design and management, cybersecurity aspects and business intelligence;\n• Create the data factory pipelines to ingest data, apply data transformations to curate data using databricks, and make data available for downstream applications (API) or reporting from SQL database, Cosmos DB or Synapse Analytics;\n• Automation and development lifecycle control by development of rbooks, fctions, devops projects, sync to repos, ARM templates and Azure Blueprints;\n• Working with the Senior Solution Engineer derstand and catalogue the current landscape of Data and systems;\n• Develop and maintain the to-be Cloud Data Platform with the required capabilities in conformance with security requirements, agreements-based, with user-driven configuration and through documented design and configuration;\n• Provide technical expertise regarding short terms solution options to leverage in the immediate future to meet urgent data related demands, with consideration of the organization’s wider needs and the potential of deploying enterprise data solution platforms at the enterprise level;\n• Provide data insights and best practices, ensuring they are reflected in the development;\n\nProfile:\n• Bachelor or Master degree;\n• +5 years of relevant experience as Data Engineer;\n• Experience with modern Data technologies such as Business Intelligence, Analytics, AI, and Big Data;\n• Experience programming multiple languages, e.g. Phyton, R, Java, Scala, etc; Professional level vendor certifications, such as Microsoft, ITIL, and others;\n• Extensive experience with Microsoft Azure and other cloud technologies and interoperable solutions,experience as a data analyst, engineer and developer;\n• Demonstrated experience in the design, development, and implementation of various integrations between diverse infrastructure services, data models and architecture;\n• Demonstrated experience with the IT systems development life cycle (SDLC), as well as Agile/Scrum methodologies and ITIL processes;\n• Ability to conduct requirements gathering, interpret needs, and design solutions and manage expectations;\n• Professional experience in technical design and support of global, distributed corporate information systems;\n• Understanding state of Azure, Google and AWS components and reference architectures;\n\nExperience in designing for Bigdata/data warehousing/business intelligence/reporting solutions for various physical environments, both on-premises as well as in the Microsoft cloud, while influencing for the most efficient approach based on business requirements;\n\nExcellent written and spoken English;\n\nInterested: Please send your resume to:\nresume@all-european-careers.com\n\nThis offer from \"All European Careers\" has been enriched by Jobgether.com and got a 77% flex score.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DSjsMVNnmi0YDQr4jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "W2 Contract || Title- Azure Data Engineer in 100% Remote || USC & GC only on W2 || 12 Years Exp.",
    "employer_name": "JS Consulting",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRE1gF8gajTg6maWTnVUROZLV3HcGESVQuhd_J8&s=0",
    "employer_website": "https://www.jsconsultingco.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Note: Candidate must be Work on W2 and USC & GC Visa only.\n\nTitle: Azure Data Engineer (12+ Years Experinece)\n\nLocation: Kansas City, MO 100% Remote\n\nDuration: 6-12+ Months (W2 Contract)\n\nInterview: Video\n\nVisa: USC/GC (must work on our w2) (need strong communication)\n\nJob Description\n\nMake sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager.\n\nDon't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes.\n\nCommunication must be flawless\n\nResume should not be more than 6 pages\n\nMust have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020.\n\nJob Description-\n\nMust have strong Azure, ADF and Databricks experience.\n\nThe purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks. It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes. This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files.\n\nEssential Duties And Responsibilities\n• Work with a highly dynamic team focused on Digital Transformation.\n• Understand the domain and business processes to implement successful data pipelines.\n• Provide work status and coordinate with Data Engineers.\n• Manage customer deliverables and regularly report the status via Weekly/Monthly reviews.\n• Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views\n• Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server.\n• Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot.\n• Design normalized database tables with proper indexing and constraints.\n• Perform SQL query tuning and performance optimization on complex and inefficient queries.\n• Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets.\n• Collaborate with DBA on database design and performance enhancements.\n• Leading in all phases of the software development life cycle in a team environment.\n• Debug existing code and troubleshoot for issues.\n• Design and provide a framework for maintaining existing data warehouse for reporting and data analytics.\n• Follow best practices, design, develop, test and document ETL processes.",
    "job_is_remote": null,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Kansas City, MO",
    "job_city": "Kansas City",
    "job_state": "Missouri",
    "job_country": "US",
    "job_latitude": 39.099726499999996,
    "job_longitude": -94.5785667,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3KUvi58ta8OiVWqPAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Note: Candidate must be Work on W2 and USC & GC Visa only",
        "Title: Azure Data Engineer (12+ Years Experinece)",
        "Visa: USC/GC (must work on our w2) (need strong communication)",
        "Communication must be flawless",
        "Resume should not be more than 6 pages",
        "Must have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020",
        "Must have strong Azure, ADF and Databricks experience",
        "Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server"
      ],
      "Responsibilities": [
        "Make sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager",
        "Don't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes",
        "The purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks",
        "It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes",
        "This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files",
        "Work with a highly dynamic team focused on Digital Transformation",
        "Understand the domain and business processes to implement successful data pipelines",
        "Provide work status and coordinate with Data Engineers",
        "Manage customer deliverables and regularly report the status via Weekly/Monthly reviews",
        "Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views",
        "Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot",
        "Design normalized database tables with proper indexing and constraints",
        "Perform SQL query tuning and performance optimization on complex and inefficient queries",
        "Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets",
        "Collaborate with DBA on database design and performance enhancements",
        "Leading in all phases of the software development life cycle in a team environment",
        "Debug existing code and troubleshoot for issues",
        "Design and provide a framework for maintaining existing data warehouse for reporting and data analytics",
        "Follow best practices, design, develop, test and document ETL processes"
      ]
    },
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineers (Hybrid | DC Area) Remote / Telecommute Jobs",
    "employer_name": "Rackner",
    "employer_logo": null,
    "employer_website": "https://rackner.com",
    "job_publisher": "Security Clearance Jobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Security Clearance Jobs",
        "apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Data Engineer\n\nLocation: Falls Church, VA (Hybrid)\n\nClearance: Secret Clearance\n\nAbout this role:\n\nRackner is looking for a Data Engineer that will be working within an Agile DevSecOps team environment using latest cloud-native technologies to architect and implement containerized applications, CI/CD pipelines, and Kubernetes platforms using best practices and leading technologies.\n\nWe are seeking professionals with:\n\nB.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms\n\nModern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)\n\nDeveloping data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules\n\nMature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR\n\nNice to have:\n\nM.S in Computer Science or related\n\nAI/ML\n\nKubernetes (Rancher RKE2, AWS EKS) and microservice architectures\n\nData engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR\n\nNIST Risk Management Framework and security accreditation process and tasks\n\nWhat will make you successful:\n\nUsing DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer\n\nDeveloping software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes\n\nLeveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs\n\nApplying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders\n\nEmbracing a shared responsibility for system security\n\nPerforming threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle\n\nContinuously engaging with project teams to deliver quality products\n\nParticipating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions\n\nWho We Are:\n\nRackner is a software consultancy that builds cloud-native solutions for startups, enterprises, and the public sector.\n\nWe are an energetic, growing consultancy with a passion for solving big problems for both startups and enterprises.\n\nEach of us enable digital transformation for large organizations through the newest in distributed technologies as we are laser focused on end-to-end application development, DevSecOps, AI/ML and systems architecture and our methodology focuses on cloud-first and cost-effective innovation.\n\nOur customers hail from a diverse, ever-growing list of industries.\n\nBenefits/Additional Info:\n\nRackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided . Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!\n\n401K with 100% matching up to 6%\n\nHighly competitive PTO\n\nGreat health insurance with large network of providers\n\nMedical/Dental/Vision\n\nLife Insurance, and short & long term disability\n\nIndustry-Leading Weekly Pay Schedule\n\nHome office & equipment plan\n\n#DataEngineer #AWS #Topsecret #FDA #publictrust #DataIngesting #DataPipeline #Python #Terraform #ETL #AI #ML #dataintegration #bigdataanalyticspipeline #awsbigdata #hadoop #apachespark #RDBMS #awsdynamoDB #collaboration #diversity #equity #Inclusion",
    "job_is_remote": null,
    "job_posted_at": "12 hours ago",
    "job_posted_at_timestamp": 1753128000,
    "job_posted_at_datetime_utc": "2025-07-21T20:00:00.000Z",
    "job_location": "Falls Church, VA",
    "job_city": "Falls Church",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.882334,
    "job_longitude": -77.1710914,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DL2b_MhwdllUxOf0jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "B.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms",
        "Modern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)",
        "Developing data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules",
        "Mature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR",
        "M.S in Computer Science or related",
        "Kubernetes (Rancher RKE2, AWS EKS) and microservice architectures",
        "Data engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR",
        "NIST Risk Management Framework and security accreditation process and tasks"
      ],
      "Benefits": [
        "Rackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided ",
        "Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!",
        "401K with 100% matching up to 6%",
        "Highly competitive PTO",
        "Great health insurance with large network of providers",
        "Life Insurance, and short & long term disability",
        "Industry-Leading Weekly Pay Schedule",
        "Home office & equipment plan"
      ],
      "Responsibilities": [
        "Using DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer",
        "Developing software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes",
        "Leveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs",
        "Applying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders",
        "Embracing a shared responsibility for system security",
        "Performing threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle",
        "Continuously engaging with project teams to deliver quality products",
        "Participating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Principal Data Engineer *Remote - Most states eligible*",
    "employer_name": "Providence",
    "employer_logo": null,
    "employer_website": "http://www.psjhealth.org/",
    "job_publisher": "Teal",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the position\n\nThe Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making. This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics. The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St. Joseph Health system, focusing on quality and value of care metrics. The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights.\n\nResponsibilities\n• Collect and analyze clinical data from various sources.\n,\n• Create visualizations and storytelling using data to communicate findings.\n,\n• Develop novel reporting platforms to address healthcare challenges.\n,\n• Collaborate with clinical and administrative leaders to evaluate new measures.\n,\n• Identify impactful findings in large clinical data stores.\n,\n• Design strong data visualizations to communicate key information across the organization.\n,\n• Support team members with experience in computational methods.\n,\n• Develop innovative methods for outcome measures in the whole person care model.\n\nRequirements\n• Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience.\n,\n• 9 years of relevant data analysis experience, preferably in a biomedical setting.\n,\n• Experience with database query and analysis languages (e.g., SQL, R, SAS, Python).\n,\n• Proficiency in data visualization tools (e.g., Tableau, D3).\n\nNice-to-haves\n• Ph.D. in a related field or equivalent experience.\n,\n• Certification in an IT discipline, application, or tool upon hire.\n,\n• Lean certification or Green Belt, Black Belt upon hire.\n,\n• Certification in Data Science upon hire.\n,\n• 4+ years of development experience in data exchange using FHIR.\n\nBenefits\n• 401(k) retirement savings plan with employer matching.\n,\n• Health care benefits (medical, dental, vision).\n,\n• Life insurance.\n,\n• Disability insurance.\n,\n• Paid parental leave.\n,\n• Vacation and holiday time off.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Renton, WA",
    "job_city": "Renton",
    "job_state": "Washington",
    "job_country": "US",
    "job_latitude": 47.4796927,
    "job_longitude": -122.2079218,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXswWukx0iLqtemRoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience",
        "9 years of relevant data analysis experience, preferably in a biomedical setting",
        "Experience with database query and analysis languages (e.g., SQL, R, SAS, Python)",
        "Proficiency in data visualization tools (e.g., Tableau, D3)",
        "Ph.D. in a related field or equivalent experience",
        "Certification in an IT discipline, application, or tool upon hire",
        "Lean certification or Green Belt, Black Belt upon hire",
        "Certification in Data Science upon hire",
        "4+ years of development experience in data exchange using FHIR"
      ],
      "Benefits": [
        "401(k) retirement savings plan with employer matching",
        "Health care benefits (medical, dental, vision)",
        "Life insurance",
        "Disability insurance",
        "Paid parental leave",
        "Vacation and holiday time off"
      ],
      "Responsibilities": [
        "The Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making",
        "This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics",
        "The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St",
        "The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights",
        "Collect and analyze clinical data from various sources",
        "Create visualizations and storytelling using data to communicate findings",
        "Develop novel reporting platforms to address healthcare challenges",
        "Collaborate with clinical and administrative leaders to evaluate new measures",
        "Identify impactful findings in large clinical data stores",
        "Design strong data visualizations to communicate key information across the organization",
        "Support team members with experience in computational methods",
        "Develop innovative methods for outcome measures in the whole person care model"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Providence",
        "score": 3.8,
        "num_stars": 4,
        "review_count": 3699,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Providence-959155fe/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Providence",
        "score": 3.5,
        "num_stars": 3.5,
        "review_count": 6699,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Providence-Reviews-E4651.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "AWS Data Engineer - Fully Remote - US Only",
    "employer_name": "Scalepex",
    "employer_logo": null,
    "employer_website": "https://scalepex.com",
    "job_publisher": "Jobs By Workable",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://apply.workable.com/scalepex/j/F5ED805D80?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobs By Workable",
        "apply_link": "https://apply.workable.com/scalepex/j/F5ED805D80?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Insight Global Jobs",
        "apply_link": "https://jobs.insightglobal.com/find_a_job/connecticut/job-313330/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ApplicantSite",
        "apply_link": "https://applicants.bairesdev.com/job/190/252655/apply?utm_source=trampedecasa&utm_medium=jobposting&utm_campaign=Remote-20240530?utm_source%3Dtrampedecasautm_medium%3Dreferral&utm_campaign=vaga-publicada-na-trampedecasa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "DailyRemote",
        "apply_link": "https://dailyremote.com/remote-job/aws-data-engineer-fully-remote-us-only-3512285?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Ladders",
        "apply_link": "https://www.theladders.com/job/aws-data-engineer-remote-nava-software-solutions-virtual-travel_82025315?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "❋ Why Scalepex?\n\nScalepex is a dynamic services firm specializing in providing solutions for premium brands like Nike, Pepsi, Toyota, Virgin and Walgreens. Our mission is to connect prominent market leaders with top-tier professionals from around the world, fostering collaboration, efficiency, and growth.\n\n❋ Take your portfolio to the next level by working with one of our fastest growing clients.\n\nJoin the Innovation Frontier at Scalepex!\n\nAbout the Role\n\nWe are seeking an experienced AWS Data Engineer with a strong background in building scalable data solutions and expertise in utilities-related datasets. The ideal candidate will have at least 5 years of experience in data engineering, a deep understanding of distributed systems, and proficiency with AWS services and tools like Step Functions, Lambda, Glue, and Redshift. This role will focus on designing, developing, and optimizing data pipelines to support analytics and decision-making in the utilities industry.\n\nKey Responsibilities\n• Design and Build Data Pipelines: Develop scalable, reliable data pipelines using AWS services (e.g., Glue, S3, Redshift) to process and transform large datasets from utility systems like smart meters or energy grids.\n• Workflow Orchestration: Use AWS Step Functions to orchestrate workflows across data pipelines; experience with Airflow is acceptable but Step Functions is preferred.\n• Data Integration and Transformation: Implement ETL/ELT processes using PySpark, Python, and Pandas to clean, transform, and integrate data from multiple sources into unified datasets.\n• Distributed Systems Expertise: Leverage experience with complex distributed systems to ensure reliability, scalability, and performance in handling large-scale utility data.\n• Serverless Application Development: Use AWS Lambda functions to build serverless solutions for automating data processing tasks.\n• Data Modeling for Analytics: Design data models tailored for utilities use cases (e.g., energy consumption forecasting) to enable advanced analytics\n• Optimize Data Pipelines: Continuously monitor and improve the performance of data pipelines to reduce latency, enhance throughput, and ensure high availability.\n• Ensure Data Security and Compliance: Implement robust security measures to protect sensitive utility data and ensure compliance with industry regulations.\n\nRequired Qualifications\n• Minimum of 5 years of experience in data engineering\n• Proficiency in AWS services such as Step Functions, Lambda, Glue, S3, DynamoDB, and Redshift.\n• Strong programming skills in Python with experience using PySpark and Pandas for large-scale data processing.\n• Hands-on experience with distributed systems and scalable architectures.\n• Knowledge of ETL/ELT processes for integrating diverse datasets into centralized systems.\n• Familiarity with utilities-specific datasets (e.g., smart meters, energy grids) is highly desirable.\n• Strong analytical skills with the ability to work on unstructured datasets.\n• Knowledge of data governance practices to ensure accuracy, consistency, and security of data.\n• Strong experience in AWS data engineering\n• Ability to work independently\n• Ability to work with a cross-functional teams, including interfacing and communicating with business stakeholders\n• Professional oral and written communication skills\n• Strong problem solving and troubleshooting skills with experience exercising mature judgement\n• Excellent teamwork and interpersonal skills\n• Ability to obtain and maintain the required clearance for this role",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DbMI3tTg8CkVNRcoxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer (Remote)",
    "employer_name": "Authority Brands",
    "employer_logo": null,
    "employer_website": "http://www.authoritybrands.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Authority-Brands/Job/Data-Engineer-(Remote)/-in-Columbia,MD?jid=4654298a59de84ef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Authority-Brands/Job/Data-Engineer-(Remote)/-in-Columbia,MD?jid=4654298a59de84ef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-remote-at-authority-brands-4266961170?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Experteer GmbH",
        "apply_link": "https://us.experteer.com/career/view-jobs/data-engineer-remote-columbia-md-usa-52992124?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/QizhCwUPqC5y2cREZFac5D84cSac8fO3fugLwCJXdQCkfBECS4Xrmg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Authority Brands Inc. headquartered in Columbia, MD is a leading provider of home services, building brands that support the success of our franchisees, as well as bettering the lives of the homeowners we serve and the people we employ.\n\nAbout Us: Authority Brands is a leading provider of home service franchises, dedicated to supporting franchise owners with elite marketing, advanced technology, and strong operational support. Founded in 2017, Authority Brands has grown to include 15 industry-leading home service franchisors, providing a wide range of services from plumbing and electrical work to lawn care and pest control.\n\nMission: Our mission is to empower franchise owners to succeed by offering best-in-class training, operational support, and marketing systems. We aim to deliver exceptional home services that customers trust and rely on.\n\nCulture: At Authority Brands, we foster a collaborative and supportive work environment where innovation and excellence are encouraged. We believe in the power of teamwork and are committed to helping our franchise owners achieve their personal and professional goals.\n\nJob Overview: The Data Analytics team at Authority Brands is seeking a skilled Data Engineer with at least 4 years of experience, strong SQL expertise, Python programming skills, and hands-on experience with AWS, including AWS Glue. You will collaborate with our data analytics and development teams to build, maintain, and optimize our data pipelines and infrastructure. We are looking for a candidate who enjoys solving complex data challenges and is eager to contribute to a dynamic team.\n\nKey Responsibilities:\n• Develop and optimize complex SQL queries to ensure efficient data retrieval, manipulation, and reporting.\n• Collaborate with data analysts, data scientists, and cross-functional teams to understand their data requirements and deliver solutions.\n• Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.\n• Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\n• Analyze and solve problems at their root, stepping back to understand the broader context in a timely manner.\n• Design, build, and maintain efficient and reliable data pipelines that support the company's data platform.\n• Leverage Python for ETL processes, automation scripts, and data pipeline development.\n• Utilize AWS services, including S3, RDS, Redshift, Lambda, and AWS Glue, to design and maintain cloud-based data infrastructure.\n• Work with AWS Glue for ETL transformations, data cataloging, and automation of data processes.\n• Monitor and improve the performance of data pipelines and data processing systems.\n\nQualifications:\n• Bachelor's degree in Computer Science, Engineering, or a related field.\n• Minimum of 4 years of experience in data engineering or a related role.\n• Strong proficiency in SQL for querying, optimizing, and managing large datasets.\n• Experience with data modeling and schema design.\n• Solid programming skills in Python, with experience in building and optimizing ETL pipelines.\n• Hands-on experience with AWS cloud services, including data-related tools such as S3, Redshift, RDS, Lambda, and AWS Glue.\n• Familiarity with version control (e.g., Git), CI/CD pipelines, and agile methodologies.\n• Experience working with large datasets and building scalable solutions.\n• Strong problem-solving skills and ability to work independently or within a team.\n• Excellent communication skills with the ability to explain technical concepts to non-technical stakeholders.\n\nWe believe our greatest assets are our employees, we offer competitive salaries and a full benefits package to include, PTO, paid holidays, 401(k) and more.\n\nAuthority Brands Inc. conducts drug screens and background checks on applicants who accept employment offers. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions however we do not sponsor Visa's at this time.\n\nAuthority Brands Inc. is an Equal Opportunity Employer",
    "job_is_remote": null,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Columbia, MD",
    "job_city": "Columbia",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.203714399999996,
    "job_longitude": -76.86104619999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDt3-pBfk2JtUPqzdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Job Overview: The Data Analytics team at Authority Brands is seeking a skilled Data Engineer with at least 4 years of experience, strong SQL expertise, Python programming skills, and hands-on experience with AWS, including AWS Glue",
        "We are looking for a candidate who enjoys solving complex data challenges and is eager to contribute to a dynamic team",
        "Bachelor's degree in Computer Science, Engineering, or a related field",
        "Minimum of 4 years of experience in data engineering or a related role",
        "Strong proficiency in SQL for querying, optimizing, and managing large datasets",
        "Experience with data modeling and schema design",
        "Solid programming skills in Python, with experience in building and optimizing ETL pipelines",
        "Hands-on experience with AWS cloud services, including data-related tools such as S3, Redshift, RDS, Lambda, and AWS Glue",
        "Familiarity with version control (e.g., Git), CI/CD pipelines, and agile methodologies",
        "Experience working with large datasets and building scalable solutions",
        "Strong problem-solving skills and ability to work independently or within a team",
        "Excellent communication skills with the ability to explain technical concepts to non-technical stakeholders",
        "conducts drug screens and background checks on applicants who accept employment offers"
      ],
      "Benefits": [
        "We believe our greatest assets are our employees, we offer competitive salaries and a full benefits package to include, PTO, paid holidays, 401(k) and more"
      ],
      "Responsibilities": [
        "You will collaborate with our data analytics and development teams to build, maintain, and optimize our data pipelines and infrastructure",
        "Develop and optimize complex SQL queries to ensure efficient data retrieval, manipulation, and reporting",
        "Collaborate with data analysts, data scientists, and cross-functional teams to understand their data requirements and deliver solutions",
        "Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc",
        "to drive key business decisions",
        "Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation",
        "Analyze and solve problems at their root, stepping back to understand the broader context in a timely manner",
        "Design, build, and maintain efficient and reliable data pipelines that support the company's data platform",
        "Leverage Python for ETL processes, automation scripts, and data pipeline development",
        "Utilize AWS services, including S3, RDS, Redshift, Lambda, and AWS Glue, to design and maintain cloud-based data infrastructure",
        "Work with AWS Glue for ETL transformations, data cataloging, and automation of data processes",
        "Monitor and improve the performance of data pipelines and data processing systems"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer | Remote | Strong on AWS Glue/AWS Services |   GC-EAD & -EAD(Only Genuine) | W2 Position",
    "employer_name": "URSI Technologies Inc.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQs7wpzA8c8DZ6Vw2RuoOcSMab3tVao0EfNq501&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/fa29282b-81e1-49d4-9386-188f2360713f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/fa29282b-81e1-49d4-9386-188f2360713f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Data EngineerLocation: Remote RoleDuration: 6+ Months ContractVisa Status: GC-EAD & -EAD ONLY!! (Need Passport Copy f & Passport Number f-EAD & EAD) No H1's/OPT/CPT!! Note: This is a Remote role, but potential office for the quarterly planning meets it s in Pittsburgh or Philadelphia, PA. Roles and Responsibilities:Collaborate closely with cross-functional teams including Data Scientists, Analysts, and Software Engineers to understand data requirements and translate them into efficient solutions using AWS Glue and AWS services.Develop and maintain ETL processes using AWS Glue to facilitate seamless and reliable data extraction, transformation, and loading.Implement robust data security measures and access controls in alignment with company policies and industry best practices.Monitor, troubleshoot, and enhance data pipelines, identifying and resolving performance bottlenecks, data quality issues, and other challenges.Stay up-to-date with the latest advancements in AWS Glue and AWS services, and advocate for their effective utilization within the organization Experience/Minimum RequirementsProven experience 8+ years as a Data Engineer, with a strong emphasis on AWS Glue and AWS services.In-depth understanding of architecture, performance optimization techniques, and best practices.Proficiency in SQL and experience with database design principles.Hands-on expertise in designing, building, and maintaining complex ETL pipelines using and AWS Glue.Familiarity with data warehousing concepts and methodologies.Competence in cloud computing and AWS services, with a focus on data-related services such as S3, Redshift, and Lambda.Proficiency in scripting and programming languages such as Python, Java, or similar.",
    "job_is_remote": null,
    "job_posted_at": "15 hours ago",
    "job_posted_at_timestamp": 1753117200,
    "job_posted_at_datetime_utc": "2025-07-21T17:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DFWriH--HRVnwoB8JAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer III (Associate) (Remote)",
    "employer_name": "Jobright.ai",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTwX24ib-DGCR06HF9IDGP_J-huDCctBW5MIxyF&s=0",
    "employer_website": "https://jobright.ai",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-associate-remote-at-jobright-ai-4270055641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-associate-remote-at-jobright-ai-4270055641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs - Alooba",
        "apply_link": "https://jobs.alooba.com/us/job/everi-holdings-inc-data-engineer-iii-remote-842416/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Truemote",
        "apply_link": "https://truemote.com/remote-job/data-engineer-iii-associate-remote-b2435c5f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "USAJobCareer.com",
        "apply_link": "https://usajobcareer.com/jobs/view/data-engineer-iii-associate-remote-1672550.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Jobs",
        "apply_link": "https://store.joblagii.com/blogs/news/data-engineer-iii-remote-id-15864?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job_Summary:\n\nAstrana Health is seeking a highly motivated Data Engineer III to join their Data - Analytics department. The role involves collaborating with product teams and clinical leaders to manage and enhance data products, ensuring high-quality data models and addressing data quality issues.\n\nResponsibilities:\n\n• Use data engineering best practices to produce high quality, maximally available data models which are intuitive and trusted by stakeholders.\n\n• Scope and implement new entities for Astrana’s unified data model.\n\n• Interfacing with business customers, gathering requirements and developing new datasets in data platform\n\n• Identifying the data quality issues to address them immediately to provide great user experience\n\n• Extracting and combining data from various heterogeneous data sources\n\n• Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets\n\n• Modelling data and metadata to support machine learning and AI\n\n• Support integration efforts from acquisitions as necessary.\n\nQualifications:\n\n-Required:\n\n• Bachelor's degree required in computer science, information technology, or related field\n\n• Strong understanding of database structures, theories, principles, and practices\n\n• Working knowledge with programming or scripting languages such as Python, Spark, and SQL\n\n• Demonstrated track record executing best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, deployment processes, testing, and operations\n\n• Familiarity with normalized, dimensional, star schema and snowflake schematic models\n\n• Healthcare domain and data experience\n\n• Strong written and oral communication skills\n\n• 4+ years of experience working with diverse healthcare data sources, for example, claims, encounters, ADT data, FHIR, EHR data etc.\n\n• 3+ years’ using cloud-based services from AWS, GCP, or Azure\n\n• 2+ years serving data sets to both BI tools and SE applications\n\n-Preferred:\n\n• Master’s degree in healthcare-related field\n\n• Working experience with Databricks\n\n• Databricks/Microsoft Azure Certification is a plus\n\nCompany:\n\nLeading physician-centric, technology-powered, risk-bearing healthcare mgmt. company delivering high quality care in a cost-effective manner Astrana Health has a track record of offering H1B sponsorships.",
    "job_is_remote": null,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Alhambra, CA",
    "job_city": "Alhambra",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.092754899999996,
    "job_longitude": -118.1268211,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVuP-kAKuKrvGJDw8AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree required in computer science, information technology, or related field",
        "Strong understanding of database structures, theories, principles, and practices",
        "Working knowledge with programming or scripting languages such as Python, Spark, and SQL",
        "Demonstrated track record executing best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, deployment processes, testing, and operations",
        "Familiarity with normalized, dimensional, star schema and snowflake schematic models",
        "Healthcare domain and data experience",
        "Strong written and oral communication skills",
        "4+ years of experience working with diverse healthcare data sources, for example, claims, encounters, ADT data, FHIR, EHR data etc",
        "3+ years’ using cloud-based services from AWS, GCP, or Azure",
        "2+ years serving data sets to both BI tools and SE applications",
        "Master’s degree in healthcare-related field",
        "Working experience with Databricks"
      ],
      "Responsibilities": [
        "The role involves collaborating with product teams and clinical leaders to manage and enhance data products, ensuring high-quality data models and addressing data quality issues",
        "Use data engineering best practices to produce high quality, maximally available data models which are intuitive and trusted by stakeholders",
        "Scope and implement new entities for Astrana’s unified data model",
        "Interfacing with business customers, gathering requirements and developing new datasets in data platform",
        "Identifying the data quality issues to address them immediately to provide great user experience",
        "Extracting and combining data from various heterogeneous data sources",
        "Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets",
        "Modelling data and metadata to support machine learning and AI",
        "Support integration efforts from acquisitions as necessary"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Consultant (Remote)",
    "employer_name": "Releady",
    "employer_logo": null,
    "employer_website": "https://www.releady.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/consultant-data-engineer_93b50b06-e8ad-4459-ac66-68a2c9229a93?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Pangian",
        "apply_link": "https://pangian.com/remote/job/data-engineer-consultant-remote-1o?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EarnBetter",
        "apply_link": "https://earnbetter.com/app/job/01JM148F98JPMEKGYAMF1HY8HZ/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "OVERVIEW\n\nThis Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team. You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies. Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure.\n• Duration: 6+ months contract\n• Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada. Working hours will be PST. Preference for California.\n• Rate: $70/hr - $85/hr DOE\n• **Must be able to work in the United States without sponsorship***\n\nRESPONSIBILITIES\n• Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)\n• Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability\n• Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling\n• Work with Collibra for data governance, quality assurance, and metadata management\n• Leverage Refuel.ai for data mastering and Striim for data validation processes\n• Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows\n• Collaborate with client stakeholders to translate requirements into effective data solutions\n• Support data visualization and reporting needs through Tableau\n• Implement CI/CD practices using Git repositories and modern DevOps tools\n• Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners\n\nQUALIFICATIONS\n• Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)\n• 10+ years of experience in data engineering or related roles\n• Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server\n• Hands-on experience with dbt Cloud for data transformation and pipeline development\n• Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse\n• Knowledge of Data Vault modeling principles and implementation techniques\n• Experience with data governance and data quality tools, particularly Collibra\n• Familiarity with data visualization platforms, especially Tableau\n• Understanding of version control systems (Git, Bitbucket) and CI/CD practices\n• Experience with scheduling systems like Tidal or Control-M\n• Working knowledge of Agile methodologies and DevOps principles applied to data pipelines\n• Preferred Skills:\n• Experience with data observability platforms and data quality monitoring\n• Knowledge of Python, R, KNIME, or Alteryx for data science applications\n• Experience with Refuel.ai and Striim technologies\n• Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms\n• Experience with enterprise scheduling tools like Tidal\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or other non-merit factor. We are committed to creating a diverse and inclusive environment for all employees.",
    "job_is_remote": null,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Sacramento, CA",
    "job_city": "Sacramento",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 38.5781342,
    "job_longitude": -121.4944209,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D00eCRolQcTBw_16xAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 70,
    "job_max_salary": 85,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "**Must be able to work in the United States without sponsorship***",
        "Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)",
        "10+ years of experience in data engineering or related roles",
        "Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server",
        "Hands-on experience with dbt Cloud for data transformation and pipeline development",
        "Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse",
        "Knowledge of Data Vault modeling principles and implementation techniques",
        "Experience with data governance and data quality tools, particularly Collibra",
        "Familiarity with data visualization platforms, especially Tableau",
        "Understanding of version control systems (Git, Bitbucket) and CI/CD practices",
        "Experience with scheduling systems like Tidal or Control-M",
        "Working knowledge of Agile methodologies and DevOps principles applied to data pipelines",
        "Experience with data observability platforms and data quality monitoring",
        "Knowledge of Python, R, KNIME, or Alteryx for data science applications",
        "Experience with Refuel.ai and Striim technologies",
        "Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms",
        "Experience with enterprise scheduling tools like Tidal"
      ],
      "Benefits": [
        "Rate: $70/hr - $85/hr DOE"
      ],
      "Responsibilities": [
        "This Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team",
        "You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies",
        "Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure",
        "Duration: 6+ months contract",
        "Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada",
        "Working hours will be PST. Preference for California",
        "Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)",
        "Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability",
        "Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling",
        "Work with Collibra for data governance, quality assurance, and metadata management",
        "Leverage Refuel.ai for data mastering and Striim for data validation processes",
        "Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows",
        "Collaborate with client stakeholders to translate requirements into effective data solutions",
        "Support data visualization and reporting needs through Tableau",
        "Implement CI/CD practices using Git repositories and modern DevOps tools",
        "Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "URGENT by 6/25:REMOTE:Certified Data Engineer-Databricks, PySpark/Scala, ADFactory, W2 Only",
    "employer_name": "Solitsys",
    "employer_logo": null,
    "employer_website": "http://www.solitsys.com",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time and Contractor",
    "job_employment_types": [
      "FULLTIME",
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "URGENTLY NEEDED: REMOTE DATA ENGINEER- DATA BRICKS , PySpark/Scala, AZURE DATA FACTORY [W2 ONLY, ALL Corp-to-Corp WILL BE REJECTED]\n\nWe cannot offer Corp-to-Corp arrangement. This position is being offered on W2 basis only (no Corp-to-Corp or 1099), we are NOT a head-hunting agency. Please respond ASAP with your detailed resume in Word format. Resume must address the minimum qualifications listed below.\n\nDATA ENGINEER - DATABRICKS SPECIALIST\n\nAre you a skilled Data Engineer with a passion for modernizing data solutions? We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity. This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity. Time-sensitive, project-based with defined deliverables. Apply now!\n\nYOUR RESUME MUST SHOW THESE MINIMUM QUALIFICATIONS:\n• 3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling.\n• 5+ years background: PySpark or Scala for data engineering tasks.\n• 5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning.\n• 5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF).\n• 3+ years of: Delta Lake architecture and implementation for data warehousing.\n• 3+ years of experience with: BI tools such as Power BI or Tableau for reporting.\n• Familiarity with: Azure Cloud environment (Blob Storage, ADLS).\n• Proficiency in: Python scripting for data manipulation and automation.\n\nEducation & Certifications:\n• Bachelor’s degree in Computer Science, Information Systems, Data Science, or related field.\n• Databricks Certified Data Engineer Associate or relevant industry certifications.\n• Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields.\n\nJob Types: Full-time, Contract\n\nPay: $60,000.00 per year\n\nCompensation Package:\n• Hourly pay\n\nSchedule:\n• 8 hour shift\n\nEducation:\n• Bachelor's (Required)\n\nExperience:\n• Databricks: 3 years (Required)\n• Python: 5 years (Required)\n• PySpark: 5 years (Required)\n• Scala: 5 years (Required)\n• Azure Data Lake: 3 years (Required)\n• Azure Data Factory: 3 years (Required)\n• Power BI: 5 years (Preferred)\n• Tableau: 5 years (Preferred)\n\nWork Location: Remote",
    "job_is_remote": null,
    "job_posted_at": "27 days ago",
    "job_posted_at_timestamp": 1750809600,
    "job_posted_at_datetime_utc": "2025-06-25T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuINRzsWvsKKLdrOtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Resume must address the minimum qualifications listed below",
        "3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling",
        "5+ years background: PySpark or Scala for data engineering tasks",
        "5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning",
        "5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF)",
        "3+ years of: Delta Lake architecture and implementation for data warehousing",
        "3+ years of experience with: BI tools such as Power BI or Tableau for reporting",
        "Familiarity with: Azure Cloud environment (Blob Storage, ADLS)",
        "Proficiency in: Python scripting for data manipulation and automation",
        "Bachelor’s degree in Computer Science, Information Systems, Data Science, or related field",
        "Databricks Certified Data Engineer Associate or relevant industry certifications",
        "Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields",
        "Bachelor's (Required)",
        "Databricks: 3 years (Required)",
        "Python: 5 years (Required)",
        "PySpark: 5 years (Required)",
        "Scala: 5 years (Required)",
        "Azure Data Lake: 3 years (Required)",
        "Azure Data Factory: 3 years (Required)"
      ],
      "Benefits": [
        "Pay: $60,000.00 per year",
        "Compensation Package:",
        "Hourly pay",
        "8 hour shift"
      ],
      "Responsibilities": [
        "We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity",
        "This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity",
        "Time-sensitive, project-based with defined deliverables"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Senior/Lead Data Engineer , Remote, $63/HR",
    "employer_name": "FASTRA LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8iFPXl68j7QGKA_ta-TCHN1e0yurtlbKR__Xc&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Quality Contracts",
        "apply_link": "https://qualitycontracts.co.uk/contract-jobs/remote-senior-lead-data-engineer-remote-63-hr-in-usa-1752741389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior/Lead Data Engineer\n\nLocation: Remote\n\nDuration: 12 Months\n\nResponsibilities:\n• Collaborate with cross-functional stakeholders (including data scientists, analysts, and business teams) to define data requirements, processing needs, and analytics solutions.\n• Create and sustain scalable data models and efficient extraction processes.\n• Establish and monitor data quality checks and validation systems.\n• Construct and enhance business intelligence dashboards.\n• Produce comprehensive documentation for data models and processes.\n• Adapt to shifting priorities and ad-hoc requests.\n• Drive continuous improvement in data practices and visualization techniques.\n\nRequirements:\n• 5+ years of proven experience in database engineering and software development.\n• Advanced skills in SQL for complex query development and database management.\n• Strong Python programming skills for automation and data processing workflows.\n• Ability to handle large-scale data processing tasks using Spark.\n• Knowledge of data visualization tools, Qlik is preferred.\n• Familiarity with cloud platforms, particularly AWS (Glue and Athena desired).\n• Growth mindset with enthusiasm to learn new technologies.",
    "job_is_remote": null,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPrsQLoxA4Q6wEGlDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer",
    "employer_name": "Nike",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRSg56NSkRfvvux00tX_ZNojvvrKnI6i-nxdfew&s=0",
    "employer_website": "http://www.nike.com/",
    "job_publisher": "Nike Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.nike.com/data-engineer/job/R-61137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Nike Careers",
        "apply_link": "https://careers.nike.com/data-engineer/job/R-61137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Cardinal Health",
        "apply_link": "https://jobs.cardinalhealth.com/search/jobdetails/data-engineer/610d0c2e-65a6-4123-82b5-db5a51021d33?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://pnc.wd5.myworkdayjobs.com/External/job/OH---Strongsville/Technology-Engineer---Data-and-Automation--Python--R--Unix-_R193407-1/apply?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Wd3.Myworkdaysite.com",
        "apply_link": "https://wd3.myworkdaysite.com/recruiting/magna/Magna/job/Lowell-Massachusetts-US/Data-Engineer_R00164181?source=BuiltInNationwide&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "StarSevenSix",
        "apply_link": "https://starsevensix.com/careers/data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "TigerData",
        "apply_link": "https://www.tigerdata.com/careers/29788682-de5c-45d8-a0b8-9603c833a8d8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Viasat Careers",
        "apply_link": "https://careers.viasat.com/jobs/4560?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobvite",
        "apply_link": "https://jobs.jobvite.com/uplight/job/o5ilwfwY?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Nike Inc.- Beaverton, OR. Design and build simple reusable components of larger process or framework to support analytics products with mentorship from experienced peers; design and implement product features in collaboration with Business and Technology partners;; clean, prepare and optimize data at scale for ingestion and consumption; support the implementation of new data management projects and re-structure of the current data architecture; implement automated workflows and routines using workflow scheduling tools; understand and use continuous integration, test driven development and production deployment frameworks; Anticipate, identify and solve issues concerning data management to improve data quality. participate in design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards; analyze and profile data for the purpose of designing scalable solutions; and solve straightforward data issues and perform root cause analysis to proactively resolve product issues. Telecommuting is available from anywhere in the U.S., except from AK, AL, AR, DE, HI, IA, ID, IN, KS, KY, LA, MT, ND, NE, NH, NM, NV, OH, OK, RI, SD, VT, WV, and WY.\n\nMust have a Master's degree in Computer Science, Engineering, Electronics Engineering and 2 years of experience in the job offered or in a data-related occupation. Experience must include;\n\n• System development life cycle\n\n• Cloud platforms, such as AWS and Databricks\n\n• SQL\n\n• Version control and CI/CD pipelines\n\n• Extracting, transforming, and loading and data pipelines\n\n• Scripting and automation\n\n• Big data technologies, such as Hadoop, and Spark\n\n• Data Warehouse concepts and methodologies\n\n• Relational and nonrelational database design\n\n• Programming languages, such as Python, Java, and Scala\n\nApply at www.Nike.com/Careers (Job #R-61137)\n\n#LI-DNI\n\nWe offer a number of accommodations to complete our interview process including screen readers, sign language interpreters, accessible and single location for in-person interviews, closed captioning, and other reasonable modifications as needed. If you discover, as you navigate our application process, that you need assistance or an accommodation due to a disability, please complete the Candidate Accommodation Request Form.",
    "job_is_remote": null,
    "job_posted_at": "18 days ago",
    "job_posted_at_timestamp": 1751587200,
    "job_posted_at_datetime_utc": "2025-07-04T00:00:00.000Z",
    "job_location": "Beaverton, OR",
    "job_city": "Beaverton",
    "job_state": "Oregon",
    "job_country": "US",
    "job_latitude": 45.486928299999995,
    "job_longitude": -122.80403199999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DGsDGmxGZgv_H39E5AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Must have a Master's degree in Computer Science, Engineering, Electronics Engineering and 2 years of experience in the job offered or in a data-related occupation",
        "System development life cycle",
        "Cloud platforms, such as AWS and Databricks",
        "SQL",
        "Version control and CI/CD pipelines",
        "Extracting, transforming, and loading and data pipelines",
        "Scripting and automation",
        "Big data technologies, such as Hadoop, and Spark",
        "Data Warehouse concepts and methodologies",
        "Relational and nonrelational database design",
        "Programming languages, such as Python, Java, and Scala"
      ],
      "Responsibilities": [
        "Design and build simple reusable components of larger process or framework to support analytics products with mentorship from experienced peers; design and implement product features in collaboration with Business and Technology partners;; clean, prepare and optimize data at scale for ingestion and consumption; support the implementation of new data management projects and re-structure of the current data architecture; implement automated workflows and routines using workflow scheduling tools; understand and use continuous integration, test driven development and production deployment frameworks; Anticipate, identify and solve issues concerning data management to improve data quality",
        "participate in design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards; analyze and profile data for the purpose of designing scalable solutions; and solve straightforward data issues and perform root cause analysis to proactively resolve product issues"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Nike",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 13011,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Nike/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Nike",
        "score": 4,
        "num_stars": 4,
        "review_count": 16535,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/NIKE-Reviews-E1699.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Nike",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 287,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/nike-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Growth Insights and Foundations",
    "employer_name": "Netflix",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQG6PlRytpbamOkfuuSsdXOdyiJlb7J7TCmmimh&s=0",
    "employer_website": "https://www.netflix.com/",
    "job_publisher": "Remote Rocketship",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Remote Rocketship",
        "apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description:\n• Partner closely with data scientists and other engineers to build low-latency data products\n• Ensure the availability of critical data to enhance \"in-the-moment\" experiences\n• Develop highly available and reliable distributed data systems and services\n• Optimize for the best customer experience with data insights\n• Ensure timely delivery of high-quality data for Netflix product\n\nRequirements:\n• Proficient in at least one major language preferably on the JVM stack (e.g., Java, Scala) and SQL (any variant)\n• Strive to write elegant and maintainable code\n• Comfortable with picking up new technologies\n• Have a product mindset and are curious to understand the business's needs\n• Naturally collaborative style to work with product management, data science, engineering, etc.\n• Strong data intuition and know how to apply analytical skills to support building high quality data products\n• Experience building applications that use large-scale distributed systems and data processing frameworks (batch and real-time)\n• Passionate about making data available for self-service and wider integration\n• Knowledge about transport protocols and building APIs/services and frameworks (e.g. Spring, gRPC)\n• Experience in supporting and maintaining products that run 24x7\n• Can craft scalable systems and solutions to realize a range of product and engineering goals\n• Strong operational awareness and design multi-tenant systems handling high-scale demands\n• Prioritize observability in designs with comprehensive monitoring, logging, and alerting\n• Own what you build and have a passion for quality\n• Comfortable working in agile environments with vague requirements\n• Nimble and can pivot easily when needed\n• Unafraid to take smart risks\n\nBenefits:\n• Health Plans\n• Mental Health support\n• 401(k) Retirement Plan with employer match\n• Stock Option Program\n• Disability Programs\n• Health Savings and Flexible Spending Accounts\n• Family-forming benefits\n• Life and Serious Injury Benefits\n• Paid leave of absence programs\n• Paid time off",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6UkieosGdj3DK2hqAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 170000,
    "job_max_salary": 720000,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Netflix",
        "score": 4,
        "num_stars": 4,
        "review_count": 820,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Netflix/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Netflix",
        "score": 4.2,
        "num_stars": 4,
        "review_count": 3404,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Netflix-Reviews-E11891.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Netflix",
        "score": 4,
        "num_stars": 4,
        "review_count": 43,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/netflix-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer (Full-Time, Remote, North Carolina Based)",
    "employer_name": "Alliance Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQc8LYAEYcIakIeMQVnhkBhH5cN4eBzxq9dkqiN&s=0",
    "employer_website": "https://www.alliancehealthplan.org",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Alliance/Job/Data-Engineer-(Full-Time,-Remote,-North-Carolina-Based)/-in-Morrisville,NC?jid=63c088f9feb31195&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-full-time-remote-north-carolina-based-alliance-health-JV_KO0,51_KE52,67.htm?jl=1009695680692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-full-time-remote-north-carolina-based-at-alliance-health-4256300763?refId=ZZ8HCMbeJKzEURTwY97AHQ%3D%3D&trackingId=Ty0o5ZD5tUD13LeFLBMwKA%3D%3D&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/685c8729de1ebd49c8be30eb-data-engineer-full-time-remote-north-carolina-based?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5274727037?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://vividhireio.com/job/606346?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kinetichires",
        "apply_link": "https://kinetichires.net/job/data-analyst-diversity-equity-inclusion-and-health-equity-full-time-remote-north-carolina-based-264200?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages. The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities.\n\nThis position is fulltime remote. Selected candidate must reside in North Carolina. Some travel for onsite meetings to the Home office at Morrisville may be required.\n\nResponsibilities & Duties\n• Analyze business and technical requirements for the design of data integration solutions\n• Define the overall data integration and dataflow architectures to support data integration projects\n• Design and develop SQL and SSIS processes to support data integration projects\n• Design and develop APIs to consume and distribute healthcare data\n• Design, develop and execute unit testing plans\n• Ensure data quality and integrity in all data integration projects\n• Develop technical and business process documentation for data integration projects\n• Maintain and continually improve data integration projects\n• Assist in establishing standards for the design, development, implementation and support of data integration projects\n• Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders\n• Any other tasks as reasonably required\n\nMinimum Requirements\n\nEducation & Experience:\n• Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position.\n• Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis.\n\nPreferred\n• Bachelor’s degree plus five (5) years of experience in a computer science related field including experience in a data integration or ETL development position including developing complex data integration software applications.\n• Microsoft Certified Solutions Expert, MuleSoft Certified Developer and/or HL7 Certifications.\n\nKnowledge, Skills, & Abilities\n• Expert programming in SQL\n• Proficient designing and developing ETL processes, preferably using SSIS\n• Proficient designing and developing APIs, preferably using .NET Framework\n• Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI\n• Experience working with API management and data integration platforms such as Apigee or MuleSoft\n• Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP\n• Experience working with HIEs and/or HISPs\n• Strong communication and organizational skills\n• Ability to access and analyze large data sets for completeness and quality\n• Ability to work independently and in a team setting\n\nEmployment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date.\n\nSalary Range\n\n$102,424-$130,591/Annually\n\nExact compensation will be determined based on the candidate's education, experience, external market data and consideration of internal equity.\n\nAn excellent fringe benefit package accompanies the salary, which includes:\n• Medical, Dental, Vision, Life, Long Term Disability\n• Generous retirement savings plan\n• Flexible work schedules including hybrid/remote options\n• Paid time off including vacation, sick leave, holiday, management leave\n• Dress flexibility\n\nEqual Opportunity Employer\nThis employer is required to notify all applicants of their rights pursuant to federal employment laws. For further information, please review the Know Your Rights notice from the Department of Labor.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "North Carolina",
    "job_city": null,
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.7595731,
    "job_longitude": -79.01929969999999,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dj0uGbWgrvId1fahdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 102424,
    "job_max_salary": 130591,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Selected candidate must reside in North Carolina",
        "Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position",
        "Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis",
        "Knowledge, Skills, & Abilities",
        "Expert programming in SQL",
        "Proficient designing and developing ETL processes, preferably using SSIS",
        "Proficient designing and developing APIs, preferably using .NET Framework",
        "Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI",
        "Experience working with API management and data integration platforms such as Apigee or MuleSoft",
        "Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP",
        "Experience working with HIEs and/or HISPs",
        "Strong communication and organizational skills",
        "Ability to access and analyze large data sets for completeness and quality",
        "Ability to work independently and in a team setting",
        "Employment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date"
      ],
      "Benefits": [
        "$102,424-$130,591/Annually",
        "An excellent fringe benefit package accompanies the salary, which includes:",
        "Medical, Dental, Vision, Life, Long Term Disability",
        "Generous retirement savings plan",
        "Flexible work schedules including hybrid/remote options",
        "Paid time off including vacation, sick leave, holiday, management leave",
        "Dress flexibility"
      ],
      "Responsibilities": [
        "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages",
        "The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities",
        "This position is fulltime remote",
        "Analyze business and technical requirements for the design of data integration solutions",
        "Define the overall data integration and dataflow architectures to support data integration projects",
        "Design and develop SQL and SSIS processes to support data integration projects",
        "Design and develop APIs to consume and distribute healthcare data",
        "Design, develop and execute unit testing plans",
        "Ensure data quality and integrity in all data integration projects",
        "Develop technical and business process documentation for data integration projects",
        "Maintain and continually improve data integration projects",
        "Assist in establishing standards for the design, development, implementation and support of data integration projects",
        "Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders",
        "Any other tasks as reasonably required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Principal Data Engineer - Remote US",
    "employer_name": "Seamless.AI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS5BfkbQOcbcbdeleTXv86f7_9MAYLP6hfW2oz8&s=0",
    "employer_website": "https://seamless.ai",
    "job_publisher": "Wellfound",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://wellfound.com/jobs/3227170-principal-data-engineer-remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Wellfound",
        "apply_link": "https://wellfound.com/jobs/3227170-principal-data-engineer-remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Seamless.AI/Job/Principal-Data-Engineer-Remote-US/-in-Columbus,OH?jid=d298d1770fe754ee&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=a0a3d5a80823c1a4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/principal-data-engineer-remote-us/4275987?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67c03123b965ace1d468e64f-principal-data-engineer---remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Startup Jobs",
        "apply_link": "https://startup.jobs/principal-data-engineer-remote-us-seamlessai-2-6223376?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-us_04bbb179-c9c8-4960-851f-91caa0a4f220?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://jooble.org/jdp/4971728553806184117?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Opportunity:\n\nAt Seamless.AI, we’re seeking a highly skilled and experienced Principal Data Engineer with expertise in Python, Spark, AWS Glue, and other ETL (Extract, Transform, Load) technologies. The ideal candidate will have a proven track record in data acquisition and transformation, as well as experience working with large data sets and applying methodologies for data matching and aggregation. Strong organizational skills and the ability to work independently as a self-starter are essential for this role.\n\nResponsibilities:\n• Design, develop, and maintain robust and scalable ETL pipelines to acquire, transform, and load data from various sources into our data ecosystem.\n• Collaborate with cross-functional teams to understand data requirements and develop efficient data acquisition and integration strategies.\n• Implement data transformation logic using Python and other relevant programming languages and frameworks.\n• Utilize AWS Glue or similar tools to create and manage ETL jobs, workflows, and data catalogs.\n• Optimize and tune ETL processes for improved performance and scalability, particularly with large data sets.\n• Apply methodologies and techniques for data matching, deduplication, and aggregation to ensure data accuracy and quality.\n• Implement and maintain data governance practices to ensure compliance, data security, and privacy.\n• Collaborate with the data engineering team to explore and adopt new technologies and tools that enhance the efficiency and effectiveness of data processing.\n\nSkillset:\n• Strong proficiency in Python and experience with related libraries and frameworks (e.g., pandas, NumPy, PySpark).\n• Hands-on experience with AWS Glue or similar ETL tools and technologies.\n• Solid understanding of data modeling, data warehousing, and data architecture principles.\n• Expertise in working with large data sets, data lakes, and distributed computing frameworks.\n• Experience developing and training machine learning models.\n• Strong proficiency in SQL.\n• Familiarity with data matching, deduplication, and aggregation methodologies.\n• Experience with data governance, data security, and privacy practices.\n• Strong problem-solving and analytical skills, with the ability to identify and resolve data-related issues.\n• Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.\n• Highly organized and self-motivated, with the ability to manage multiple projects and priorities simultaneously.\n\nEducation and Requirements:\n• Bachelor's degree in Computer Science, Information Systems, related fields or equivalent years of work experience.\n• 7+ years of experience as a Data Engineer, with a focus on ETL processes and data integration.\n• Professional experience with Spark and AWS pipeline development required.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Columbus, OH",
    "job_city": "Columbus",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.9625112,
    "job_longitude": -83.00322179999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D7jGiM9T-GjtGDCWIAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate will have a proven track record in data acquisition and transformation, as well as experience working with large data sets and applying methodologies for data matching and aggregation",
        "Strong organizational skills and the ability to work independently as a self-starter are essential for this role",
        "Strong proficiency in Python and experience with related libraries and frameworks (e.g., pandas, NumPy, PySpark)",
        "Hands-on experience with AWS Glue or similar ETL tools and technologies",
        "Solid understanding of data modeling, data warehousing, and data architecture principles",
        "Expertise in working with large data sets, data lakes, and distributed computing frameworks",
        "Experience developing and training machine learning models",
        "Strong proficiency in SQL",
        "Familiarity with data matching, deduplication, and aggregation methodologies",
        "Experience with data governance, data security, and privacy practices",
        "Strong problem-solving and analytical skills, with the ability to identify and resolve data-related issues",
        "Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams",
        "Highly organized and self-motivated, with the ability to manage multiple projects and priorities simultaneously",
        "Bachelor's degree in Computer Science, Information Systems, related fields or equivalent years of work experience",
        "7+ years of experience as a Data Engineer, with a focus on ETL processes and data integration",
        "Professional experience with Spark and AWS pipeline development required"
      ],
      "Responsibilities": [
        "Design, develop, and maintain robust and scalable ETL pipelines to acquire, transform, and load data from various sources into our data ecosystem",
        "Collaborate with cross-functional teams to understand data requirements and develop efficient data acquisition and integration strategies",
        "Implement data transformation logic using Python and other relevant programming languages and frameworks",
        "Utilize AWS Glue or similar tools to create and manage ETL jobs, workflows, and data catalogs",
        "Optimize and tune ETL processes for improved performance and scalability, particularly with large data sets",
        "Apply methodologies and techniques for data matching, deduplication, and aggregation to ensure data accuracy and quality",
        "Implement and maintain data governance practices to ensure compliance, data security, and privacy",
        "Collaborate with the data engineering team to explore and adopt new technologies and tools that enhance the efficiency and effectiveness of data processing"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Google Cloud Platform Data Engineer(Fulltime) for Remote",
    "employer_name": "Amaze Systems Inc",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ3j6-Rx9U3c7Gzc5qfkYkFeEVFSNq9iCBifDLC&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs™ - DentaQuest",
        "apply_link": "https://dev-cm.dentaquest.com/job/work-from-home-system-data-analyst-google-cloud-platform-data-fn8ov.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://jobxpedia.com/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://buzzcloud.in/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Fulltime opportunity\n\nRole: Google Cloud Platform Data Engineer\n\nLocation: Remote - USA\n\nDuration: FTE only\n\nJob Description:\n• 10+ years' proven experience as a Data Engineer with a focus on Google Cloud Platform services.\n• Strong proficiency in Google Cloud Platform services such as GCS, Dataflow with Apache Beam (Batch & Stream data processing), BigQuery, cloud Composer and Pub/Sub.\n• Proficiency in SQL and Python for data manipulation and analysis is mandatory.\n• Solid understanding of data warehousing concepts and ETL processes.\n\nThanks &Regards\n\nRahul Sharma | Lead Technical Recruiter\nAmaze Systems Inc\n\nE: |",
    "job_is_remote": null,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1752883200,
    "job_posted_at_datetime_utc": "2025-07-19T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVSDv3_uV9S8qF8byAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer 5 - Playback",
    "employer_name": "Netflix",
    "employer_logo": null,
    "employer_website": "https://about.netflix.com",
    "job_publisher": "Careers At Netflix",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At Netflix",
        "apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://netflix.wd1.myworkdayjobs.com/netflix/job/usa---remote/data-engineer-5---playback_jr33081?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=949db8d2e2fdda48&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ShowbizJobs",
        "apply_link": "https://www.showbizjobs.com/jobs/netflix-data-engineer-games-in-los-gatos/jid-23obn2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-5-playback-netflix-JV_KO0,24_KE25,32.htm?jl=1009715302012&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/freelancer-data-engineer-developer-at-norconsulting-global-recruitment-4267468273?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6803ddfc9501a3a17fa1d905-data-engineer-5---playback?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Taro",
        "apply_link": "https://www.jointaro.com/jobs/netflix/data-engineer-5-playback/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\n\nWe are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.\n\nThe Playback pod within the Consumer Engineering Product Data Engineering team partners with client and edge systems to produce playback datasets, a source of truth for Title, Live and Ads performance, title popularity (Netflix top 10), and member quality of experience. This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP. It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network.\n\nThis role is focused on supporting Netflix’s core datasets, delivering high-quality business metrics, and building systems to process batch and real-time data at a large scale. Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques. Since data engineers are responsible for their pipelines at Netflix, this role requires engineers to take ownership of the operational excellence in their domain. In addition, the ideal candidate will have excellent data intuition and share our passion for continuously improving how we handle streaming data at Netflix.\n\nWho are you?\n• You have strong product ownership and good intuition on how data is used to drive business decisions\n• You strive to write elegant code and are comfortable with independently picking up new technologies.\n• You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL.\n• You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models.\n• You have a strong background in at least one of the following: distributed data processing or software engineering of data services.\n• You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets.\n• You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation.\n• At Netflix, you own your code, services, and pipelines. You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain.\n• You have an eye for detail, good data intuition, and a passion for data quality.\n• You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.\n• You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks.\n\nWhat you will do:\n• Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences.\n• Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data\n• Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets.\n• Maintain and rethink existing pipelines to improve scalability and maintainability.\n\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\n\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\n\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\n\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\nJob is open for no less than 7 days and will be removed when the position is filled.",
    "job_is_remote": null,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1752364800,
    "job_posted_at_datetime_utc": "2025-07-13T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DeCMK1KxoFvtFAEKKAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques",
        "You have strong product ownership and good intuition on how data is used to drive business decisions",
        "You strive to write elegant code and are comfortable with independently picking up new technologies",
        "You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL",
        "You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models",
        "You have a strong background in at least one of the following: distributed data processing or software engineering of data services",
        "You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets",
        "You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation",
        "At Netflix, you own your code, services, and pipelines",
        "You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain",
        "You have an eye for detail, good data intuition, and a passion for data quality",
        "You relate to and embody many of the aspects of the Netflix Culture",
        "You love working independently while also collaborating and giving/receiving candid feedback",
        "You are comfortable working in a rapidly changing environment with ambiguous requirements",
        "You are nimble and take intelligent risks"
      ],
      "Benefits": [
        "Our compensation structure consists solely of an annual salary; we do not have bonuses",
        "You choose each year how much of your compensation you want in salary versus stock options",
        "To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range",
        "The range for this role is $170,000 - $720,000",
        "Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits",
        "We also offer paid leave of absence programs",
        "Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off",
        "Full-time salaried employees are immediately entitled to flexible time off"
      ],
      "Responsibilities": [
        "This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP",
        "It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network",
        "Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences",
        "Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data",
        "Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets",
        "Maintain and rethink existing pipelines to improve scalability and maintainability"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - MLOps | Remote in the USA - 1725",
    "employer_name": "PlacingIT",
    "employer_logo": null,
    "employer_website": "https://www.placingit.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - MLOps | Remote in the USA - 1725\n\nLocation: Hybrid - Onsite 4 days a week in Dallas, TX\nEmployment Type 6-12+ months contract w/possible extensions\nInterview: In-Person interview is required for this role.\nResidency Requirements: Those authorized to work in the US are encouraged to apply.\n\nRequired Qualifications\n• 5+ years of software development experience building and delivering customer facing cloud-based analytic solutions.\n• 5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives.\n• 5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope.\n• 5+ years coding in Python/PySpark, SQL, and LLM modeling.\n• 5+ years in cloud-based platforms such as Azure, GCP and AWS.\n\nPreferred Qualifications\n• 5+ years of experience leading the design, development, and deployment of applications on Palantir Foundry Platform\n• Retail merchandising domain expertise including category management, assortment optimization, product clustering, product price sensitivity and promotion affinity evaluation, store clustering and localization.\n• Experience with Prompt Engineering developing solutions leveraging LLM models.\n\nResponsibilities include:\n• You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions.\n• You will solve complex problems and deliver decision support tools to improve customer experience.\n• Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses.\n• Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces.\n• Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses.\n• Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry.\n• Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle.\n• Provide technical guidance, mentorship, and support to junior developers and team members.\n• Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem.",
    "job_is_remote": null,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1752537600,
    "job_posted_at_datetime_utc": "2025-07-15T00:00:00.000Z",
    "job_location": "Dallas, TX",
    "job_city": "Dallas",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7766642,
    "job_longitude": -96.79698789999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRxeJao3yHoNlD18FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Residency Requirements: Those authorized to work in the US are encouraged to apply",
        "5+ years of software development experience building and delivering customer facing cloud-based analytic solutions",
        "5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives",
        "5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope",
        "5+ years coding in Python/PySpark, SQL, and LLM modeling",
        "5+ years in cloud-based platforms such as Azure, GCP and AWS"
      ],
      "Responsibilities": [
        "You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions",
        "You will solve complex problems and deliver decision support tools to improve customer experience",
        "Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses",
        "Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces",
        "Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses",
        "Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry",
        "Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle",
        "Provide technical guidance, mentorship, and support to junior developers and team members",
        "Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": null,
    "job_title": "W2 Contract || Title- Azure Data Engineer in 100% Remote || USC & GC only on W2 || 12 Years Exp.",
    "employer_name": "JS Consulting",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRE1gF8gajTg6maWTnVUROZLV3HcGESVQuhd_J8&s=0",
    "employer_website": "https://www.jsconsultingco.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Note: Candidate must be Work on W2 and USC & GC Visa only.\n\nTitle: Azure Data Engineer (12+ Years Experinece)\n\nLocation: Kansas City, MO 100% Remote\n\nDuration: 6-12+ Months (W2 Contract)\n\nInterview: Video\n\nVisa: USC/GC (must work on our w2) (need strong communication)\n\nJob Description\n\nMake sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager.\n\nDon't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes.\n\nCommunication must be flawless\n\nResume should not be more than 6 pages\n\nMust have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020.\n\nJob Description-\n\nMust have strong Azure, ADF and Databricks experience.\n\nThe purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks. It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes. This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files.\n\nEssential Duties And Responsibilities\n• Work with a highly dynamic team focused on Digital Transformation.\n• Understand the domain and business processes to implement successful data pipelines.\n• Provide work status and coordinate with Data Engineers.\n• Manage customer deliverables and regularly report the status via Weekly/Monthly reviews.\n• Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views\n• Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server.\n• Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot.\n• Design normalized database tables with proper indexing and constraints.\n• Perform SQL query tuning and performance optimization on complex and inefficient queries.\n• Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets.\n• Collaborate with DBA on database design and performance enhancements.\n• Leading in all phases of the software development life cycle in a team environment.\n• Debug existing code and troubleshoot for issues.\n• Design and provide a framework for maintaining existing data warehouse for reporting and data analytics.\n• Follow best practices, design, develop, test and document ETL processes.",
    "job_is_remote": null,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Kansas City, MO",
    "job_city": "Kansas City",
    "job_state": "Missouri",
    "job_country": "US",
    "job_latitude": 39.099726499999996,
    "job_longitude": -94.5785667,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3KUvi58ta8OiVWqPAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Note: Candidate must be Work on W2 and USC & GC Visa only",
        "Title: Azure Data Engineer (12+ Years Experinece)",
        "Visa: USC/GC (must work on our w2) (need strong communication)",
        "Communication must be flawless",
        "Resume should not be more than 6 pages",
        "Must have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020",
        "Must have strong Azure, ADF and Databricks experience",
        "Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server"
      ],
      "Responsibilities": [
        "Make sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager",
        "Don't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes",
        "The purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks",
        "It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes",
        "This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files",
        "Work with a highly dynamic team focused on Digital Transformation",
        "Understand the domain and business processes to implement successful data pipelines",
        "Provide work status and coordinate with Data Engineers",
        "Manage customer deliverables and regularly report the status via Weekly/Monthly reviews",
        "Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views",
        "Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot",
        "Design normalized database tables with proper indexing and constraints",
        "Perform SQL query tuning and performance optimization on complex and inefficient queries",
        "Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets",
        "Collaborate with DBA on database design and performance enhancements",
        "Leading in all phases of the software development life cycle in a team environment",
        "Debug existing code and troubleshoot for issues",
        "Design and provide a framework for maintaining existing data warehouse for reporting and data analytics",
        "Follow best practices, design, develop, test and document ETL processes"
      ]
    },
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineers (Hybrid | DC Area) Remote / Telecommute Jobs",
    "employer_name": "Rackner",
    "employer_logo": null,
    "employer_website": "https://rackner.com",
    "job_publisher": "Security Clearance Jobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Security Clearance Jobs",
        "apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Data Engineer\n\nLocation: Falls Church, VA (Hybrid)\n\nClearance: Secret Clearance\n\nAbout this role:\n\nRackner is looking for a Data Engineer that will be working within an Agile DevSecOps team environment using latest cloud-native technologies to architect and implement containerized applications, CI/CD pipelines, and Kubernetes platforms using best practices and leading technologies.\n\nWe are seeking professionals with:\n\nB.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms\n\nModern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)\n\nDeveloping data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules\n\nMature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR\n\nNice to have:\n\nM.S in Computer Science or related\n\nAI/ML\n\nKubernetes (Rancher RKE2, AWS EKS) and microservice architectures\n\nData engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR\n\nNIST Risk Management Framework and security accreditation process and tasks\n\nWhat will make you successful:\n\nUsing DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer\n\nDeveloping software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes\n\nLeveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs\n\nApplying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders\n\nEmbracing a shared responsibility for system security\n\nPerforming threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle\n\nContinuously engaging with project teams to deliver quality products\n\nParticipating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions\n\nWho We Are:\n\nRackner is a software consultancy that builds cloud-native solutions for startups, enterprises, and the public sector.\n\nWe are an energetic, growing consultancy with a passion for solving big problems for both startups and enterprises.\n\nEach of us enable digital transformation for large organizations through the newest in distributed technologies as we are laser focused on end-to-end application development, DevSecOps, AI/ML and systems architecture and our methodology focuses on cloud-first and cost-effective innovation.\n\nOur customers hail from a diverse, ever-growing list of industries.\n\nBenefits/Additional Info:\n\nRackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided . Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!\n\n401K with 100% matching up to 6%\n\nHighly competitive PTO\n\nGreat health insurance with large network of providers\n\nMedical/Dental/Vision\n\nLife Insurance, and short & long term disability\n\nIndustry-Leading Weekly Pay Schedule\n\nHome office & equipment plan\n\n#DataEngineer #AWS #Topsecret #FDA #publictrust #DataIngesting #DataPipeline #Python #Terraform #ETL #AI #ML #dataintegration #bigdataanalyticspipeline #awsbigdata #hadoop #apachespark #RDBMS #awsdynamoDB #collaboration #diversity #equity #Inclusion",
    "job_is_remote": null,
    "job_posted_at": "12 hours ago",
    "job_posted_at_timestamp": 1753128000,
    "job_posted_at_datetime_utc": "2025-07-21T20:00:00.000Z",
    "job_location": "Falls Church, VA",
    "job_city": "Falls Church",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.882334,
    "job_longitude": -77.1710914,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DL2b_MhwdllUxOf0jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "B.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms",
        "Modern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)",
        "Developing data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules",
        "Mature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR",
        "M.S in Computer Science or related",
        "Kubernetes (Rancher RKE2, AWS EKS) and microservice architectures",
        "Data engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR",
        "NIST Risk Management Framework and security accreditation process and tasks"
      ],
      "Benefits": [
        "Rackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided ",
        "Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!",
        "401K with 100% matching up to 6%",
        "Highly competitive PTO",
        "Great health insurance with large network of providers",
        "Life Insurance, and short & long term disability",
        "Industry-Leading Weekly Pay Schedule",
        "Home office & equipment plan"
      ],
      "Responsibilities": [
        "Using DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer",
        "Developing software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes",
        "Leveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs",
        "Applying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders",
        "Embracing a shared responsibility for system security",
        "Performing threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle",
        "Continuously engaging with project teams to deliver quality products",
        "Participating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Senior Data Engineer - Full remote",
    "employer_name": "All European Careers",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsGyayz3Bns7zN430bb2TN0tCgmnP5cORT61YZ&s=0",
    "employer_website": "https://www.all-european-careers.com",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JOBITT",
        "apply_link": "https://jobitt.com/job-openings/external/senior-engineer-full-stack-7870835690680623803?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: Europe\n\nThis is a remote position.\n\nFor an International Institution in Geneva, we are urgently looking for an experienced Senior Data Engineer preferably with Azure experience in Geneva or Full remote.As Senior Data Engineer, you work with the Cloud Architect, Data Architect, Solution Engineer and other technical professionals to act as technical focal point for the development of prototypes, providing advice and technical feasibility view, and other technical activities to ensure the Cloud data Platform architecture and technical configuration will address business and IT objectives.\n\nCandidates need to be fluent in English. This positions is long-term. Work permit not required.Candidates need to be based in Europe.\n\nTasks and Responsibilities:\n• Plan the activites and concrete milestones to deliver the new Cloud Data Platform Architecture;\n• Lead the development and technical implementation aligned to the Data Platform Architecture;\n• Provide expertise and technical advice in the development of conceptual, logical and physical data models, in support of interoperability protocols, API design and management, cybersecurity aspects and business intelligence;\n• Create the data factory pipelines to ingest data, apply data transformations to curate data using databricks, and make data available for downstream applications (API) or reporting from SQL database, Cosmos DB or Synapse Analytics;\n• Automation and development lifecycle control by development of rbooks, fctions, devops projects, sync to repos, ARM templates and Azure Blueprints;\n• Working with the Senior Solution Engineer derstand and catalogue the current landscape of Data and systems;\n• Develop and maintain the to-be Cloud Data Platform with the required capabilities in conformance with security requirements, agreements-based, with user-driven configuration and through documented design and configuration;\n• Provide technical expertise regarding short terms solution options to leverage in the immediate future to meet urgent data related demands, with consideration of the organization’s wider needs and the potential of deploying enterprise data solution platforms at the enterprise level;\n• Provide data insights and best practices, ensuring they are reflected in the development;\n\nProfile:\n• Bachelor or Master degree;\n• +5 years of relevant experience as Data Engineer;\n• Experience with modern Data technologies such as Business Intelligence, Analytics, AI, and Big Data;\n• Experience programming multiple languages, e.g. Phyton, R, Java, Scala, etc; Professional level vendor certifications, such as Microsoft, ITIL, and others;\n• Extensive experience with Microsoft Azure and other cloud technologies and interoperable solutions,experience as a data analyst, engineer and developer;\n• Demonstrated experience in the design, development, and implementation of various integrations between diverse infrastructure services, data models and architecture;\n• Demonstrated experience with the IT systems development life cycle (SDLC), as well as Agile/Scrum methodologies and ITIL processes;\n• Ability to conduct requirements gathering, interpret needs, and design solutions and manage expectations;\n• Professional experience in technical design and support of global, distributed corporate information systems;\n• Understanding state of Azure, Google and AWS components and reference architectures;\n\nExperience in designing for Bigdata/data warehousing/business intelligence/reporting solutions for various physical environments, both on-premises as well as in the Microsoft cloud, while influencing for the most efficient approach based on business requirements;\n\nExcellent written and spoken English;\n\nInterested: Please send your resume to:\nresume@all-european-careers.com\n\nThis offer from \"All European Careers\" has been enriched by Jobgether.com and got a 77% flex score.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DSjsMVNnmi0YDQr4jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Principal Data Engineer *Remote - Most states eligible*",
    "employer_name": "Providence",
    "employer_logo": null,
    "employer_website": "http://www.psjhealth.org/",
    "job_publisher": "Teal",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the position\n\nThe Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making. This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics. The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St. Joseph Health system, focusing on quality and value of care metrics. The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights.\n\nResponsibilities\n• Collect and analyze clinical data from various sources.\n,\n• Create visualizations and storytelling using data to communicate findings.\n,\n• Develop novel reporting platforms to address healthcare challenges.\n,\n• Collaborate with clinical and administrative leaders to evaluate new measures.\n,\n• Identify impactful findings in large clinical data stores.\n,\n• Design strong data visualizations to communicate key information across the organization.\n,\n• Support team members with experience in computational methods.\n,\n• Develop innovative methods for outcome measures in the whole person care model.\n\nRequirements\n• Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience.\n,\n• 9 years of relevant data analysis experience, preferably in a biomedical setting.\n,\n• Experience with database query and analysis languages (e.g., SQL, R, SAS, Python).\n,\n• Proficiency in data visualization tools (e.g., Tableau, D3).\n\nNice-to-haves\n• Ph.D. in a related field or equivalent experience.\n,\n• Certification in an IT discipline, application, or tool upon hire.\n,\n• Lean certification or Green Belt, Black Belt upon hire.\n,\n• Certification in Data Science upon hire.\n,\n• 4+ years of development experience in data exchange using FHIR.\n\nBenefits\n• 401(k) retirement savings plan with employer matching.\n,\n• Health care benefits (medical, dental, vision).\n,\n• Life insurance.\n,\n• Disability insurance.\n,\n• Paid parental leave.\n,\n• Vacation and holiday time off.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Renton, WA",
    "job_city": "Renton",
    "job_state": "Washington",
    "job_country": "US",
    "job_latitude": 47.4796927,
    "job_longitude": -122.2079218,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXswWukx0iLqtemRoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience",
        "9 years of relevant data analysis experience, preferably in a biomedical setting",
        "Experience with database query and analysis languages (e.g., SQL, R, SAS, Python)",
        "Proficiency in data visualization tools (e.g., Tableau, D3)",
        "Ph.D. in a related field or equivalent experience",
        "Certification in an IT discipline, application, or tool upon hire",
        "Lean certification or Green Belt, Black Belt upon hire",
        "Certification in Data Science upon hire",
        "4+ years of development experience in data exchange using FHIR"
      ],
      "Benefits": [
        "401(k) retirement savings plan with employer matching",
        "Health care benefits (medical, dental, vision)",
        "Life insurance",
        "Disability insurance",
        "Paid parental leave",
        "Vacation and holiday time off"
      ],
      "Responsibilities": [
        "The Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making",
        "This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics",
        "The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St",
        "The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights",
        "Collect and analyze clinical data from various sources",
        "Create visualizations and storytelling using data to communicate findings",
        "Develop novel reporting platforms to address healthcare challenges",
        "Collaborate with clinical and administrative leaders to evaluate new measures",
        "Identify impactful findings in large clinical data stores",
        "Design strong data visualizations to communicate key information across the organization",
        "Support team members with experience in computational methods",
        "Develop innovative methods for outcome measures in the whole person care model"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Providence",
        "score": 3.8,
        "num_stars": 4,
        "review_count": 3699,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Providence-959155fe/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Providence",
        "score": 3.5,
        "num_stars": 3.5,
        "review_count": 6699,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Providence-Reviews-E4651.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer (Remote)",
    "employer_name": "Authority Brands",
    "employer_logo": null,
    "employer_website": "http://www.authoritybrands.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Authority-Brands/Job/Data-Engineer-(Remote)/-in-Columbia,MD?jid=4654298a59de84ef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Authority-Brands/Job/Data-Engineer-(Remote)/-in-Columbia,MD?jid=4654298a59de84ef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-remote-at-authority-brands-4266961170?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Experteer GmbH",
        "apply_link": "https://us.experteer.com/career/view-jobs/data-engineer-remote-columbia-md-usa-52992124?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/QizhCwUPqC5y2cREZFac5D84cSac8fO3fugLwCJXdQCkfBECS4Xrmg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Authority Brands Inc. headquartered in Columbia, MD is a leading provider of home services, building brands that support the success of our franchisees, as well as bettering the lives of the homeowners we serve and the people we employ.\n\nAbout Us: Authority Brands is a leading provider of home service franchises, dedicated to supporting franchise owners with elite marketing, advanced technology, and strong operational support. Founded in 2017, Authority Brands has grown to include 15 industry-leading home service franchisors, providing a wide range of services from plumbing and electrical work to lawn care and pest control.\n\nMission: Our mission is to empower franchise owners to succeed by offering best-in-class training, operational support, and marketing systems. We aim to deliver exceptional home services that customers trust and rely on.\n\nCulture: At Authority Brands, we foster a collaborative and supportive work environment where innovation and excellence are encouraged. We believe in the power of teamwork and are committed to helping our franchise owners achieve their personal and professional goals.\n\nJob Overview: The Data Analytics team at Authority Brands is seeking a skilled Data Engineer with at least 4 years of experience, strong SQL expertise, Python programming skills, and hands-on experience with AWS, including AWS Glue. You will collaborate with our data analytics and development teams to build, maintain, and optimize our data pipelines and infrastructure. We are looking for a candidate who enjoys solving complex data challenges and is eager to contribute to a dynamic team.\n\nKey Responsibilities:\n• Develop and optimize complex SQL queries to ensure efficient data retrieval, manipulation, and reporting.\n• Collaborate with data analysts, data scientists, and cross-functional teams to understand their data requirements and deliver solutions.\n• Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.\n• Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\n• Analyze and solve problems at their root, stepping back to understand the broader context in a timely manner.\n• Design, build, and maintain efficient and reliable data pipelines that support the company's data platform.\n• Leverage Python for ETL processes, automation scripts, and data pipeline development.\n• Utilize AWS services, including S3, RDS, Redshift, Lambda, and AWS Glue, to design and maintain cloud-based data infrastructure.\n• Work with AWS Glue for ETL transformations, data cataloging, and automation of data processes.\n• Monitor and improve the performance of data pipelines and data processing systems.\n\nQualifications:\n• Bachelor's degree in Computer Science, Engineering, or a related field.\n• Minimum of 4 years of experience in data engineering or a related role.\n• Strong proficiency in SQL for querying, optimizing, and managing large datasets.\n• Experience with data modeling and schema design.\n• Solid programming skills in Python, with experience in building and optimizing ETL pipelines.\n• Hands-on experience with AWS cloud services, including data-related tools such as S3, Redshift, RDS, Lambda, and AWS Glue.\n• Familiarity with version control (e.g., Git), CI/CD pipelines, and agile methodologies.\n• Experience working with large datasets and building scalable solutions.\n• Strong problem-solving skills and ability to work independently or within a team.\n• Excellent communication skills with the ability to explain technical concepts to non-technical stakeholders.\n\nWe believe our greatest assets are our employees, we offer competitive salaries and a full benefits package to include, PTO, paid holidays, 401(k) and more.\n\nAuthority Brands Inc. conducts drug screens and background checks on applicants who accept employment offers. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions however we do not sponsor Visa's at this time.\n\nAuthority Brands Inc. is an Equal Opportunity Employer",
    "job_is_remote": null,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Columbia, MD",
    "job_city": "Columbia",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.203714399999996,
    "job_longitude": -76.86104619999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDt3-pBfk2JtUPqzdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Job Overview: The Data Analytics team at Authority Brands is seeking a skilled Data Engineer with at least 4 years of experience, strong SQL expertise, Python programming skills, and hands-on experience with AWS, including AWS Glue",
        "We are looking for a candidate who enjoys solving complex data challenges and is eager to contribute to a dynamic team",
        "Bachelor's degree in Computer Science, Engineering, or a related field",
        "Minimum of 4 years of experience in data engineering or a related role",
        "Strong proficiency in SQL for querying, optimizing, and managing large datasets",
        "Experience with data modeling and schema design",
        "Solid programming skills in Python, with experience in building and optimizing ETL pipelines",
        "Hands-on experience with AWS cloud services, including data-related tools such as S3, Redshift, RDS, Lambda, and AWS Glue",
        "Familiarity with version control (e.g., Git), CI/CD pipelines, and agile methodologies",
        "Experience working with large datasets and building scalable solutions",
        "Strong problem-solving skills and ability to work independently or within a team",
        "Excellent communication skills with the ability to explain technical concepts to non-technical stakeholders",
        "conducts drug screens and background checks on applicants who accept employment offers"
      ],
      "Benefits": [
        "We believe our greatest assets are our employees, we offer competitive salaries and a full benefits package to include, PTO, paid holidays, 401(k) and more"
      ],
      "Responsibilities": [
        "You will collaborate with our data analytics and development teams to build, maintain, and optimize our data pipelines and infrastructure",
        "Develop and optimize complex SQL queries to ensure efficient data retrieval, manipulation, and reporting",
        "Collaborate with data analysts, data scientists, and cross-functional teams to understand their data requirements and deliver solutions",
        "Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc",
        "to drive key business decisions",
        "Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation",
        "Analyze and solve problems at their root, stepping back to understand the broader context in a timely manner",
        "Design, build, and maintain efficient and reliable data pipelines that support the company's data platform",
        "Leverage Python for ETL processes, automation scripts, and data pipeline development",
        "Utilize AWS services, including S3, RDS, Redshift, Lambda, and AWS Glue, to design and maintain cloud-based data infrastructure",
        "Work with AWS Glue for ETL transformations, data cataloging, and automation of data processes",
        "Monitor and improve the performance of data pipelines and data processing systems"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer II",
    "employer_name": "Astrana Health, Inc.",
    "employer_logo": null,
    "employer_website": "http://www.astranahealth.com/",
    "job_publisher": "Careers At - Astrana Health",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.astranahealth.com/postings/eff68530-94e4-421c-b7c2-6fb55c1c3bae?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At - Astrana Health",
        "apply_link": "https://careers.astranahealth.com/postings/eff68530-94e4-421c-b7c2-6fb55c1c3bae?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://hhmi.wd1.myworkdayjobs.com/en-US/External/job/Data-Engineer-II_R-3508?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Cotiviti | Careers Center - ICIMS",
        "apply_link": "https://careers-cotiviti.icims.com/jobs/15601/data-engineer-ai-ii/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/e1c121d0-c400-4d02-8c12-c3bb503ad9b5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=7991269a18c80541&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Welcome To The Jungle",
        "apply_link": "https://www.welcometothejungle.com/fr/companies/kraken/jobs/data-engineer-ii_new-york?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Careers Home - Confluent Health",
        "apply_link": "https://careers.goconfluent.com/corporate/jobs/35194?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobRxiv",
        "apply_link": "https://jobrxiv.org/job/data-engineer-ii/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer II\n\nDepartment: Data - Analytics\n\nEmployment Type: Full Time\n\nLocation: 1668 S. Garfield Ave. 2nd Floor, Alhambra, CA 91801\n\nCompensation: $105,000 - $115,000 / year\n\nDescription\nWe are currently seeking a highly motivated Data Engineer II. This role will report to the Manager of Data Integration and work closely with data analysts, data engineers, data scientists, and clinical leaders to produce deliverables for internal and external clients. With over a million managed lives across the country and terabytes of data generated, our teams need to be continuously equipped with the tools and insights to drive strategy and innovation to further our core values of improving patient outcomes and empowering our providers.\n\nOur Values:\n• Put Patients First\n• Empower Entrepreneurial Provider and Care Teams\n• Operate with Integrity & Excellence\n• Be Innovative\n• Work As One Team\n\nWhat You'll Do\n• Use data engineering best practices to produce high quality, maximally available data models which are intuitive to data analysts and trusted by stakeholders\n• Develop deep domain knowledge in healthcare operations, tracking regulatory developments related to analytics products you maintain\n• Apply quality measures and other metrics to datasets originating from internal and external clients\n• Build scalable ELT pipelines and business intelligence dashboards as needed, embracing automation wherever possible\n• Implement data quality checks which proactively identify data issues and distributional shifts to ensure accuracy of downstream analytical products\n\nQualifications\n• Bachelor's degree required in healthcare, analytics, statistics, finance, business, or related field; Master’s degree (MBA, MPH) preferred.\n• Experience with relational databases.\n• Strong understanding of database structures, theories, principles, and practices\n• Working knowledge with programming or scripting languages such as Python, Spark, and SQL.\n• Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, build processes, testing, and operations.\n• Familiarity with normalized, dimensional, star schema and snowflake schematic models\n• Working experience with Databricks preferred\n• Familiarity with business intelligence exploratory or visualization tools (e.g., Tableau, PowerBI.) preferred\n• Strong written and oral communication skills.\n• Experience with Excel.\n\nYou're a great for this role if:\n• 2+ years of experience working in the data and analytics landscape\n• 2+ years of experience using version control to manage code changes\n• 2+ years of experience in managed care or other healthcare data field preferred\n• 1+ years’ using cloud-based services from AWS, GCP, or Azure\n\nEnvironmental Job Requirements and Working Conditions\n• This position is remotely based in the U.S.\n• The total compensation target pay range for this role is: $105,000 - $115,000. The salary range represents our national target range for this role.\n\nAstrana Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. All employment is decided based on qualifications, merit, and business need. If you require assistance in applying for open positions due to a disability, please email us at humanresourcesdept@astranahealth.com to request an accommodation.\n\nAdditional Information:\nThe job description does not constitute an employment agreement between the employer and employee and is subject to change by the employer as the needs of the employer and requirements of the job change.\n\n#LI-remote",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Alhambra, CA",
    "job_city": "Alhambra",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.092754899999996,
    "job_longitude": -118.1268211,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Debx6kTHusl1EdZhFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 105000,
    "job_max_salary": 115000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Experience with relational databases",
        "Strong understanding of database structures, theories, principles, and practices",
        "Working knowledge with programming or scripting languages such as Python, Spark, and SQL",
        "Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, build processes, testing, and operations",
        "Familiarity with normalized, dimensional, star schema and snowflake schematic models",
        "Strong written and oral communication skills",
        "Experience with Excel",
        "2+ years of experience working in the data and analytics landscape",
        "2+ years of experience using version control to manage code changes",
        "1+ years’ using cloud-based services from AWS, GCP, or Azure",
        "This position is remotely based in the U.S"
      ],
      "Benefits": [
        "Compensation: $105,000 - $115,000 / year",
        "The total compensation target pay range for this role is: $105,000 - $115,000"
      ],
      "Responsibilities": [
        "This role will report to the Manager of Data Integration and work closely with data analysts, data engineers, data scientists, and clinical leaders to produce deliverables for internal and external clients",
        "Empower Entrepreneurial Provider and Care Teams",
        "Operate with Integrity & Excellence",
        "Use data engineering best practices to produce high quality, maximally available data models which are intuitive to data analysts and trusted by stakeholders",
        "Develop deep domain knowledge in healthcare operations, tracking regulatory developments related to analytics products you maintain",
        "Apply quality measures and other metrics to datasets originating from internal and external clients",
        "Build scalable ELT pipelines and business intelligence dashboards as needed, embracing automation wherever possible",
        "Implement data quality checks which proactively identify data issues and distributional shifts to ensure accuracy of downstream analytical products"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Astrana Health, Inc.",
        "score": 2.3,
        "num_stars": 2.5,
        "review_count": 4,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Astrana-Health-1/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Astrana Health, Inc.",
        "score": 3,
        "num_stars": 3,
        "review_count": 57,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Astrana-Health-Reviews-E5891272.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Senior Data Engineer",
    "employer_name": "Manulife",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQvZIXUr-_I4BE0Gx8s1feFU4YCLk_uQnQrkAC9&s=0",
    "employer_website": "http://www.manulife.com/",
    "job_publisher": "Manulife Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.manulife.com/global/en/job/JR25011423/Senior-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Manulife Careers",
        "apply_link": "https://careers.manulife.com/global/en/job/JR25011423/Senior-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/senior-data-engineer/4834642?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Philippines",
        "apply_link": "https://ph.linkedin.com/jobs/view/senior-data-engineer-at-manulife-4265089535?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "DEjobs.org",
        "apply_link": "https://dejobs.org/quezon-city-phl/senior-data-engineer/0049DDD1BD044C388C36AB75E6E8D578/job/?vs=28&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://ph.jooble.org/jdp/-3266237644944307244?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/philippines/job/full-time/others/senior-data-engineer-139271256?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs Trabajo.org",
        "apply_link": "https://ph.trabajo.org/job-3165-8815421647ebd7c5701dfb4e0f347a4b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/senior-data-engineer-quezon-city-national-capital-region-ph-manulife-jr25041427?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Senior Data Engineer position at Manulife presents a remarkable opportunity to significantly contribute to our goal of becoming a customer-centric digital leader. This role is pivotal in designing, building, and maintaining innovative data solutions that align with our strategic priorities, ensuring effective data collection, processing, and analysis. By joining our team, you will work with cutting-edge technologies, collaborate with a global network of industry experts, and benefit from comprehensive training resources within a supportive and inclusive environment.\n\nPosition Responsibilities:\n• Provide hands-on data engineering and technical oversight in developing batch and real-time data pipelines, DevOps pipelines, and end-to-end processes.\n• Lead the agile software development process, including roadmap planning, cross-team coordination, and task assignments.\n• Translate business requirements into technical solutions, collaborating closely with development teams, architects, and business analysts.\n• Design and build modern data pipelines, data streams, and data service APIs using Azure cloud data technologies.\n• Mentor other staff members and contribute to knowledge transfer across teams and departments.\n• Work Arrangement: Hybrid Work Set-Up\n• Schedule: Rotating shift\n\nRequired Qualifications:\n• At least 5-6 years of professional experience in designing and developing data warehouses and ETL tools and technologies.\n• Strong experience as an Azure Data Engineer, with expertise in Azure Databricks.\n• Bachelor's degree in a relevant field.\n• No licenses required.\n\nPreferred Qualifications:\n• Expert understanding of Azure Data Lake, Azure SQL databases, Azure Data Factory, and Azure Databricks.\n• Hands-on experience in designing and developing scripts for ETL processes and automation with Azure Data Factory, Azure Databricks, and PySpark.\n• Experience with the Hadoop ecosystem and toolset, including HDFS, MapReduce, Spark, Python, Scala, Hive, and Oozie.\n• Experience with CI/CD tools, such as GitHub, Jenkins, and Azure DevOps.\n• Knowledge of BI tools, such as PowerBI.\n• Strong interpersonal, written, and oral communication skills.\n• Ability to adapt to new service needs and a rapidly changing environment.\n\nWhen you join our team:\n• We’ll empower you to learn and grow the career you want.\n• We’ll recognize and support you in a flexible environment where well-being and inclusion are more than just words.\n• As part of our global team, we’ll support you in shaping the future you want to see.\n\nAbout Manulife and John Hancock\n\nManulife Financial Corporation is a leading international financial services provider, helping people make their decisions easier and lives better. To learn more about us, visit https://www.manulife.com/en/about/our-story.html.\n\nManulife is an Equal Opportunity Employer\n\nAt Manulife/John Hancock, we embrace our diversity. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention, advancement and compensation, and we administer all of our practices and programs without discrimination on the basis of race, ancestry, place of origin, colour, ethnic origin, citizenship, religion or religious beliefs, creed, sex (including pregnancy and pregnancy-related conditions), sexual orientation, genetic characteristics, veteran status, gender identity, gender expression, age, marital status, family status, disability, or any other ground protected by applicable law.\n\nIt is our priority to remove barriers to provide equal access to employment. A Human Resources representative will work with applicants who request a reasonable accommodation during the application process. All information shared during the accommodation request process will be stored and used in a manner that is consistent with applicable laws and Manulife/John Hancock policies. To request a reasonable accommodation in the application process, contact recruitment@manulife.com.\n\nWorking Arrangement\n\nHybrid",
    "job_is_remote": null,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1752019200,
    "job_posted_at_datetime_utc": "2025-07-09T00:00:00.000Z",
    "job_location": "Quezon City, Metro Manila, Philippines",
    "job_city": "Quezon City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.6487853,
    "job_longitude": 121.0509385,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3Xcxv18vCFCCxy4SAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Manulife",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 6026,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Manulife-Reviews-E9373.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "Manulife",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 1801,
        "max_score": 5,
        "reviews_link": "https://ca.indeed.com/cmp/Manulife/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Manulife",
        "score": 4.3,
        "num_stars": 4.5,
        "review_count": 5,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/manulife-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer",
    "employer_name": "ING",
    "employer_logo": null,
    "employer_website": "http://www.ing.com/",
    "job_publisher": "ING's Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.ing.com/en/job/city-of-taguig/data-engineer/3121/26099673984?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ING's Careers",
        "apply_link": "https://careers.ing.com/en/job/city-of-taguig/data-engineer/3121/26099673984?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs At Atos",
        "apply_link": "https://jobs.atos.net/job/Taguig-City-DATA-ENGINEER/1223031701/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=d86f775779e743db&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Focus Global Inc",
        "apply_link": "https://focusglobalinc.com/jobs/data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kalibrr",
        "apply_link": "https://www.kalibrr.com/c/yondu-inc/jobs/255869/data-engineer-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85317301?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-dapl-it-services-JV_IC2340354_KO0,13_KE14,30.htm?jl=1009695476276&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs By Workable",
        "apply_link": "https://apply.workable.com/octal-philippines-inc-4/j/E2D6A08BFE?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Data Engineer we are currently looking for is someone who:\n\n· Has more than 4 years’ experience within ETL IBM DataStage tool.\n\n· Has technical knowledge in Databases, ETL Processes & BI Reporting.\n\n· Is an expert in Unix scripting, PL/SQL and Data Modelling for Informational platforms.\n\n· Has knowledge & experience in Azure, and other cloud environments.\n\n· Has good analytical skills, with proven domain experience in data warehousing within Bank and Finance Industry.\n\n· Has working experience in Agile environments.\n\n· Has clear communications skills as well as a high English level (written and oral).\n\n· Is able to work without instructions and to take initiative.\n\n· Makes things happen thanks to his/her ‘can do’ attitude.\n\n· Has a problem solving mindset.\n\n· Is a team player (daily connection with other departments is expected)\n\n· Has the ability to work on an international level.\n\n· Takes the initiative for improving processes, ensuring creative ideas are implemented.\n\n· Is a leader of our Orange Code culture, displaying a positive attitude, commitment towards our goals, accountable, eager to learn and is not afraid of change.\n\nConsidering the software you may use as a Data Software Engineer, specific experience would be desirable for the following:\n\n· Oracle (19c).\n\n· IBM Infosphere Information Server Suite or similar Data Processing tools\n\n· IBM Cognos, Power BI and/or MicroStrategy or similar BI tools\n\n· Azure Devops or other devops cloud service\n\n· Security, monitoring and logging skills (e.g. ELK, Prometheus, Grafana)\n\n· Python\n\n· Elastic Cloud Storage (ECS)",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Taguig, Metro Manila, Philippines",
    "job_city": "Taguig",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.530631699999999,
    "job_longitude": 121.0575482,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dev9B9WLM3AqUvc4XAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "ING",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 6212,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/ING-Reviews-E4264.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "ING",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 2182,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Ing/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "ING",
        "score": 4.4,
        "num_stars": 4.5,
        "review_count": 20,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/ing-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer",
    "employer_name": "Michael Page Philippines",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTtL8Q36ioUybwIGJT__ggJHyoAzcoshQE_bGNe&s=0",
    "employer_website": "https://www.michaelpage.com.ph",
    "job_publisher": "Michael Page Philippines",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.michaelpage.com.ph/job-detail/data-engineer/ref/jn-112024-6593349?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Michael Page Philippines",
        "apply_link": "https://www.michaelpage.com.ph/job-detail/data-engineer/ref/jn-112024-6593349?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://worldvision.wd1.myworkdayjobs.com/en-US/WorldVisionInternational/job/Philippines---Home-Working/Data-Engineer_JR43497?jobFamilyGroup=9953ea81b11801011a0d47c420530000&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=4f4ae0a5337acf31&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Asia Select Inc.",
        "apply_link": "https://careers.asiaselect.ph/job/44264?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-iscale-solutions-inc-JV_KO0,13_KE14,34.htm?jl=1009782390416&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/data-engineer/4427293?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Sourcefit",
        "apply_link": "https://sourcefit.breezy.hr/p/92d5ab1d1a87-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/674dda59b10568bbc7b65291-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "• Seeking of a stimulating workplace that fosters growth and innovation\n• Global training opportunities\n\nAbout Our Client\n\nThe client is a Fortune 500 energy company that has a strong presence in the Philippines as one of the country's top employers, recognized for their commitment to diversity, inclusion, and employee development\n\nJob Description\n• Design and develop ETLprocessesETLprocesses using Azure services (Data Factory, Synapse, DatabricksDatabricks, Fabric)\n• Manage data storage solutions using Azure Data Lake Storage Gen 2 & Blob storage\n• OptimizeOptimize data pipelines for performance, scalabilityscalability, and reliability\n• Implement data validation techniques and quality frameworks\n• Manage CI/CD processes for data solutions\n• Collaborate with data scientists, analysts, and architects\n• Create and maintain technical documentation\n• Monitor and troubleshoot pipeline issues\n• Stay current with emerging technologies and industry trends\n\nThe Successful Applicant\n\nMust-have:\n• Bachelor's degree in Computer Science, Engineering, or related field\n• 3+ years experience as a Data Engineer\n• Strong expertise in Microsoft Azure services\n• Proficient in SQL, DML and modern RDBMS\n• RDBMS Experience with software engineering principles (CI/CD, version control)\n• Knowledge of big data technologies (e.g., Spark)\n• Strong problem-solving skills\n\nPreferred:\n• Python programming skills (or Scala,Scala Java, C#)\n• Experience with PySpark and file formats (Parquet, Delta, Avro)\n• Knowledge of Azure DevOps pipelines and Git workflows\n• workflowsUnderstanding of Ansible\n• AnsibleAPI integration experience\n• Learning agility and technical leadership capabilities\n\nWhat's on Offer\n• Competitive salary package\n• Hybrid work model with work-from-home flexibility\n• Comprehensive health care coverage (including dependents)\n• Life insurance\n• Career development opportunities through training and mentoring\n• Opportunity to work with cutting-edge technologies",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D2fW_NLNaKzL_7uOnAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer (Azure & On Prem)",
    "employer_name": "Lingaro",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS4XfQEqzeBNUCL9Js9FMkJA2lCLmSpTDp4Ago6&s=0",
    "employer_website": "http://www.lingarogroup.com/",
    "job_publisher": "LinkedIn Philippines",
    "job_employment_type": null,
    "job_employment_types": [],
    "job_apply_link": "https://ph.linkedin.com/jobs/view/data-engineer-azure-on-prem-at-lingaro-4270215602?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Philippines",
        "apply_link": "https://ph.linkedin.com/jobs/view/data-engineer-azure-on-prem-at-lingaro-4270215602?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Task\n• Designing and implementing data processing systems using distributed frameworks like Hadoop, Spark, Snowflake, Airflow, or other similar technologies. This involves writing efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data.\n• Building data pipelines to ingest data from various sources such as databases, APIs, or streaming platforms. Integrating and transforming data to ensure its compatibility with the target data model or format.\n• Designing and optimizing data storage architectures, including data lakes, data warehouses, or distributed file systems. Implementing techniques like partitioning, compression, or indexing to optimize data storage and retrieval. Identifying and resolving bottlenecks, tuning queries, and implementing caching strategies to enhance data retrieval speed and overall system efficiency.\n• Designing and implementing data models that support efficient data storage, retrieval, and analysis. Collaborating with data scientists and analysts to understand their requirements and provide them with well-structured and optimized data for analysis and modeling purposes.\n• Utilizing frameworks like Hadoop or Spark to perform distributed computing tasks, such as parallel processing, distributed data processing, or machine learning algorithms\n• Implementing security measures to protect sensitive data and ensuring compliance with data privacy regulations. Establishing data governance practices to maintain data integrity, quality, and consistency.\n• Identifying and resolving issues related to data processing, storage, or infrastructure. Monitoring system performance, identifying anomalies, and conducting root cause analysis to ensure smooth and uninterrupted data operations.\n• Collaborating with cross-functional teams including data scientists, analysts, and business stakeholders to understand their requirements and provide technical solutions. Communicating complex technical concepts to non-technical stakeholders in a clear and concise manner.\n• Independence and responsibility for delivering a solution\n• Ability to work under Agile and Scrum development methodologies\n• Staying updated with emerging technologies, tools, and techniques in the field of big data engineering. Exploring and recommending new technologies to enhance data processing, storage, and analysis capabilities.\n• Train and mentor junior data engineers, providing guidance and knowledge transfer.\n\nRequirements:\n• A bachelor's or master's degree in Computer Science, Information Systems, or a related field is typically required.\n• Work commercial experience as a Data Engineer or a similar role.\n• Proficiency in programming languages such as Python, R or Scala is essential.\n• In-depth knowledge and experience with distributed systems and technologies, including On-prem Platforms, Apache Hadoop, Spark, Hive or similar frameworks. Familiarity with cloud-based platforms like AWS, Azure, or Google Cloud is highly desirable.\n• Solid understanding of data processing techniques such as batch processing, real-time streaming, and data integration. Experience with data analytics tools and frameworks like Apache Kafka, Apache Flink, or Apache Storm is a plus.\n• Proficiency in working with relational and non-relational databases such as MSSQL, MySQL, PostgreSQL or Cassandra. Knowledge of data warehousing concepts and technologies like Redshift, Snowflake, or BigQuery is beneficial.\n• Good knowledge of data storage architectures, including delta lakes, data warehouses, or distributed file systems\n• Experience in designing and building data pipelines (ELT/ETL) for large-scale datasets. Familiarity with tools like Databricks, Apache Nifi, Apache Airflow, or Informatica is advantageous. Experience with integration of data from multiple data sources.\n• Nice to have knowledge of data warehousing concepts and technologies like Synapse, Redshift, Snowflake, or BigQuery. Experience with MS Fabric is a plus.\n• Strong understanding of distributed computing principles, including parallel processing, data partitioning, and fault-tolerance.\n• Proficient in data modeling techniques and database optimization. Knowledge of query optimization, indexing, and performance tuning is necessary for efficient data retrieval and processing.\n• Understanding of data security best practices and experience implementing data governance policies. Familiarity with data privacy regulations and compliance standards is a plus.\n• Strong problem-solving abilities to identify and resolve issues related to data processing, storage, or infrastructure. Analytical mindset to analyze and interpret complex datasets for meaningful insights.\n• Knowledge of data orchestration tool will be beneficial\n• Experience in designing and creating integration and unit test will be beneficial.\n• Excellent communication skills to effectively collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders. Ability to convey technical concepts to non-technical stakeholders in a clear and concise manner.\n• A passion for staying updated with emerging technologies and industry trends in the field of big data engineering. Willingness to learn and adapt to new tools and techniques to enhance data processing, storage, and analysis capabilities.\n• Additional certifications in big data technologies or cloud platforms are advantageous\n\nOffer\n• Stable employment. On the market since 2008, 1200+ talents currently on board in 7 global sites.\n• “Office as an option” model. You can choose to work remotely or in the office.\n• Flexibility regarding working hours and your preferred form of contract.\n• Comprehensive online onboarding program with a “Buddy” from day 1.\n• Cooperation with top-tier engineers and experts.\n• Certificate training programs. Lingarians earn 500+ technology certificates yearly.\n• Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.\n• Grow as we grow as a company. 76% of our managers are internal promotions.\n• A diverse, inclusive, and values-driven community.\n• Autonomy to choose the way you work. We trust your ideas.\n• Create our community together. Refer your friends to receive bonuses.\n• Activities to support your well-being and health.\n• Plenty of opportunities to donate to charities and support the environment.\n• Modern office equipment. Purchased for you or available to borrow, depending on your location.\n• Great Place to Work Certified Employer in the Philippines",
    "job_is_remote": null,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Philippines",
    "job_city": null,
    "job_state": null,
    "job_country": "PH",
    "job_latitude": 12.879721,
    "job_longitude": 121.774017,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6tnO5hV6ICVJHf55AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Lingaro",
        "score": 4,
        "num_stars": 4,
        "review_count": 223,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Lingaro-Reviews-E676785.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Lingaro",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 22,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/lingaro-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed Job Search",
        "employer_name": "Lingaro",
        "score": 3.7,
        "num_stars": 3.5,
        "review_count": 6,
        "max_score": 5,
        "reviews_link": "https://ph.indeed.com/cmp/Lingaro/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Analyst / Data Engineer (Mid - Senior Level)",
    "employer_name": "Nityo Infotech Services Philippines Inc.",
    "employer_logo": null,
    "employer_website": "http://www.nityo.com/",
    "job_publisher": "Jobstreet",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.jobstreet.com/job/85890418?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85890418?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobted.com.ph",
        "apply_link": "https://www.jobted.com.ph/job/100ad61381178e19250628793e5ab2dc?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We're Hiring! Data Roles at All Levels | Join a Dynamic and Innovative Tech Team\n\nWe’re on the lookout for passionate and highly skilled Data Professionals to join our team and help drive next-generation data-driven solutions. Whether you're an early-career data enthusiast or a senior cloud expert, we have a spot for you!\nOPEN POSITIONS\n1. Data Engineer\n\nLocation: Bagumbayan, Quezon City\nSchedule: Monday – Friday | Day Shift\nSetup: Onsite\nExperience: 1–2 years\nRate: Open\n\nQualifications:\n• Solid hands-on experience in Databricks, AWS, and Python\n• Strong understanding of data pipelines and modern data stack\n• Experience in implementing ETL workflows and working with cloud-native tools\n\n2. Data Analyst – Power BI\n\nLocation: Makati - Hybrid\nSchedule: Monday – Friday | Morning Shift\nExperience: 3-5 years\nRate: Open\n\nQualifications:\n• Strong background in data analysis and visualization using Power BI\n• Proven ability to translate data into meaningful business insights\n• Proficiency in SQL and DAX is a plus\n\n3. Senior / Principal Azure Data Engineer\n\nLocation: Makati - Hybrid\nSchedule: Monday – Friday | Morning Shift\nExperience: 3-5 years\nRate: Open\n\nQualifications:\n• Extensive experience with Microsoft Azure Data Services:\n• Azure App Services, Azure Databricks, Azure Functions, Azure Logic Apps\n• Azure Synapse Analytics, Azure Cosmos DB\n• Expertise in architecting and developing scalable, cloud-based data solutions\n• Proven track record of leading data projects in a senior or principal capacity\n\n4. Data Engineer (Mid to Senior Level)\n\nLocation: Makati or Ortigas\nSetup: Hybrid (2 days onsite, 3 days WFH)\nSchedule: Morning Shift\nSalary: Up to Php 200,000\n\nQualifications:\n• Proven experience as a Data Engineer or similar role with a focus on data pipelines and ETL processes\n• Strong knowledge of Microsoft Azure services:\n• Azure Data Factory, Azure Synapse, Azure Databricks\n• Azure Blob Storage, Azure Data Lake Gen 2\n• Proficient in writing efficient SQL queries (SQL Server, PostgreSQL)\n• Understanding of Software Engineering principles applied to Data Engineering (e.g., CI/CD, version control, testing)\n• Experience working with big data technologies like Apache Spark\n\nWhy Join Us?\n• Work with leading technologies and cloud platforms\n• Hybrid work options available for most roles\n• Competitive compensation and open rate offers\n• Opportunity to grow within a tech-forward environment",
    "job_is_remote": null,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Manila, Metro Manila, Philippines",
    "job_city": "Manila",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.5995133,
    "job_longitude": 120.984234,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dhfv80ygn_76dxUXKAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - IoT Cost-Optimisation",
    "employer_name": "ERNI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQSfKTOmSX7ecDxCqDh9hvZEHHjEMcT0x20ssDg&s=0",
    "employer_website": null,
    "job_publisher": "Indeed Job Search",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.indeed.com/viewjob?jk=cb26d5bce7ad9aaf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=cb26d5bce7ad9aaf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-iot-cost-optimisation-erni-JV_IC4747401_KO0,35_KE36,40.htm?jl=1009812396511&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6877f38f57fb149ae371f879-data-engineer---iot-cost-optimisation?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Rocketship",
        "apply_link": "https://www.remoterocketship.com/company/betterask-erni/jobs/data-engineer-iot-cost-optimisation-mandaluyong-city-hybrid?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://ph.jooble.org/jdp/-9021195257779167983?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/data-engineer-iot-cost-optimisation-jobs/EA2B195CC497D9C9?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the position\n\nFounded in 1994 and headquartered in Switzerland, ERNI is a leading Software Development company with over 800 employees worldwide. Specializing in IT and software engineering, we drive innovation in process and technology. Our first service center in Asia Pacific, located in Metro Manila (Mandaluyong), supports clients across Europe, APAC, the Philippines, and the USA. As we continue to grow, we're looking for passionate and motivated individuals to join our team.\n\nWhy ERNI is the Perfect Place for You:\n• International Exposure: Work with global clients on cutting-edge projects.\n• Inclusive Culture: Thrive in a collaborative and diverse work environment.\n• Career Development: Enjoy continuous learning and professional growth opportunities.\n\nPerks and Benefits:\n• Career Stability: Enjoy a stable career path with ample project opportunities.\n• Skill Enhancement: Access free training and certifications.\n• Baby Basket: To welcome your newborn to the ERNI family.\n• Fruit Basket: Boost of vitamins during hospitalization.\n• Office Perks: Enjoy free snacks and coffee.\n\nGrowth and Opportunities:\n• Free Training: Advance your skills through technical and non-technical training.\n• Challenging Projects: Engage in complex software projects across MedTech, Industry, Finance, and Transportation.\n• Supportive Environment: Benefit from a team dedicated to guiding and supporting your success.\n• Recognition and Advancement: Receive acknowledgment for your efforts and opportunities for promotion.\n• Open Communication: Experience transparency and value your input in our culture.\n• Flexibility:\n• Hybrid Work Setup: Balance remote and in-person work for better work-life integration.\n\nEvents:\n• Connect and Celebrate: Participate in a variety of events including leisure, summer, family, social, and year-end gatherings.\n\nWhat are our wishes:\n• 3+ years building data solutions on Microsoft Azure (PaaS & serverless) – most of the following Azure services: IoT Hub, Event Hub, Stream Analytics, Functions, Cosmos DB, Databricks.\n• Solid SQL and C# knowledge.\n• Experience squeezing data size/frequency in high-throughput IoT or event-driven systems.\n• Fluent English; clear, pragmatic communication.\n\nNice-to-have\n• MQTT / OPC-UA or other device protocols; edge-computing toolchains.\n• Power BI, Kusto / ADX, or Grafana Loki for observability.\n• Experience with IoT solutions.\n\nEngagement details:\n• Timeline: ASAP 30 Sep 2025 (approx. 50–60 person-days).\n• Location: Remote first; on-site Düsseldorf workshops ad-hoc.\n• Working in a small agile team: Principal IoT Consultant, KPI Governance Lead, Azure Architect, Embedded developer.\n• Outcome: Clear cost breakdown and optimisation roadmap for a big IoT system based on Azure.\n\nThe team is mixed from Germany, Switzerland and Spain. And most of the work can be done remotely.\n\nHow can you contribute to the team?\n• Design & run Azure-native data pipelines that collect, transform and store telemetry at the lowest safe cost.\n• Profile data volumes and flows from gateway to cloud, spot “chatty” patterns and propose compression, batching or edge-processing fixes.\n• Build the active cost-monitoring stack (e.g. Cost Management API + Power BI / Grafana) and KPI dashboards demanded by the project deliverables.\n• Feed your findings into the Health-Check Report and the cost-reduction backlog; quantify €-savings vs. engineering effort.\n\nEmployment Type: Project-based (possibility to work onsite in Germany for 1 month)\n\nSwitzerland · Germany · Spain · Slovakia · Romania · Philippines · Singapore · USA\n\nERNI Development Center Philippines Inc., 9th Floor, Lica Malls Shaw, 500 Shaw Boulevard, 1555, Mandaluyong City, Philippines\n\n+63 5310 1707 | www.betterask.erni | [email protected]",
    "job_is_remote": null,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1752537600,
    "job_posted_at_datetime_utc": "2025-07-15T00:00:00.000Z",
    "job_location": "Mandaluyong City, Metro Manila, Philippines",
    "job_city": "Mandaluyong City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.576267999999999,
    "job_longitude": 121.03924359999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DdiLjnls6cNsN2moZAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer Informatica",
    "employer_name": "Nezda Technologies, Inc.",
    "employer_logo": null,
    "employer_website": "http://www.nezdaglobal.com/",
    "job_publisher": "Jobstreet",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.jobstreet.com/job/85907725?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85907725?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "• Design and develop scalable and efficient ETL solutions using Informatica tools.\n• Extract, transform, and load data from a variety of sources to enterprise data warehouses.\n• Collaborate with business and technical stakeholders to gather requirements and translate them into ETL processes.\n• Participate in the full project lifecycle: requirements gathering, design, development, testing, deployment, and maintenance.\n• Optimize existing ETL workflows for performance and reliability.\n• Ensure data quality, consistency, and integrity across systems.\n• Support production deployments and resolve data integration issues in a timely manner.",
    "job_is_remote": null,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Quezon City, Metro Manila, Philippines",
    "job_city": "Quezon City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.6487853,
    "job_longitude": 121.0509385,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DkceiJkBPTHGOW4nYAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Nezda Technologies, Inc.",
        "score": 3.3,
        "num_stars": 3.5,
        "review_count": 157,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Nezda-Technologies-Reviews-E1595889.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Nezda Technologies, Inc.",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 3,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/overview/nezda-technologies-overview?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Senior Data Engineer",
    "employer_name": "ING",
    "employer_logo": null,
    "employer_website": "http://www.ing.com/",
    "job_publisher": "ING's Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.ing.com/en/job/makati-city/senior-data-engineer/3121/25683897664?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ING's Careers",
        "apply_link": "https://careers.ing.com/en/job/makati-city/senior-data-engineer/3121/25683897664?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=851d3e5f5a203430&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85746384?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/senior-data-engineer-ing-JV_IC4778930_KO0,20_KE21,24.htm?jl=1009760187122&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jora",
        "apply_link": "https://ph.jora.com/job/Data-Engineer-acd369b1fbb11d01bb9ee353504ea8da?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "MyCareersDB",
        "apply_link": "https://mycareersdb.com/jobs/view/10986/Senior-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Key Responsibilities\n• Implement and maintain data workflows, ETL / ELT processes, and pipelines to move identified data into target platforms for further consumption and integration as required by product owners.\n• Design and implement identified data processes and storage such as usage of databases, data warehouses, data marts, and data lakes with data security in mind.\n• Build supporting tooling and procedures needed for data movement and monitoring activities.\n• Implement available automations for data movement processes.\n• Ensure delivered workflows and processes meet expected quality outputs through conducting testing.\n• Document specs and implementations of deliverables mentioned.\n• Work with operations teams for maintenance and continuous improvement of workflows and pipelines.\n• Work on distributed Data processing and database with Distribution & partitioning\n• Able to convert LDM to PDM in Data-vault Data warehouse.\n• Ability to perform tasks related to end-to-end Data Journey including Reporting.\n• Ensure integrated data pipeline validation is performed.\n\nKey Capabilities/Experience\n• Expertise in one/ some of ETL tools like ADF, Airflow.\n• Expertise in SQL / RDBMS databases, handling of structured data.\n• Extensive experience in design and implementation of data handling processes (ingestion, transformation, modelling, and storage)\n• Experience in creating of Big Data warehouses, its design, and its various implementation methods.\n• Flexible and willing to learn various ETL / data integration tools and products.\n• Strong problem solving and solutions engineering mindset.\n• Experience in working and coordinating remotely with teams and stakeholders.\n• Experience with data integration and ETL technologies.\n• Experienced with software version control technologies such as Git.\n• Experience in CI / CD and DevOps is a plus.\n• Experience in Agile way of working is a plus.\n\nMinimum Qualifications\n• Expertise in one/ some of ETL tools like ADF, Airflow.\n• Expertise in SQL / RDBMS databases, handling of structured data.\n• Extensive experience in design and implementation of data handling processes (ingestion, transformation, modelling, and storage)\n• Experience in creating of Big Data warehouses, its design, and its various implementation methods.\n\nNice to have\n• Proficiency and experience in programming languages such as Python, shell scripting\n• Experience in cloud platforms and technologies (Azure, AWS, GCP) is a plus.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DHMkxh5yn4utQb_2yAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "ING",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 6212,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/ING-Reviews-E4264.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "ING",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 2182,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Ing/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "ING",
        "score": 4.4,
        "num_stars": 4.5,
        "review_count": 20,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/ing-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Lead Data Engineer",
    "employer_name": "Nimbyx Philippines, Inc.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTDDVs7gmLle4Indp5lizdEap-2dO0VSaKARkM9&s=0",
    "employer_website": null,
    "job_publisher": "Smart Recruiters Jobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.smartrecruiters.com/NimbyxPhilippinesInc/744000062113733-lead-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Smart Recruiters Jobs",
        "apply_link": "https://jobs.smartrecruiters.com/NimbyxPhilippinesInc/744000062113733-lead-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/lead-data-engineer-nimbyx-philippines-inc-JV_IC4778930_KO0,18_KE19,41.htm?jl=1009759137369&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.fr/job-listing/lead-data-engineer-nimbyx-philippines-inc-JV_IC4778930_KO0,18_KE19,41.htm?jl=1009759137369&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Towards AI Jobs",
        "apply_link": "https://jobs.towardsai.net/job/nimbyx-philippines-inc-lead-data-engineer-jcb7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://ph.bebee.com/job/241fdd8aced0de44321b296e27c70e1f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://ph.jooble.org/jdp/6624875891375357054?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido.com",
        "apply_link": "https://ph.jobrapido.com/jobpreview/4799667421189242880?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/lead-data-engineer-jobs/737A132869043248?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Company Description\n\nWe create what others only imagine.\n\nAs an AI-driven venture capital firm based in BGC, Philippines with offices in Vancouver, Canada and Seoul, Korea, we build AI agents that eliminate the mundane, so people can unlock their full creative and strategic potential.\n\nLife at Nimbyx\n\nWe’re a team of curious minds, fearless builders, and lifelong learners, driven by the belief that technology should empower, not replace.\n\nWe move fast, break boundaries, cut through bureaucracy, and operate by simple, powerful rules. What matters here isn’t titles or politics—it’s impact.\n\nWe’re looking for people bold enough to help us build a world that works smarter, faster, and better for everyone.\n\nIf you want comfort, look elsewhere. If you want to shape the future, welcome to Nimbyx!\n\nJob Description\n\nLead Data Engineer\n\nDrive innovation. Shape strategy. Power smarter decisions.\n\nAt Nimbyx, we’re building AI-powered platforms that are reshaping how businesses and industries work - from healthcare to beyond. As Lead Data Engineer, you won’t just be building pipelines - you’ll be building the data backbone of bold ideas, fueling intelligent decisions across our entire ecosystem.\n\nWe’re looking for a visionary technical leader who can architect scalable solutions, inspire a growing team, and turn complex data into business-defining insight. If you’re passionate about solving real-world problems through data, and love rolling up your sleeves while setting the direction—this is your moment.\n\nWhat You’ll Do\n• Architect, build, and scale production-grade data pipelines and ETL flows using Azure Data Factory and Databricks\n• Design and implement high-performance, cloud-native data warehouses (Snowflake preferred)\n• Lead the evolution of our data engineering practice, driving excellence in DevOps, data modeling, security, and automation\n• Transform diverse data sources (social, web, internal systems, product analytics) into clean, usable datasets for real-time and batch processing\n• Partner with analysts and stakeholders to develop interactive dashboards and visualizations in Power BI or Tableau that guide business strategy\n• Mentor and lead a team of engineers, setting technical direction, coaching on best practices, and ensuring alignment with cross-functional goals\n• Own database performance and optimization efforts (SQL Server, Snowflake), including stored procedures, views, and tuning\n• Explore, evaluate, and introduce new tools and technologies that make our data infrastructure smarter, faster, and more powerful\n\nQualifications\n\nWhat You Bring\n• 5+ years in data engineering, with 1–2 years in a leadership role driving results\n• Proven expertise in designing scalable pipelines and data architectures on Azure\n• Mastery of Python, SQL, and solid working knowledge of Snowflake, SQL Server\n• Deep understanding of data modeling, warehousing concepts, and ETL best practices (bonus if you're Kimball-savvy)\n• Experience building rich, insightful dashboards using Power BI or Tableau\n• Familiarity with CI/CD, infrastructure-as-code, and agile environments\n• A collaborative spirit, bias toward action, and thrive on turning data into impact\n\nWhy Join Nimbyx?\n• Be part of a mission-driven team building AI-powered tools that change industries\n• Own your impact and work on meaningful projects that directly shape product and strategy\n• Join a culture that values curiosity, bold thinking, and continuous learning\n• Collaborate with brilliant engineers, product leaders, and innovators\n• Competitive compensation, benefits, and growth opportunities in a fast-scaling company\n\nThis isn’t just another data role. It’s a chance to lead the charge in how data drives the future.\n\nIf you're ready to architect what's next, we want to meet you.\n\nAdditional Information\n\n🌟 Work setup: FULL ONSITE, Monday to Friday or Tuesday to Saturday\n\n⏰ Schedule: 8am to 5pm\n\n📍 Location: One World Place, 32nd Street, BGC, Taguig City\n\n💼 Employment Type: Full-time + Permanent\n\nGet to know us more:\n\nEvident: AI-Powered Growth Partner for Digital Dentists & Labs\n\n• https://www.instagram.com/evidentdigital/\n\n• https://evidentdigital.com\n\nNimbyx on Instagram: Global Venture Capital firm in BGC, Vancouver, & Seoul.\n\n• https://www.instagram.com/nimbyx.official/\n\n• https://nimbyx.com/careers/",
    "job_is_remote": null,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DMzFJX9JnRS0wxeJhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer",
    "employer_name": "Schneider Electric",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRRGjhn06EO9EiU-zbpMp-FchCgG3co-jMdRp7P&s=0",
    "employer_website": "http://www.se.com/",
    "job_publisher": "Schneider Electric",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.se.com/ww/en/about-us/careers/job-details/data-engineer/009GNR/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Schneider Electric",
        "apply_link": "https://www.se.com/ww/en/about-us/careers/job-details/data-engineer/009GNR/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Philippines",
        "apply_link": "https://ph.linkedin.com/jobs/view/data-engineer-at-schneider-electric-4256748633?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85234826?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Schneiderele.taleo.net",
        "apply_link": "https://schneiderele.taleo.net/careersection/2/jobdetail.ftl?job=009GNR&lang=en&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Fairygodboss",
        "apply_link": "https://fairygodboss.com/jobs/schneider-electric/data-engineer-2a7ae718230abb3a6087e963ab5d8523?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://ph.bebee.com/job/3b75a0b1a0953be932e165146b4989e6?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/ph/job/data-engineer--cavite-city--e1f89d61b3168327a13126ce74778ca97?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/data-engineer-cavite-calabarzon-region-iv-a-ph-schneider-electric-north-america-009gnr?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "#LI-Hybrid\n\nSummary\n\nWe are seeking a skilled Data Engineer to join our dynamic team. In this role, you will be responsible for designing, developing, and maintaining robust data pipelines and ETL processes. You will work heavily with SQL and Alteryx to manage dataflows from sources such as AWS Redshift, enabling our business intelligence analysts and data visualization specialists to derive actionable insights.\n\nKey Responsibilities\n\n· Design, develop, and maintain scalable ETL pipelines.\n\n· Perform data extraction, transformation, and loading (ETL) from various data sources\n\n· Optimize and manage dataflows to ensure data integrity and performance.\n\n· Collaborate with business intelligence analysts and data visualization specialists to understand data requirements.\n\n· Develop and maintain SQL queries and scripts for data manipulation and analysis.\n\n· Utilize Alteryx (or similar) for data preparation, blending, and advanced analytics.\n\n· Monitor and troubleshoot data pipeline issues to ensure timely data availability.\n\n· Implement data quality checks and validation processes.\n\n· Document data processes, workflows, and best practices.\n\nQualifications\n\nRequired Skills\n\n· Proficiency in SQL and experience with relational databases.\n\n· Strong experience with ETL tools\n\n· Knowledge of AWS Redshift and other cloud-based data warehousing solutions.\n\n· Familiarity with data modeling and database design.\n\n· Strong problem-solving and analytical skills.\n\n· Ability to work collaboratively in a team environment.\n\n· Excellent communication skills.\n\nNice-to-Have Skills\n\n· Experience with other ETL tools and platforms.\n\n· Knowledge of Python or other programming languages.\n\n· Familiarity with data visualization tools such as Tableau or Power BI.\n\n· Understanding of data governance and security best practices.\n\n· Experience with big data technologies (e.g., Hadoop, Spark).\n\nEducational Background\n\n· Bachelor's degree in Computer Science, Information Technology, Data Science, or a related field. A master's degree is a plus.\n\nWork Experience\n\n· 3+ years of experience in data engineering or a related field.\n\n· Proven experience in designing and managing ETL processes.\n\n· Hands-on experience with SQL and Alteryx.\n\n· Experience working with cloud-based data solutions, particularly AWS Redshift.\n\nSchedule: Full-time\nReq: 009GNR",
    "job_is_remote": null,
    "job_posted_at": "25 days ago",
    "job_posted_at_timestamp": 1750982400,
    "job_posted_at_datetime_utc": "2025-06-27T00:00:00.000Z",
    "job_location": "Cavite, Philippines",
    "job_city": null,
    "job_state": "Cavite",
    "job_country": "PH",
    "job_latitude": 14.245632899999999,
    "job_longitude": 120.87856579999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D818dd7Ajlnxb8TiDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Schneider Electric",
        "score": 4.3,
        "num_stars": 4.5,
        "review_count": 18808,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Schneider-Electric-Reviews-E3956.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "Schneider Electric",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 5804,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Schneider-Electric/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Schneider Electric",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 4311,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/schneider-electric-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineering (Data Migration) Consultant - Engineering - PH PDC",
    "employer_name": "Deloitte SEA",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcShsixgRt3WRc10wjbaJT_JkfBy3JX38Lplyni0&s=0",
    "employer_website": "https://www2.deloitte.com",
    "job_publisher": "Careers Job Search - Deloitte",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.sea.deloitte.com/job/Manila-Data-Engineering-%28Data-Migration%29-Consultant-Engineering-PH-PDC/1065324866/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers Job Search - Deloitte",
        "apply_link": "https://jobs.sea.deloitte.com/job/Manila-Data-Engineering-%28Data-Migration%29-Consultant-Engineering-PH-PDC/1065324866/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=1273d3c86eb0feee&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/azure-data-engineering-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,52_KE53,61.htm?jl=1009763959918&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.it/job-listing/azure-integration-developer-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,57_KE58,66.htm?jl=1009669949659&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.br/job-listing/cloud-data-engineering-aws-azure-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,62_KE63,71.htm?jl=1009676951205&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.es/job-listing/azure-integration-developer-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,57_KE58,66.htm?jl=1009669949659&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.at/job-listing/data-engineering-data-migration-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,61_KE62,70.htm?jl=1009741352311&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit Philippines",
        "apply_link": "https://www.foundit.com.ph/job/data-engineering-data-migration-consultant-engineering-ph-pdc-deloitte-philippines-34922227?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Cloud Consultant based in Deloitte Consulting Philippines Delivery Center\n\nAre you ready to unleash your potential?\n\nAt Deloitte, our purpose is to make an impact that matters for our clients, our people, and the communities we serve.\n\nWe believe we have a responsibility to be a force for good, and WorldImpact is our portfolio of initiatives focused on making a tangible impact on society’s biggest challenges and creating a better future. We strive to advise clients on how to deliver purpose-led growth and embed more equitable, inclusive as well as sustainable business practices.\n\nHence, we seek talented individuals driven to excel and innovate, working together to achieve our shared goals.\n\nWe are committed to creating positive work experiences that foster a culture of respect and inclusion, where diverse perspectives are celebrated, and everyone is recognized for their contributions.\n\nReady to unleash your potential with us? Join the winning team now!\n\nWork you will do\n\nDeloitte’s Engineering offers help to enable organization’s end-to-end journey from on-premise legacy systems to the cloud, from design through deployment, and leading to the ultimate destination—a transformed organization primed for growth.\n\nAs a Cloud Engineer, you will help build and connect sustainable cloud-native systems to thrive in the cloud ecosystem. Services include: Strategy & Architecture; Assets & Industry Solutions; Engineering & Integration\n• Based on selection criteria set up driver tables for key migration objects to track data through migration process\n• Based on mapping rules, develop data migration transform jobs using SQL with adequate tracing\n• Establish exception handling through transform process\n• Set up store procedures in Amazon Aurora PostgreSQL database to automate the migration pipeline\n• Setup and manage code versioning using CodeCommit in AWS\n• Follow best practice coding principles ensuring it is well structured and comments are included in code so it can be understood and diagnosed by another developer\n• Produce documentation outlining pipeline and key code elements to enable handover of code\n• Use AWS Data Migration Service to load data into target structure\n• Perform and monitor ETL activities for test loads in Non-Production environments\n• Receive feedback from peer review of code\n• Promptly raise questions/issues or escalate blockers regarding the mapping documents so rules can be clarified by the team\n• Clearly communicate and track progress within delivery tracking tools (ADO)\n\nYour role as a leader\n\nAt Deloitte, we believe in the importance of empowering our people to be leaders at all levels. We connect our purpose and shared values to identify issues as well as to make an impact that matters to our clients, people and the communities. Additionally, Consultants across our Firm are expected to:\n• Demonstrate a strong commitment to personal learning and development.\n• Understand how our daily work contributes to the priorities of the team and business.\n• Understand the set expectations and demonstrate accountability in keeping personal performance on track.\n• Actively focus on developing effective communications and relationship-building skills with stakeholders, clients and team.\n• Demonstrate an appreciation for working with others.\n• Understand what is fundamental to Deloitte’s success as a business.\n• Demonstrate integrity and an awareness of strengths, differences, and personal impact.\n• Develop their understanding of Deloitte and offer a fresh perspective.\n\nEnough about us, let’s talk about you\n• Bachelor’s degree in Software Engineering, Information Technology, or equivalent\n• Bachelor’s degree in Software Engineering, Information Technology, or equivalent\n• At least 7 years working experience as a Data Engineer; adept experience in data engineering principles and how to apply advanced data manipulation processes to various types of data.\n• Knowledge and experience of setting up and managing data engineering (ETL) and store procedures in AWS platform\n• Built SQL to perform transformation of data for either analytical reporting or migration purposes – can provide an example of how it was applied and how it was ensured to be performant\n• Experience in taking mapping rules to convert into code and then generating technical documentation\n• Experience of generating code that is performant across high volume datasets and tuning to ensure transformation can run within the cutover window\n• Experience in generating technical documentation\n• High attention to detail\n• Motivated and interested to learn\n• Clear written and spoken communication in English\n\nWhat is in store for you?\n• Embrace the dynamic nature of our work environment with the opportunity to work on a hybrid set-up and on a shifting schedule.\n• Rewards platform – your hard work won't go unnoticed at Deloitte!\n• Training and development - at Deloitte we believe in investing in our best assets, the people! You will have access to world class training and funding towards industry and other professional certifications.\n• Receive support andv mentoring to progress your career. You will have access to mentors and coaches who will help you pave a path for career progression.\n• Benefits effective upon hiring including paid time off and holidays, health, and life insurance!\n\nNext Steps\n\nSound like the sort of role for you? Apply now.\n\nDue to volume of applications, we regret only shortlisted candidates will be notified.\n\nCandidates will only be contacted by authorized Deloitte Recruiters via firm’s business contact number or business email address.\n\n© 2025 DCPDC Inc.",
    "job_is_remote": null,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1752105600,
    "job_posted_at_datetime_utc": "2025-07-10T00:00:00.000Z",
    "job_location": "Manila, Metro Manila, Philippines",
    "job_city": "Manila",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.5995133,
    "job_longitude": 120.984234,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DG4ToQAsZXpo3M6EHAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer",
    "employer_name": "Robert Walters",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRVgWeMhSmIgzJ9I_Sp82E4NCMWvK4MwA492xBF&s=0",
    "employer_website": "https://www.robertwalters.com/",
    "job_publisher": "Robert Walters Philippines",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.robertwalters.com.ph/techtransformation/jobs/businessintelligenceanalytics/1839295-data-engineer.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Robert Walters Philippines",
        "apply_link": "https://www.robertwalters.com.ph/techtransformation/jobs/businessintelligenceanalytics/1839295-data-engineer.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ING's Careers",
        "apply_link": "https://careers.ing.com/en/job/makati-city/data-engineer/3121/26173398656?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Chevron Careers",
        "apply_link": "https://careers.chevron.com/job/makati-city/data-engineer/38138/71451952528?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Careers At Mace - Mace Group",
        "apply_link": "https://careers.macegroup.com/gb/en/job/38325/Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=79a46234e7c94a85&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kalibrr",
        "apply_link": "https://www.kalibrr.com/c/questronix-corporation/jobs/188233/data-engineer-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85298335?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-ksearch-asia-consulting-JV_IC4778930_KO0,13_KE14,37.htm?jl=1009012570692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "An exciting opportunity awaits you to join a pioneering company in the energy and power sector.\n\nWhat you'll do:\n• Identify, design, and implement automated processes such as billing systems, alert mechanisms, query usage analysis, and resource monitoring to optimise internal operations.\n• Ideate, iterate on, design, and build new features from scratch or enhance existing commercial projects to deliver additional value for platform users.\n• Provide constructive feedback or contribute directly to platform feature improvements alongside the Data Infrastructure & Products team to boost efficiency and capabilities.\n• Lead end-to-end delivery of commercial projects by managing timelines, resources, and technical execution from inception through completion.\n• Collaborate with cross-functional teams to understand complex data requirements and develop tailored solutions that address specific business needs.\n• Participate in client-facing meetings to finalise project details, clarify requirements, provide updates, and ensure alignment between stakeholders.\n• Design and implement configurable data pipelines for ingestion, transformation, reverse ETL processes based on detailed client functional requirements.\n• Develop comprehensive data quality checks at multiple checkpoints to ensure accuracy, completeness, and consistency throughout all stages of data processing.\n• Work closely with Machine Learning engineers to create and maintain ML data pipelines for training models, batch inference tasks, deployment workflows, validation routines, and ongoing monitoring.\n• Assist in troubleshooting and resolving any issues related to data integrity or pipeline performance while maintaining clear documentation.\n\nWhat you bring:\n• A Bachelor’s degree in Computer Science, Engineering or a related discipline provides you with a solid academic foundation for tackling complex technical challenges.\n• Proficiency in SQL and Python enables you to efficiently manipulate large datasets and automate repetitive tasks within various environments.\n• A thorough understanding of conventional database concepts ensures you can design effective storage solutions that meet evolving business needs.\n• Hands-on experience with modern data warehouse architectures (such as lakehouse or lake) empowers you to build scalable systems capable of handling vast amounts of information.\n• Practical knowledge of cloud platforms like AWS, Azure or GCP allows you to deploy resilient infrastructure that supports high-availability applications.\n• Demonstrated analytical skills help you break down intricate problems into manageable components while identifying optimal solutions quickly.\n• Experience implementing comprehensive Data Quality Checks guarantees the reliability of insights derived from processed information.\n• Familiarity with data processing frameworks such as dbt streamlines your ability to transform raw inputs into actionable outputs efficiently.\n• Expertise using integration tools like Airflow enhances your capacity for orchestrating complex ETL workflows across distributed systems.\n• Exposure to modern warehouse/lakehouse solutions (e.g., Delta Lake, Redshift or Snowflake) equips you with versatile skills applicable across multiple platforms.\n• Working knowledge of Databricks (on either Azure or AWS) combined with an understanding of Spark positions you well for advanced analytics tasks.\n• Excellent communication abilities foster smooth collaboration within multidisciplinary teams while ensuring clarity when interacting with clients or stakeholders.\n• Self-motivation paired with a passion for continuous learning drives your commitment towards personal growth and staying current with industry trends.\n\nWhat sets this company apart:\n\nThis organisation stands out as one of Makati’s most promising start-ups dedicated exclusively to harnessing the power of data science and artificial intelligence for meaningful societal impact. Here you’ll find an inclusive culture that values knowledge sharing just as much as technical excellence—ensuring everyone has access to mentorship regardless of experience level. Flexible working opportunities allow you to balance professional ambitions with personal commitments without compromise. The leadership team is deeply invested in nurturing talent through regular training sessions designed around emerging industry trends so that every member remains ahead of the curve. Collaboration is woven into every aspect of daily operations; whether brainstorming new features or refining existing processes together with colleagues from diverse backgrounds fosters genuine camaraderie among peers. By joining this team not only do you gain exposure to cutting-edge technology stacks but also become part of a supportive network committed towards collective success—making it an ideal place for those who wish their work could truly make a difference beyond business metrics alone.\n\nWhat's next:\n\nIf you're ready to take your career in data engineering further while making an impact alongside passionate professionals—this is your moment!\n\nApply today by clicking on the link provided below; seize this chance to shape tomorrow’s world through transformative data solutions.\n\nDue to the high volume of applications we are experiencing, our team will only be in touch with you if your application is shortlisted.",
    "job_is_remote": null,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1752105600,
    "job_posted_at_datetime_utc": "2025-07-10T00:00:00.000Z",
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D_uf3ZFI92dW4jerlAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Robert Walters",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 2728,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Robert-Walters-Reviews-E12235.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "Robert Walters",
        "score": 3.6,
        "num_stars": 3.5,
        "review_count": 298,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Robert-Walters/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Robert Walters",
        "score": 2.9,
        "num_stars": 3,
        "review_count": 16,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/overview/robert-walters-overview?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Principal Data Engineer (Remote, Day Shift)",
    "employer_name": "SYMPHONY",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Jobstreet",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.jobstreet.com/job/85814863?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85814863?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "JOB OVERVIEW\n\nWe are looking for a Principal Data Engineer to lead the technical vision, architecture, and delivery of our next-generation GRC data platform. You’ll work at the intersection of AI and cloud-native architecture for building custom data solutions that addresses user critical problems and help us scale intelligent, secure, and resilient data systems across multiple global regions.\n\nYou will play a key role in enabling AI-driven insights, automating compliance workflows, and ensuring our platform remains robust and audit-ready for regulated industries.\n\nKEY RESPONSIBILITIES\n\nData Architecture & Engineering Leadership:\n• Responsible for identifying, analyzing, and conducting the client's business requirements gap analysis and processes through document analysis, interviews, workshops, and workflow analysis.\n• Guide and support the Data Engineer in the team for executing on projects involving technology upgrade and implementing optimizations wherever appropriate.\n• Define data architecture for scalability, performance and cost-efficiency\n• Evaluate and implement new technologies that facilitates or simplifies the end-to-end data lifecycle targeted for reporting and AI use cases\n• Documenting the information gathered during the data requirements workshop and preparing the solution architecture documents.\n• Partnering with internal stakeholders that includes heads of product, engineering and customer success to identify and streamline priorities and execute it with the team for delivering data processes and products.\n• Act as a technical authority providing guidance on data modelling, schema design, metadata management and governance\n• Mentor data engineers on the team and help grow the technical capabilities of the data team\n• Promote best practices in the ELT processes and the accompanying CI/CD and DevOps for data\n• Developing and maintaining SQL queries using various tools like DBT, Azure Synapse, SQL server, etc. in line with business and customer requirements.\n• Developing and maintaining reports in various Business Intelligence (BI) platforms, including Yellowfin, in line with business and customer requirements.\n• Using project management methodologies, principles and techniques, developing project plans and estimate the required effort (cost) and resources\n• Working with the broader engineering and product team and other stakeholders on troubleshooting, investigating, and remediating reporting bugs and issues.\n• Analyzing data and actively coordinating with stakeholders to evaluate the inefficiencies of business process and to ensure that they are accurately updated to eliminate any potential issues.\n• Implementing enterprise data warehouse infrastructure and managing the data pipelines feeding into analytics and advanced AI powered analytics subsystems\n\nAI/ML Enablement:\n• Design ML-ready data systems that feed into the 6Clicks AI system, including feature stores, training pipelines and inference-serving infrastructure.\n• Collaborate with the AI engineering, core engineering and product teams to operationalize models that facilitate automation in the GRC processes\n• Partner with product, Artificial Intelligence/Machine Learning (AI/ML), and platform teams to deliver data solutions that directly power client-facing features\n• Set technical direction and best practices for the data engineering functions that drive AI/ML functionalities\n• Evaluate emerging technologies and lead critical platform upgrades or migrations that will accelerate AI/ML feature development\n\nWhat you bring\n• Preferred candidate with Master’s in Data Analytics or related field of study.\n• Minimum 12-15 years of relevant work experience.\n• Prior experience in a principal, staff or lead engineer role within high-growth SaaS or enterprise tech environments\n• Strong understanding of data security, compliance, and multi-region cloud deployments\n• Proven experience in designing and developing ETL pipelines using cloud-based technologies.\n• Experience integrating data from multiple sources, including SQL databases, Excel spreadsheets, and APIs.\n• Excellent communication and collaboration skills, with the ability to effectively interact with stakeholders at all levels of the organization.\n• Detail-oriented with strong analytical and problem-solving skills.\n• Self-motivated and able to work independently as well as part of a team. Ability to relate to people, understand their needs and align it with proposed solutions.\n• Previous experience in Governance, Risk & Compliance (GRC) or GRC technologies is a plus.",
    "job_is_remote": null,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": "Metro Manila, Philippines",
    "job_city": null,
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.609053699999999,
    "job_longitude": 121.0222565,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DdQYKP8za-cksWVy7AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Python, ETL, SQL (Hybrid)",
    "employer_name": "Tamaray People Solutions Corp,",
    "employer_logo": null,
    "employer_website": "https://www.tamaray.com",
    "job_publisher": "Glassdoor",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.glassdoor.com/job-listing/data-engineer-python-etl-sql-hybrid-tamaray-people-solutions-corp-JV_IC2340354_KO0,35_KE36,65.htm?jl=1009803230662&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-python-etl-sql-hybrid-tamaray-people-solutions-corp-JV_IC2340354_KO0,35_KE36,65.htm?jl=1009803230662&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "DUTIES AND RESPONSIBILITIES:\n• Design, build, and maintain scalable, secure data pipelines and storage systems; ensure data quality through ETL processes and regular checks.\n• Implement policies and practices to control, optimize, and secure data assets, ensuring data integrity and accessibility.\n• Develop and maintain data models, structures, and databases to meet business needs; communicate data architecture effectively.\n• Develop, test, and maintain scripts and programs to automate data processing and pipelines, adhering to industry standards.\n• Create and operationalize data visualization solutions to simplify complex data for stakeholders and decision-making.\n• Work with cross-functional teams to gather data requirements, optimize existing processes, and deliver ad hoc reports and insights\n\nJOB SPECIFICATIONS:\n• Graduate with a Bachelor’s or Master's Degree in IT, Computer Science, Engineering, or any related course.\n• At least 2 years of experience in data engineering, data analytics, or related fields.\n• Proficiency in Python\n• Hands-on experience with data manipulation tools (e.g., pandas, dplyr, or Spark).\n• Proficiency in Extract, Transform, and Load (ETL) processes for efficient data pipeline management.\n• Expertise in both SQL and NoSQL query languages for database interaction and management.\n• Experience with big data storage and processing solutions, such as MongoDB, Spark, Hive, Snowflake, Redshift, or similar technologies.\n• Experience with cloud-based or server-based data processing environments (e.g., AWS, Azure, GCP).\n\nJob Types: Full-time, Permanent\n\nPay: Php45,000.00 - Php55,000.00 per month\n\nBenefits:\n• Company events\n• Health insurance\n• Life insurance\n• Paid training\n• Promotion to permanent employee\n• Work from home\n\nSchedule:\n• 8 hour shift\n• Day shift\n• Monday to Friday\n\nSupplemental Pay:\n• 13th month salary\n\nAbility to commute/relocate:\n• Taguig: Reliably commute or planning to relocate before starting work (Preferred)\n\nApplication Question(s):\n• This is 3x/week onsite in BGC, Taguig. Are you amenable?\n\nEducation:\n• Bachelor's (Preferred)\n\nExperience:\n• Data Engineer: 2 years (Preferred)\n• Python: 2 years (Preferred)\n• ETL: 2 years (Preferred)\n• SQL: 2 years (Preferred)\n\nWork Location: Hybrid remote in Taguig",
    "job_is_remote": null,
    "job_posted_at": "15 days ago",
    "job_posted_at_timestamp": 1751846400,
    "job_posted_at_datetime_utc": "2025-07-07T00:00:00.000Z",
    "job_location": "Taguig, Metro Manila, Philippines",
    "job_city": "Taguig",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.530631699999999,
    "job_longitude": 121.0575482,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVMzXnWB28OOEUSPsAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer / Data Architect",
    "employer_name": "Quantrics Enterprises Inc.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTEfPIghSrpZtSFQtUndVjF1ID2WeJ4nKCSzUSd&s=0",
    "employer_website": null,
    "job_publisher": "Expertini",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.expertini.com/jobs/job/data-engineer-data-architect-rizal-quantrics-enterprises-inc-649-912532/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Expertini",
        "apply_link": "https://ph.expertini.com/jobs/job/data-engineer-data-architect-rizal-quantrics-enterprises-inc-649-912532/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Drive innovation in cybersecurity, software development, and big data technology as part of our award-winning IT team. You can play a critical role as part of a global agile team for Bell Canada, Canada’s largest telco, media, and tech company.\n\nWe know our success is fueled by our people, so we empower our Qmunity and provide a workplace where you can flourish and grow. We offer premium benefits, including:\n• Home-based setup\n• Miscellaneous allowances, performance-based bonus, and yearly increase\n• HMO from day 1 for you + 2 free dependents\n• 6 months paid maternity; 15 days paternity leave\n• Company-sponsored training and upskilling, and career growth opportunities!\n\nYou will have the opportunity to...\n• Lead the design and implementation of robust and scalable pipelines, ETL processes and data integration solutions.\n• Collaborate with Solution Architects, BI Developers, Data analysts and stakeholders to understand their data requirements, translate business needs into technical specifications.\n• Identify and resolve complex data-related issues, performance bottlenecks, and data pipelines failures.\n• Evaluate the performance of existing processes and optimize whenever required\n\nCore Competencies (Must-have Competencies)\n• 3+ years of experience in development using Python\n• 3-5+ years of experience in data integration. Must have solid understanding of data modeling, Data warehousing concepts, and data integration techniques along with ETL development.\n• 3+ years of experience in Web APIs development\n• 3-5+ years of experience in SQL and Data Modelling\n• 1+ year of experience in data level security\n\nComplementary Competencies (Good-to-have Competencies)\n• SAP BW, MicroStrategy\n\nQUALIFICATIONS\n\nEducational Qualifications\n• Bachelor’s Degree in Computer Science, Information Technology, Engineering, or related fields\n\nWork Conditions\n• Work from Home Set-up\n• Mid-Shift: 4:00 PM – 1:00 AM Manila\n\n#WorkFromHome\n#J-18808-Ljbffr",
    "job_is_remote": null,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1752883200,
    "job_posted_at_datetime_utc": "2025-07-19T00:00:00.000Z",
    "job_location": "Pasay City, Metro Manila, Philippines",
    "job_city": "Pasay City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.5376686,
    "job_longitude": 121.0008137,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DB5FpeM6W7FmlpxxTAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15119900",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Multinational Bank",
    "employer_name": "Michael Page Philippines",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTtL8Q36ioUybwIGJT__ggJHyoAzcoshQE_bGNe&s=0",
    "employer_website": "https://www.michaelpage.com.ph",
    "job_publisher": "Michael Page Philippines",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.michaelpage.com.ph/job-detail/data-engineer-multinational-bank/ref/jn-092024-6543522?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Michael Page Philippines",
        "apply_link": "https://www.michaelpage.com.ph/job-detail/data-engineer-multinational-bank/ref/jn-092024-6543522?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=8a0f42da91051dac&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.br/job-listing/data-engineer-hybrid-techtiera-corp-JV_KO0,20_KE21,35.htm?jl=1009771853053&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-hybrid-techtiera-JV_KO0,20_KE21,30.htm?jl=1009771853053&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "• Join a growing global team\n• Enjoy market-aligned salaries & benefits\n\nAbout Our Client\n\nThe company is a multinational bank operating in global markets, offering diverse financial services to their customers and partners.\n\nJob Description\n• Design, develop, and optimize complex data pipelines to facilitate seamless data integration from multiple sources into our data warehouse.\n• Collaborate with data analysts, data scientists, and other stakeholders to understand data requirements and deliver actionable insights.\n• Implement and maintain data warehousing solutions, ensuring high performance, availability, and reliability of data.\n• Utilize advanced data modeling and schema design techniques to enhance data quality and accessibility.\n• Troubleshoot and resolve data-related issues, optimizing ETL processes for efficiency and reliability.\n• Ensure compliance with industry regulations and data governance frameworks.\n• Mentor and guide junior data engineers, fostering a culture of continuous improvement and knowledge sharing.\n• Stay updated with the latest technologies and best practices in data engineering and software development.\n\nThe Successful Applicant\n• Minimum of 5 years of technical experience in data pipelining and data warehousing. Background in banking, financial services, and shared services is preferred.\n• Strong understanding of software engineering principles and best practices.\n• Proficiency in programming languages such as Python, Java, or Scala for data processing and ETL development.\n• Extensive experience with SQL, including PL/SQL, for data manipulation and querying.\n• Familiarity with IBM DataStage or similar ETL tools is highly desirable.\n• Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark) is a plus.\n• Strong analytical and problem-solving skills with an attention to detail.\n• Excellent communication skills and the ability to work effectively in a collaborative team environment.\n• Amenable to work in a mid-shift\n• Amenable to work in Makati\n• Amenable to a hybrid work setup\n\nWhat's on Offer\n• Competitive pay & benefits\n• Healthcare (HMO, Insurance)\n• Performance bonus/Performance incentives\n• Leave incentives & Time Off\n• Training & development",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DWs8I9bYnL08zTz_-AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Sr. Data Engineer",
    "employer_name": "Dev Partners Philippines",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTgNIhgfLT9489YanVIbJbMBnAvizSKLqqke_JI&s=0",
    "employer_website": null,
    "job_publisher": "Wellfound",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://wellfound.com/jobs/3318771-sr-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Wellfound",
        "apply_link": "https://wellfound.com/jobs/3318771-sr-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://pwc.wd3.myworkdayjobs.com/pt-BR/Global_Experienced_Careers/job/Sr-Data-Engineer_566505WD?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "🚀 Now Hiring: Senior Data Engineer (100% Remote)\n\nWe are looking for a highly skilled Senior Data Engineer with strong expertise in Microsoft Business Central to join our team and help drive data integration, modeling, and analytics capabilities across the organization.\n\nAs a Senior Data Engineer, you will be responsible for:\n\n🗸 Design, build, and maintain scalable data pipelines and ETL processes integrating Microsoft Business Central with other data platforms.\n\n🗸 Work closely with business stakeholders to understand data needs and translate them into technical solutions.\n\n🗸 Develop and optimize data models and database architectures to support analytics and reporting.\n\n🗸 Ensure data quality, security, and governance standards are met across data workflows.\n\n🗸 Collaborate with cross-functional teams including data analysts, engineers, and product managers to deliver timely and actionable data insights.\n\n🗸Troubleshoot and resolve data-related issues and optimize system performance.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Philippines",
    "job_city": null,
    "job_state": null,
    "job_country": "PH",
    "job_latitude": 12.879721,
    "job_longitude": 121.774017,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuTNSxvjgp0m6Q28FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 70000,
    "job_max_salary": 80000,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Dev Partners Philippines",
        "score": 4.9,
        "num_stars": 5,
        "review_count": 100,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Overview/Working-at-Dev-Partners-EI_IE2355990.11,23.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineering Lead",
    "employer_name": "IWG",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSH5Mnsj_OEtOJklm8o6hgyCe3MrnvsDKO3Eq0I&s=0",
    "employer_website": "http://www.iwgplc.com/",
    "job_publisher": "IWG",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.iwgplc.com/jobs/data-engineering-lead-bonifacio-global-city-philippines?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "IWG",
        "apply_link": "https://jobs.iwgplc.com/jobs/data-engineering-lead-bonifacio-global-city-philippines?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=ab4f0d7de391798a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit Philippines",
        "apply_link": "https://www.foundit.com.ph/job/data-engineering-lead-international-workplace-group-plc-philippines-34817953?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the company\n\nIWG has been at the forefront of the flexible workspace revolution for more than 30 years. We have made it possible for organisations and individuals everywhere to take a new approach to the traditional working day. We have over 3,400 locations across over 110 countries allowing millions of people every day to have a great day at work.\n\nOur customers are start-ups, small and medium-sized enterprises, and large multinationals. With unique business goals, people and aspirations. They want workspaces and communities to match their needs. We provide them with choice through our portfolio of brands, covering serviced offices (Regus, Spaces, HQ, Signature and No18), commercial real estate brokerage and managed office solutions.\n\nJoin us at www.iwgplc.com\n\nOverview:\n\nThe Data Engineering Lead is a key leadership role responsible for guiding a team of data engineers, designing and optimizing data architecture, and ensuring the efficient operation of data pipelines. This role involves managing data quality, collaborating with stakeholders to support data-driven decision-making, and setting the strategic direction for the data engineering function. The Data Engineering Lead also ensures compliance with data governance standards, provides technical leadership, and stays informed about emerging data technologies to drive innovation and efficiency.\n\nKey Responsibilities:\n• Leadership:\n• Recruit, manage, and mentor a team of data engineers.\n• Set performance expectations, provide feedback, and foster career development.\n• Create a positive team culture and promote collaboration.\n• Data Architecture Design:\n• Develop and maintain a scalable data architecture, including data ingestion, data pipelines, data warehousing, and data modeling.\n• Define data governance policies and standards to ensure data quality and compliance.\n• Data Pipeline Development:\n• Design and implement robust, efficient data pipelines for structured, semi-structured, and unstructured data.\n• Optimize data processing and transformation to ensure timely data delivery.\n• Stakeholder Management:\n• Collaborate with business stakeholders to understand data needs and translate them into actionable data solutions.\n• Communicate data insights and findings to key decision-makers.\n• Technology Evaluation and Innovation:\n• Stay abreast of emerging data technologies and tools.\n• Evaluate and implement new technologies to improve data processing efficiency and scalability.\n• Performance Monitoring and Optimization:\n• Monitor data pipeline performance and identify areas for improvement.\n• Implement performance optimization strategies to address data bottlenecks.\n\nRequired Skills and Experience:\n• Graduate of any IT or Computer related course preferably Engineering, Business Intelligence, Computer Science or Statistics related\n• 3 to 5 years' experience\n• Strong understanding of data engineering principles, including data warehousing, data modeling, ETL/ELT processes.\n• Expertise in various data technologies (e.g. Spark, Kafka, SQL, NoSQL databases)\n• Experience with cloud computing platforms (Azure, AWS, GCP)\n• Proven leadership skills in managing and mentoring technical teams\n• Excellent communication and collaboration skills to work effectively with cross-functional teams\n\nTechnical Skills Candidate must have:\n• MS SSIS or any other Microsoft tool for data integration\n• SQL (Advanced skills) and Spark\n• Power BI and Excel (using Power Query)\n• Python",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Philippines",
    "job_city": null,
    "job_state": null,
    "job_country": "PH",
    "job_latitude": 12.879721,
    "job_longitude": 121.774017,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DmK0hlY_kQCAgQ12LAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "IWG",
        "score": 2.3,
        "num_stars": 2.5,
        "review_count": 108,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/International-Workplace-Group-1/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "IWG",
        "score": 3.2,
        "num_stars": 3,
        "review_count": 969,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/International-Workplace-Group-Reviews-E1937369.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Comparably",
        "employer_name": "IWG",
        "score": 2.8,
        "num_stars": 3,
        "review_count": 34,
        "max_score": 5,
        "reviews_link": "https://www.comparably.com/companies/iwg-international-workplace-group/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "DATA ENGINEER",
    "employer_name": "Unilab, Inc.",
    "employer_logo": null,
    "employer_website": "https://www.unilab.com.ph/",
    "job_publisher": "Careers | Unilab",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.unilab.com.ph/job/MANDALUYONG-CITY-DATA-ENGINEER-METR/1210770401/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers | Unilab",
        "apply_link": "https://careers.unilab.com.ph/job/MANDALUYONG-CITY-DATA-ENGINEER-METR/1210770401/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=70ca3e155ee81d06&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-unilab-JV_IC4747401_KO0,13_KE14,20.htm?jl=1009781261604&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Philippines",
        "apply_link": "https://ph.linkedin.com/jobs/view/data-engineer-at-unilab-inc-4250883335?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs At Stratpoint - Trakstar",
        "apply_link": "https://stratpoint.hire.trakstar.com/jobs/fk0px4x?apply=true&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.br/job-listing/data-engineer-unilab-JV_IC4747401_KO0,13_KE14,20.htm?jl=1009781261604&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.mx/job-listing/data-engineer-unilab-JV_IC4747401_KO0,13_KE14,20.htm?jl=1009781261604&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/84987443?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "It is the spirit of Bayanihan that drives us to continue our legacy of excellence and commitment to care. As an organization, we achieve our successes through good, honest, and persevering hard work - TOGETHER. It is in this way in which our company was built; we progressed as the country's leading Pharmaceutical company, not by sheer luck, but by pure perseverance, integrity, and brotherhood. Grow with us, and be a part of the Bayanihan spirit.\n\nRole Overview:\n• The Data Engineer is responsible for translating the analytics requirements of the Grupong Tamang Alaga IT & Analytics Team into technical plans and execution. Responsibilities include, but may not be limited to:\n- Acquire and process datasets required by the business\n- Build, test and maintain optimal data pipeline architectures\n- Identify, design and implement internal process improvements\n- Develop algorithms to transform data into useful and actionable information\n- Monitors data update and resolves data issues\n- Collaborate with other internal IT groups, management and external parties\n- Ensure compliance with data governance and security policies\n• The role will also be responsible for data engineering-related initiatives worked together with Unilab Corporate IT's Data Team, as well as projects that are outsourced to third-party providers.\n\nRequired Qualifications:\n• SOFT SKILLS:\n- Ability to effectively partner with internal CIT/Unilab/Business Unit functional and technical teams\n- Excellent verbal and written communication skills\n- Analytical thinker with creative problem-solving skills and attention to detail.\n• TECH SKILLS:\n- Bachelor's Degree holder of Computer Science, Engineering, Statistics, Management Information Systems, or other Science and Technology related fields\n- Experience with managing structured data in RDBMS with practitioner level experience with MS SQL (Database and SSIS)\n- Experience with Tableau development is a plus\n- Experience with managing non-structured data, including use of cloud native services from AWS and GCP\n- Certifications for database administration, agile development, and/or cloud services are a plus\n- Awareness level with machine learning and AI\n• TARGET INDUSTRIES:\n- Consulting companies, Telco, Local conglomerates, Research Firms\n• ACADEMIC BACKGROUND:\n- Computer Science, Engineering, Statistics, Mangement Information Systems, or other Science and Technology related fields\n\nWe are committed to providing our employees with the best possible experience. As a LEARNING ORGANIZATION, we are eager to support your development and create the most fitting career path for you. As DESIGNERS AND DRIVERS OF INNOVATION, we are keen to provide you with opportunities to positively transform processes that will intensify business growth. As a NURTURING FAMILY, we are passionate about conducting programs that can promote your wellness, and help you be the best that you can be. As BELIEVERS OF OUR PURPOSE, we are and we will always remain earnest in giving meaningful tasks that will keep you delighted and fulfilled - at work and beyond.",
    "job_is_remote": null,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": "Mandaluyong City, Metro Manila, Philippines",
    "job_city": "Mandaluyong City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.576267999999999,
    "job_longitude": 121.03924359999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DV3I9UcijELmuRko3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Unilab, Inc.",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 340,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Unilab-Reviews-E601294.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed Job Search",
        "employer_name": "Unilab, Inc.",
        "score": 4.3,
        "num_stars": 4.5,
        "review_count": 167,
        "max_score": 5,
        "reviews_link": "https://ph.indeed.com/cmp/Unilab/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Payscale",
        "employer_name": "Unilab, Inc.",
        "score": 3.67,
        "num_stars": 3.5,
        "review_count": 3,
        "max_score": 5,
        "reviews_link": "https://www.payscale.com/research/PH/Employer=UNILAB/Reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Sao Paolo",
    "employer_name": "Capital Markets Gateway",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "GrabJobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://grabjobs.co/philippines/job/full-time/others/remote-data-engineer-sao-paolo-137391453?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/philippines/job/full-time/others/remote-data-engineer-sao-paolo-137391453?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About us\n\nCapital Markets Gateway (CMG) is a financial technology firm that is modernizing the equity capital markets (ECM).CMG connects investors and underwriters via a neutral platform that delivers integrated ECM data and analytics, transparency, and workflow efficiencies. Providing a digital system of record for firm-wide deal activity, CMG helps clients make more timely, better-informed decisions.\n\nLaunched in 2017 by a team of ECM practitioners, the CMG platform is currently relied upon by nearly 100 buy side firms representing $20 trillion in AUM and 15 investment banks. CMG’s goal is to alleviate pain points resulting from disparate solutions, fragmented data, and frenzied communication. CMG’s DataLab product solves for data analytics, while CMG’s XC platform establishes connectivity between buy- and sell-side firms.\n\nOur engineering team\n\nThe CMG engineering team consists of domain experts who work collaboratively within a culture of cross-domain knowledge sharing. We value engineers who are passionate about modern technologies and best practices.\n\nOur engineers are willing to challenge the status-quo and are constantly seeking improvement and efficiency in our code-base and platform. CMG engineers are empowered to explore solutions and bring recommendations to the table. We’re in a period of making impactful engineering decisions. As part of our process, we believe in taking the time for research and prototyping - this is critical in making the right decisions.\n\nGiven the experience of our team, we have naturally adopted best practices from local development, through code review and into production rollouts. Besides the standard pull requests, test automation, code coverage tracking, containerization, and one-click deployments we are constantly reviewing these foundational components to develop new best practices.\n\nThe role\n\nWe are looking for a Data Engineer to join our passionate team. In this role, you will be part of a team responsible for building and maintaining scalable data pipelines and architecture, utilizing the latest data technologies to solve complex problems in investment finance. Your work will enable our clients to access mission-critical data and analytics to make impactful decisions in global capital markets.\n\nYour responsibilities will include\n• Exploring and loading a variety of data sets from market data sources.\n• Transforming, reconciling, and integrating source data into easily consumable data products for our clients.\n• Contributing to the development of a scalable data architecture to support large data volumes and various access mechanisms.\n• Contributing to the design and implementation of new data systems and infrastructure.\n• Advancing sound data engineering practices to automate processing, testing, and quality checks.\nQualifications\n• Higher education degree in Computer Science or a related field is preferred.\n• Understanding of data modeling, data lakes, and data warehousing concepts.\n• Deep knowledge of SQL and relational databases, experience with SnowSQL is a plus.\n• Experience using scripting languages such as Python and shell script for common data processing tasks.\n• Analytical skills to understand complex requirements, process large volume of information, and design creative solutions.\n• Strong communication and collaboration skills.\n• Experience with Snowflake and Kubernetes is desirable.\n• Experience with DBT and ARGO is a plus.\n• Experience working with financial market data, including fundamentals, pricing, and corporate actions, is a big plus.\n• Some analytics and AI experience is advantageous.\n• Possibility to work in a US time zone\n• Must be based in Sao Paolo, Argentina\n• English level - C1 or C2\nOur values\n• We innovate with purpose\n• We focus on outcomes vs. output\n• We believe diverse and inclusive teams fuel innovation\n• We are humble yet candid\n• We do right by the customer\nWhat we offer\n• 2 year+ contract\n• 15 days of vacation\n• Gym membership contribution\n• Language courses\n• Tech courses and conferences\n• Top-of-the-line MacBook\n• Potential trips to the USA\n• Company team-building events\n• Flexible working hours and the possibility to work from home\n\nWe celebrate diversity and are committed to creating an inclusive work environment. CMG is an equal-opportunity employer.\n\nOriginal job Data Engineer - Sao Paolo posted on GrabJobs ©. To flag any issues with this job please use the Report Job button on GrabJobs.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Quezon City, Metro Manila, Philippines",
    "job_city": "Quezon City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.6487853,
    "job_longitude": 121.0509385,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dip_KnQGFo1bEqILzAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer | Remote | Strong on AWS Glue/AWS Services |   GC-EAD & -EAD(Only Genuine) | W2 Position",
    "employer_name": "URSI Technologies Inc.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQs7wpzA8c8DZ6Vw2RuoOcSMab3tVao0EfNq501&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/fa29282b-81e1-49d4-9386-188f2360713f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/fa29282b-81e1-49d4-9386-188f2360713f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Data EngineerLocation: Remote RoleDuration: 6+ Months ContractVisa Status: GC-EAD & -EAD ONLY!! (Need Passport Copy f & Passport Number f-EAD & EAD) No H1's/OPT/CPT!! Note: This is a Remote role, but potential office for the quarterly planning meets it s in Pittsburgh or Philadelphia, PA. Roles and Responsibilities:Collaborate closely with cross-functional teams including Data Scientists, Analysts, and Software Engineers to understand data requirements and translate them into efficient solutions using AWS Glue and AWS services.Develop and maintain ETL processes using AWS Glue to facilitate seamless and reliable data extraction, transformation, and loading.Implement robust data security measures and access controls in alignment with company policies and industry best practices.Monitor, troubleshoot, and enhance data pipelines, identifying and resolving performance bottlenecks, data quality issues, and other challenges.Stay up-to-date with the latest advancements in AWS Glue and AWS services, and advocate for their effective utilization within the organization Experience/Minimum RequirementsProven experience 8+ years as a Data Engineer, with a strong emphasis on AWS Glue and AWS services.In-depth understanding of architecture, performance optimization techniques, and best practices.Proficiency in SQL and experience with database design principles.Hands-on expertise in designing, building, and maintaining complex ETL pipelines using and AWS Glue.Familiarity with data warehousing concepts and methodologies.Competence in cloud computing and AWS services, with a focus on data-related services such as S3, Redshift, and Lambda.Proficiency in scripting and programming languages such as Python, Java, or similar.",
    "job_is_remote": null,
    "job_posted_at": "15 hours ago",
    "job_posted_at_timestamp": 1753117200,
    "job_posted_at_datetime_utc": "2025-07-21T17:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DFWriH--HRVnwoB8JAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer III (Associate) (Remote)",
    "employer_name": "Jobright.ai",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTwX24ib-DGCR06HF9IDGP_J-huDCctBW5MIxyF&s=0",
    "employer_website": "https://jobright.ai",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-associate-remote-at-jobright-ai-4270055641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-associate-remote-at-jobright-ai-4270055641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs - Alooba",
        "apply_link": "https://jobs.alooba.com/us/job/everi-holdings-inc-data-engineer-iii-remote-842416/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Truemote",
        "apply_link": "https://truemote.com/remote-job/data-engineer-iii-associate-remote-b2435c5f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "USAJobCareer.com",
        "apply_link": "https://usajobcareer.com/jobs/view/data-engineer-iii-associate-remote-1672550.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Jobs",
        "apply_link": "https://store.joblagii.com/blogs/news/data-engineer-iii-remote-id-15864?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job_Summary:\n\nAstrana Health is seeking a highly motivated Data Engineer III to join their Data - Analytics department. The role involves collaborating with product teams and clinical leaders to manage and enhance data products, ensuring high-quality data models and addressing data quality issues.\n\nResponsibilities:\n\n• Use data engineering best practices to produce high quality, maximally available data models which are intuitive and trusted by stakeholders.\n\n• Scope and implement new entities for Astrana’s unified data model.\n\n• Interfacing with business customers, gathering requirements and developing new datasets in data platform\n\n• Identifying the data quality issues to address them immediately to provide great user experience\n\n• Extracting and combining data from various heterogeneous data sources\n\n• Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets\n\n• Modelling data and metadata to support machine learning and AI\n\n• Support integration efforts from acquisitions as necessary.\n\nQualifications:\n\n-Required:\n\n• Bachelor's degree required in computer science, information technology, or related field\n\n• Strong understanding of database structures, theories, principles, and practices\n\n• Working knowledge with programming or scripting languages such as Python, Spark, and SQL\n\n• Demonstrated track record executing best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, deployment processes, testing, and operations\n\n• Familiarity with normalized, dimensional, star schema and snowflake schematic models\n\n• Healthcare domain and data experience\n\n• Strong written and oral communication skills\n\n• 4+ years of experience working with diverse healthcare data sources, for example, claims, encounters, ADT data, FHIR, EHR data etc.\n\n• 3+ years’ using cloud-based services from AWS, GCP, or Azure\n\n• 2+ years serving data sets to both BI tools and SE applications\n\n-Preferred:\n\n• Master’s degree in healthcare-related field\n\n• Working experience with Databricks\n\n• Databricks/Microsoft Azure Certification is a plus\n\nCompany:\n\nLeading physician-centric, technology-powered, risk-bearing healthcare mgmt. company delivering high quality care in a cost-effective manner Astrana Health has a track record of offering H1B sponsorships.",
    "job_is_remote": null,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Alhambra, CA",
    "job_city": "Alhambra",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.092754899999996,
    "job_longitude": -118.1268211,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVuP-kAKuKrvGJDw8AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree required in computer science, information technology, or related field",
        "Strong understanding of database structures, theories, principles, and practices",
        "Working knowledge with programming or scripting languages such as Python, Spark, and SQL",
        "Demonstrated track record executing best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, deployment processes, testing, and operations",
        "Familiarity with normalized, dimensional, star schema and snowflake schematic models",
        "Healthcare domain and data experience",
        "Strong written and oral communication skills",
        "4+ years of experience working with diverse healthcare data sources, for example, claims, encounters, ADT data, FHIR, EHR data etc",
        "3+ years’ using cloud-based services from AWS, GCP, or Azure",
        "2+ years serving data sets to both BI tools and SE applications",
        "Master’s degree in healthcare-related field",
        "Working experience with Databricks"
      ],
      "Responsibilities": [
        "The role involves collaborating with product teams and clinical leaders to manage and enhance data products, ensuring high-quality data models and addressing data quality issues",
        "Use data engineering best practices to produce high quality, maximally available data models which are intuitive and trusted by stakeholders",
        "Scope and implement new entities for Astrana’s unified data model",
        "Interfacing with business customers, gathering requirements and developing new datasets in data platform",
        "Identifying the data quality issues to address them immediately to provide great user experience",
        "Extracting and combining data from various heterogeneous data sources",
        "Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets",
        "Modelling data and metadata to support machine learning and AI",
        "Support integration efforts from acquisitions as necessary"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Consultant (Remote)",
    "employer_name": "Releady",
    "employer_logo": null,
    "employer_website": "https://www.releady.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/consultant-data-engineer_93b50b06-e8ad-4459-ac66-68a2c9229a93?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Pangian",
        "apply_link": "https://pangian.com/remote/job/data-engineer-consultant-remote-1o?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EarnBetter",
        "apply_link": "https://earnbetter.com/app/job/01JM148F98JPMEKGYAMF1HY8HZ/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "OVERVIEW\n\nThis Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team. You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies. Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure.\n• Duration: 6+ months contract\n• Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada. Working hours will be PST. Preference for California.\n• Rate: $70/hr - $85/hr DOE\n• **Must be able to work in the United States without sponsorship***\n\nRESPONSIBILITIES\n• Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)\n• Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability\n• Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling\n• Work with Collibra for data governance, quality assurance, and metadata management\n• Leverage Refuel.ai for data mastering and Striim for data validation processes\n• Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows\n• Collaborate with client stakeholders to translate requirements into effective data solutions\n• Support data visualization and reporting needs through Tableau\n• Implement CI/CD practices using Git repositories and modern DevOps tools\n• Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners\n\nQUALIFICATIONS\n• Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)\n• 10+ years of experience in data engineering or related roles\n• Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server\n• Hands-on experience with dbt Cloud for data transformation and pipeline development\n• Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse\n• Knowledge of Data Vault modeling principles and implementation techniques\n• Experience with data governance and data quality tools, particularly Collibra\n• Familiarity with data visualization platforms, especially Tableau\n• Understanding of version control systems (Git, Bitbucket) and CI/CD practices\n• Experience with scheduling systems like Tidal or Control-M\n• Working knowledge of Agile methodologies and DevOps principles applied to data pipelines\n• Preferred Skills:\n• Experience with data observability platforms and data quality monitoring\n• Knowledge of Python, R, KNIME, or Alteryx for data science applications\n• Experience with Refuel.ai and Striim technologies\n• Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms\n• Experience with enterprise scheduling tools like Tidal\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or other non-merit factor. We are committed to creating a diverse and inclusive environment for all employees.",
    "job_is_remote": null,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Sacramento, CA",
    "job_city": "Sacramento",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 38.5781342,
    "job_longitude": -121.4944209,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D00eCRolQcTBw_16xAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 70,
    "job_max_salary": 85,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "**Must be able to work in the United States without sponsorship***",
        "Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)",
        "10+ years of experience in data engineering or related roles",
        "Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server",
        "Hands-on experience with dbt Cloud for data transformation and pipeline development",
        "Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse",
        "Knowledge of Data Vault modeling principles and implementation techniques",
        "Experience with data governance and data quality tools, particularly Collibra",
        "Familiarity with data visualization platforms, especially Tableau",
        "Understanding of version control systems (Git, Bitbucket) and CI/CD practices",
        "Experience with scheduling systems like Tidal or Control-M",
        "Working knowledge of Agile methodologies and DevOps principles applied to data pipelines",
        "Experience with data observability platforms and data quality monitoring",
        "Knowledge of Python, R, KNIME, or Alteryx for data science applications",
        "Experience with Refuel.ai and Striim technologies",
        "Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms",
        "Experience with enterprise scheduling tools like Tidal"
      ],
      "Benefits": [
        "Rate: $70/hr - $85/hr DOE"
      ],
      "Responsibilities": [
        "This Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team",
        "You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies",
        "Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure",
        "Duration: 6+ months contract",
        "Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada",
        "Working hours will be PST. Preference for California",
        "Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)",
        "Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability",
        "Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling",
        "Work with Collibra for data governance, quality assurance, and metadata management",
        "Leverage Refuel.ai for data mastering and Striim for data validation processes",
        "Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows",
        "Collaborate with client stakeholders to translate requirements into effective data solutions",
        "Support data visualization and reporting needs through Tableau",
        "Implement CI/CD practices using Git repositories and modern DevOps tools",
        "Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "URGENT by 6/25:REMOTE:Certified Data Engineer-Databricks, PySpark/Scala, ADFactory, W2 Only",
    "employer_name": "Solitsys",
    "employer_logo": null,
    "employer_website": "http://www.solitsys.com",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time and Contractor",
    "job_employment_types": [
      "FULLTIME",
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "URGENTLY NEEDED: REMOTE DATA ENGINEER- DATA BRICKS , PySpark/Scala, AZURE DATA FACTORY [W2 ONLY, ALL Corp-to-Corp WILL BE REJECTED]\n\nWe cannot offer Corp-to-Corp arrangement. This position is being offered on W2 basis only (no Corp-to-Corp or 1099), we are NOT a head-hunting agency. Please respond ASAP with your detailed resume in Word format. Resume must address the minimum qualifications listed below.\n\nDATA ENGINEER - DATABRICKS SPECIALIST\n\nAre you a skilled Data Engineer with a passion for modernizing data solutions? We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity. This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity. Time-sensitive, project-based with defined deliverables. Apply now!\n\nYOUR RESUME MUST SHOW THESE MINIMUM QUALIFICATIONS:\n• 3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling.\n• 5+ years background: PySpark or Scala for data engineering tasks.\n• 5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning.\n• 5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF).\n• 3+ years of: Delta Lake architecture and implementation for data warehousing.\n• 3+ years of experience with: BI tools such as Power BI or Tableau for reporting.\n• Familiarity with: Azure Cloud environment (Blob Storage, ADLS).\n• Proficiency in: Python scripting for data manipulation and automation.\n\nEducation & Certifications:\n• Bachelor’s degree in Computer Science, Information Systems, Data Science, or related field.\n• Databricks Certified Data Engineer Associate or relevant industry certifications.\n• Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields.\n\nJob Types: Full-time, Contract\n\nPay: $60,000.00 per year\n\nCompensation Package:\n• Hourly pay\n\nSchedule:\n• 8 hour shift\n\nEducation:\n• Bachelor's (Required)\n\nExperience:\n• Databricks: 3 years (Required)\n• Python: 5 years (Required)\n• PySpark: 5 years (Required)\n• Scala: 5 years (Required)\n• Azure Data Lake: 3 years (Required)\n• Azure Data Factory: 3 years (Required)\n• Power BI: 5 years (Preferred)\n• Tableau: 5 years (Preferred)\n\nWork Location: Remote",
    "job_is_remote": null,
    "job_posted_at": "27 days ago",
    "job_posted_at_timestamp": 1750809600,
    "job_posted_at_datetime_utc": "2025-06-25T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuINRzsWvsKKLdrOtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Resume must address the minimum qualifications listed below",
        "3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling",
        "5+ years background: PySpark or Scala for data engineering tasks",
        "5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning",
        "5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF)",
        "3+ years of: Delta Lake architecture and implementation for data warehousing",
        "3+ years of experience with: BI tools such as Power BI or Tableau for reporting",
        "Familiarity with: Azure Cloud environment (Blob Storage, ADLS)",
        "Proficiency in: Python scripting for data manipulation and automation",
        "Bachelor’s degree in Computer Science, Information Systems, Data Science, or related field",
        "Databricks Certified Data Engineer Associate or relevant industry certifications",
        "Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields",
        "Bachelor's (Required)",
        "Databricks: 3 years (Required)",
        "Python: 5 years (Required)",
        "PySpark: 5 years (Required)",
        "Scala: 5 years (Required)",
        "Azure Data Lake: 3 years (Required)",
        "Azure Data Factory: 3 years (Required)"
      ],
      "Benefits": [
        "Pay: $60,000.00 per year",
        "Compensation Package:",
        "Hourly pay",
        "8 hour shift"
      ],
      "Responsibilities": [
        "We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity",
        "This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity",
        "Time-sensitive, project-based with defined deliverables"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Senior/Lead Data Engineer , Remote, $63/HR",
    "employer_name": "FASTRA LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8iFPXl68j7QGKA_ta-TCHN1e0yurtlbKR__Xc&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Quality Contracts",
        "apply_link": "https://qualitycontracts.co.uk/contract-jobs/remote-senior-lead-data-engineer-remote-63-hr-in-usa-1752741389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior/Lead Data Engineer\n\nLocation: Remote\n\nDuration: 12 Months\n\nResponsibilities:\n• Collaborate with cross-functional stakeholders (including data scientists, analysts, and business teams) to define data requirements, processing needs, and analytics solutions.\n• Create and sustain scalable data models and efficient extraction processes.\n• Establish and monitor data quality checks and validation systems.\n• Construct and enhance business intelligence dashboards.\n• Produce comprehensive documentation for data models and processes.\n• Adapt to shifting priorities and ad-hoc requests.\n• Drive continuous improvement in data practices and visualization techniques.\n\nRequirements:\n• 5+ years of proven experience in database engineering and software development.\n• Advanced skills in SQL for complex query development and database management.\n• Strong Python programming skills for automation and data processing workflows.\n• Ability to handle large-scale data processing tasks using Spark.\n• Knowledge of data visualization tools, Qlik is preferred.\n• Familiarity with cloud platforms, particularly AWS (Glue and Athena desired).\n• Growth mindset with enthusiasm to learn new technologies.",
    "job_is_remote": null,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPrsQLoxA4Q6wEGlDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer",
    "employer_name": "Nike",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRSg56NSkRfvvux00tX_ZNojvvrKnI6i-nxdfew&s=0",
    "employer_website": "http://www.nike.com/",
    "job_publisher": "Nike Careers",
    "job_employment_type": "Full–time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.nike.com/data-engineer/job/R-61137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Nike Careers",
        "apply_link": "https://careers.nike.com/data-engineer/job/R-61137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Cardinal Health",
        "apply_link": "https://jobs.cardinalhealth.com/search/jobdetails/data-engineer/610d0c2e-65a6-4123-82b5-db5a51021d33?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://pnc.wd5.myworkdayjobs.com/External/job/OH---Strongsville/Technology-Engineer---Data-and-Automation--Python--R--Unix-_R193407-1/apply?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Wd3.Myworkdaysite.com",
        "apply_link": "https://wd3.myworkdaysite.com/recruiting/magna/Magna/job/Lowell-Massachusetts-US/Data-Engineer_R00164181?source=BuiltInNationwide&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "StarSevenSix",
        "apply_link": "https://starsevensix.com/careers/data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "TigerData",
        "apply_link": "https://www.tigerdata.com/careers/29788682-de5c-45d8-a0b8-9603c833a8d8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Viasat Careers",
        "apply_link": "https://careers.viasat.com/jobs/4560?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobvite",
        "apply_link": "https://jobs.jobvite.com/uplight/job/o5ilwfwY?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Nike Inc.- Beaverton, OR. Design and build simple reusable components of larger process or framework to support analytics products with mentorship from experienced peers; design and implement product features in collaboration with Business and Technology partners;; clean, prepare and optimize data at scale for ingestion and consumption; support the implementation of new data management projects and re-structure of the current data architecture; implement automated workflows and routines using workflow scheduling tools; understand and use continuous integration, test driven development and production deployment frameworks; Anticipate, identify and solve issues concerning data management to improve data quality. participate in design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards; analyze and profile data for the purpose of designing scalable solutions; and solve straightforward data issues and perform root cause analysis to proactively resolve product issues. Telecommuting is available from anywhere in the U.S., except from AK, AL, AR, DE, HI, IA, ID, IN, KS, KY, LA, MT, ND, NE, NH, NM, NV, OH, OK, RI, SD, VT, WV, and WY.\n\nMust have a Master's degree in Computer Science, Engineering, Electronics Engineering and 2 years of experience in the job offered or in a data-related occupation. Experience must include;\n\n• System development life cycle\n\n• Cloud platforms, such as AWS and Databricks\n\n• SQL\n\n• Version control and CI/CD pipelines\n\n• Extracting, transforming, and loading and data pipelines\n\n• Scripting and automation\n\n• Big data technologies, such as Hadoop, and Spark\n\n• Data Warehouse concepts and methodologies\n\n• Relational and nonrelational database design\n\n• Programming languages, such as Python, Java, and Scala\n\nApply at www.Nike.com/Careers (Job #R-61137)\n\n#LI-DNI\n\nWe offer a number of accommodations to complete our interview process including screen readers, sign language interpreters, accessible and single location for in-person interviews, closed captioning, and other reasonable modifications as needed. If you discover, as you navigate our application process, that you need assistance or an accommodation due to a disability, please complete the Candidate Accommodation Request Form.",
    "job_is_remote": null,
    "job_posted_at": "18 days ago",
    "job_posted_at_timestamp": 1751587200,
    "job_posted_at_datetime_utc": "2025-07-04T00:00:00.000Z",
    "job_location": "Beaverton, OR, United States",
    "job_city": "Beaverton",
    "job_state": "Oregon",
    "job_country": "US",
    "job_latitude": 45.486928299999995,
    "job_longitude": -122.80403199999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DGsDGmxGZgv_H39E5AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Must have a Master's degree in Computer Science, Engineering, Electronics Engineering and 2 years of experience in the job offered or in a data-related occupation",
        "System development life cycle",
        "Cloud platforms, such as AWS and Databricks",
        "SQL",
        "Version control and CI/CD pipelines",
        "Extracting, transforming, and loading and data pipelines",
        "Scripting and automation",
        "Big data technologies, such as Hadoop, and Spark",
        "Data Warehouse concepts and methodologies",
        "Relational and nonrelational database design",
        "Programming languages, such as Python, Java, and Scala"
      ],
      "Responsibilities": [
        "Design and build simple reusable components of larger process or framework to support analytics products with mentorship from experienced peers; design and implement product features in collaboration with Business and Technology partners;; clean, prepare and optimize data at scale for ingestion and consumption; support the implementation of new data management projects and re-structure of the current data architecture; implement automated workflows and routines using workflow scheduling tools; understand and use continuous integration, test driven development and production deployment frameworks; Anticipate, identify and solve issues concerning data management to improve data quality",
        "participate in design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards; analyze and profile data for the purpose of designing scalable solutions; and solve straightforward data issues and perform root cause analysis to proactively resolve product issues"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Nike",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 13011,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Nike/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Nike",
        "score": 4,
        "num_stars": 4,
        "review_count": 16535,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/NIKE-Reviews-E1699.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Nike",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 287,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/nike-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Growth Insights and Foundations",
    "employer_name": "Netflix",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQG6PlRytpbamOkfuuSsdXOdyiJlb7J7TCmmimh&s=0",
    "employer_website": "https://www.netflix.com/",
    "job_publisher": "Remote Rocketship",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Remote Rocketship",
        "apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description:\n• Partner closely with data scientists and other engineers to build low-latency data products\n• Ensure the availability of critical data to enhance \"in-the-moment\" experiences\n• Develop highly available and reliable distributed data systems and services\n• Optimize for the best customer experience with data insights\n• Ensure timely delivery of high-quality data for Netflix product\n\nRequirements:\n• Proficient in at least one major language preferably on the JVM stack (e.g., Java, Scala) and SQL (any variant)\n• Strive to write elegant and maintainable code\n• Comfortable with picking up new technologies\n• Have a product mindset and are curious to understand the business's needs\n• Naturally collaborative style to work with product management, data science, engineering, etc.\n• Strong data intuition and know how to apply analytical skills to support building high quality data products\n• Experience building applications that use large-scale distributed systems and data processing frameworks (batch and real-time)\n• Passionate about making data available for self-service and wider integration\n• Knowledge about transport protocols and building APIs/services and frameworks (e.g. Spring, gRPC)\n• Experience in supporting and maintaining products that run 24x7\n• Can craft scalable systems and solutions to realize a range of product and engineering goals\n• Strong operational awareness and design multi-tenant systems handling high-scale demands\n• Prioritize observability in designs with comprehensive monitoring, logging, and alerting\n• Own what you build and have a passion for quality\n• Comfortable working in agile environments with vague requirements\n• Nimble and can pivot easily when needed\n• Unafraid to take smart risks\n\nBenefits:\n• Health Plans\n• Mental Health support\n• 401(k) Retirement Plan with employer match\n• Stock Option Program\n• Disability Programs\n• Health Savings and Flexible Spending Accounts\n• Family-forming benefits\n• Life and Serious Injury Benefits\n• Paid leave of absence programs\n• Paid time off",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6UkieosGdj3DK2hqAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 170000,
    "job_max_salary": 720000,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Netflix",
        "score": 4,
        "num_stars": 4,
        "review_count": 820,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Netflix/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Netflix",
        "score": 4.2,
        "num_stars": 4,
        "review_count": 3404,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Netflix-Reviews-E11891.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Netflix",
        "score": 4,
        "num_stars": 4,
        "review_count": 43,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/netflix-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer (Full-Time, Remote, North Carolina Based)",
    "employer_name": "Alliance Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQc8LYAEYcIakIeMQVnhkBhH5cN4eBzxq9dkqiN&s=0",
    "employer_website": "https://www.alliancehealthplan.org",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Alliance/Job/Data-Engineer-(Full-Time,-Remote,-North-Carolina-Based)/-in-Morrisville,NC?jid=63c088f9feb31195&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-full-time-remote-north-carolina-based-alliance-health-JV_KO0,51_KE52,67.htm?jl=1009695680692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-full-time-remote-north-carolina-based-at-alliance-health-4256300763?refId=ZZ8HCMbeJKzEURTwY97AHQ%3D%3D&trackingId=Ty0o5ZD5tUD13LeFLBMwKA%3D%3D&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/685c8729de1ebd49c8be30eb-data-engineer-full-time-remote-north-carolina-based?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5274727037?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://vividhireio.com/job/606346?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kinetichires",
        "apply_link": "https://kinetichires.net/job/data-analyst-diversity-equity-inclusion-and-health-equity-full-time-remote-north-carolina-based-264200?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages. The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities.\n\nThis position is fulltime remote. Selected candidate must reside in North Carolina. Some travel for onsite meetings to the Home office at Morrisville may be required.\n\nResponsibilities & Duties\n• Analyze business and technical requirements for the design of data integration solutions\n• Define the overall data integration and dataflow architectures to support data integration projects\n• Design and develop SQL and SSIS processes to support data integration projects\n• Design and develop APIs to consume and distribute healthcare data\n• Design, develop and execute unit testing plans\n• Ensure data quality and integrity in all data integration projects\n• Develop technical and business process documentation for data integration projects\n• Maintain and continually improve data integration projects\n• Assist in establishing standards for the design, development, implementation and support of data integration projects\n• Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders\n• Any other tasks as reasonably required\n\nMinimum Requirements\n\nEducation & Experience:\n• Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position.\n• Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis.\n\nPreferred\n• Bachelor’s degree plus five (5) years of experience in a computer science related field including experience in a data integration or ETL development position including developing complex data integration software applications.\n• Microsoft Certified Solutions Expert, MuleSoft Certified Developer and/or HL7 Certifications.\n\nKnowledge, Skills, & Abilities\n• Expert programming in SQL\n• Proficient designing and developing ETL processes, preferably using SSIS\n• Proficient designing and developing APIs, preferably using .NET Framework\n• Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI\n• Experience working with API management and data integration platforms such as Apigee or MuleSoft\n• Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP\n• Experience working with HIEs and/or HISPs\n• Strong communication and organizational skills\n• Ability to access and analyze large data sets for completeness and quality\n• Ability to work independently and in a team setting\n\nEmployment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date.\n\nSalary Range\n\n$102,424-$130,591/Annually\n\nExact compensation will be determined based on the candidate's education, experience, external market data and consideration of internal equity.\n\nAn excellent fringe benefit package accompanies the salary, which includes:\n• Medical, Dental, Vision, Life, Long Term Disability\n• Generous retirement savings plan\n• Flexible work schedules including hybrid/remote options\n• Paid time off including vacation, sick leave, holiday, management leave\n• Dress flexibility\n\nEqual Opportunity Employer\nThis employer is required to notify all applicants of their rights pursuant to federal employment laws. For further information, please review the Know Your Rights notice from the Department of Labor.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "North Carolina",
    "job_city": null,
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.7595731,
    "job_longitude": -79.01929969999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dj0uGbWgrvId1fahdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 102424,
    "job_max_salary": 130591,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Selected candidate must reside in North Carolina",
        "Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position",
        "Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis",
        "Knowledge, Skills, & Abilities",
        "Expert programming in SQL",
        "Proficient designing and developing ETL processes, preferably using SSIS",
        "Proficient designing and developing APIs, preferably using .NET Framework",
        "Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI",
        "Experience working with API management and data integration platforms such as Apigee or MuleSoft",
        "Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP",
        "Experience working with HIEs and/or HISPs",
        "Strong communication and organizational skills",
        "Ability to access and analyze large data sets for completeness and quality",
        "Ability to work independently and in a team setting",
        "Employment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date"
      ],
      "Benefits": [
        "$102,424-$130,591/Annually",
        "An excellent fringe benefit package accompanies the salary, which includes:",
        "Medical, Dental, Vision, Life, Long Term Disability",
        "Generous retirement savings plan",
        "Flexible work schedules including hybrid/remote options",
        "Paid time off including vacation, sick leave, holiday, management leave",
        "Dress flexibility"
      ],
      "Responsibilities": [
        "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages",
        "The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities",
        "This position is fulltime remote",
        "Analyze business and technical requirements for the design of data integration solutions",
        "Define the overall data integration and dataflow architectures to support data integration projects",
        "Design and develop SQL and SSIS processes to support data integration projects",
        "Design and develop APIs to consume and distribute healthcare data",
        "Design, develop and execute unit testing plans",
        "Ensure data quality and integrity in all data integration projects",
        "Develop technical and business process documentation for data integration projects",
        "Maintain and continually improve data integration projects",
        "Assist in establishing standards for the design, development, implementation and support of data integration projects",
        "Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders",
        "Any other tasks as reasonably required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Principal Data Engineer - Remote US",
    "employer_name": "Seamless.AI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS5BfkbQOcbcbdeleTXv86f7_9MAYLP6hfW2oz8&s=0",
    "employer_website": "https://seamless.ai",
    "job_publisher": "Wellfound",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://wellfound.com/jobs/3227170-principal-data-engineer-remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Wellfound",
        "apply_link": "https://wellfound.com/jobs/3227170-principal-data-engineer-remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Seamless.AI/Job/Principal-Data-Engineer-Remote-US/-in-Columbus,OH?jid=d298d1770fe754ee&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=a0a3d5a80823c1a4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/principal-data-engineer-remote-us/4275987?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67c03123b965ace1d468e64f-principal-data-engineer---remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Startup Jobs",
        "apply_link": "https://startup.jobs/principal-data-engineer-remote-us-seamlessai-2-6223376?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-us_04bbb179-c9c8-4960-851f-91caa0a4f220?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://jooble.org/jdp/4971728553806184117?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Opportunity:\n\nAt Seamless.AI, we’re seeking a highly skilled and experienced Principal Data Engineer with expertise in Python, Spark, AWS Glue, and other ETL (Extract, Transform, Load) technologies. The ideal candidate will have a proven track record in data acquisition and transformation, as well as experience working with large data sets and applying methodologies for data matching and aggregation. Strong organizational skills and the ability to work independently as a self-starter are essential for this role.\n\nResponsibilities:\n• Design, develop, and maintain robust and scalable ETL pipelines to acquire, transform, and load data from various sources into our data ecosystem.\n• Collaborate with cross-functional teams to understand data requirements and develop efficient data acquisition and integration strategies.\n• Implement data transformation logic using Python and other relevant programming languages and frameworks.\n• Utilize AWS Glue or similar tools to create and manage ETL jobs, workflows, and data catalogs.\n• Optimize and tune ETL processes for improved performance and scalability, particularly with large data sets.\n• Apply methodologies and techniques for data matching, deduplication, and aggregation to ensure data accuracy and quality.\n• Implement and maintain data governance practices to ensure compliance, data security, and privacy.\n• Collaborate with the data engineering team to explore and adopt new technologies and tools that enhance the efficiency and effectiveness of data processing.\n\nSkillset:\n• Strong proficiency in Python and experience with related libraries and frameworks (e.g., pandas, NumPy, PySpark).\n• Hands-on experience with AWS Glue or similar ETL tools and technologies.\n• Solid understanding of data modeling, data warehousing, and data architecture principles.\n• Expertise in working with large data sets, data lakes, and distributed computing frameworks.\n• Experience developing and training machine learning models.\n• Strong proficiency in SQL.\n• Familiarity with data matching, deduplication, and aggregation methodologies.\n• Experience with data governance, data security, and privacy practices.\n• Strong problem-solving and analytical skills, with the ability to identify and resolve data-related issues.\n• Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.\n• Highly organized and self-motivated, with the ability to manage multiple projects and priorities simultaneously.\n\nEducation and Requirements:\n• Bachelor's degree in Computer Science, Information Systems, related fields or equivalent years of work experience.\n• 7+ years of experience as a Data Engineer, with a focus on ETL processes and data integration.\n• Professional experience with Spark and AWS pipeline development required.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Columbus, OH",
    "job_city": "Columbus",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.9625112,
    "job_longitude": -83.00322179999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D7jGiM9T-GjtGDCWIAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate will have a proven track record in data acquisition and transformation, as well as experience working with large data sets and applying methodologies for data matching and aggregation",
        "Strong organizational skills and the ability to work independently as a self-starter are essential for this role",
        "Strong proficiency in Python and experience with related libraries and frameworks (e.g., pandas, NumPy, PySpark)",
        "Hands-on experience with AWS Glue or similar ETL tools and technologies",
        "Solid understanding of data modeling, data warehousing, and data architecture principles",
        "Expertise in working with large data sets, data lakes, and distributed computing frameworks",
        "Experience developing and training machine learning models",
        "Strong proficiency in SQL",
        "Familiarity with data matching, deduplication, and aggregation methodologies",
        "Experience with data governance, data security, and privacy practices",
        "Strong problem-solving and analytical skills, with the ability to identify and resolve data-related issues",
        "Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams",
        "Highly organized and self-motivated, with the ability to manage multiple projects and priorities simultaneously",
        "Bachelor's degree in Computer Science, Information Systems, related fields or equivalent years of work experience",
        "7+ years of experience as a Data Engineer, with a focus on ETL processes and data integration",
        "Professional experience with Spark and AWS pipeline development required"
      ],
      "Responsibilities": [
        "Design, develop, and maintain robust and scalable ETL pipelines to acquire, transform, and load data from various sources into our data ecosystem",
        "Collaborate with cross-functional teams to understand data requirements and develop efficient data acquisition and integration strategies",
        "Implement data transformation logic using Python and other relevant programming languages and frameworks",
        "Utilize AWS Glue or similar tools to create and manage ETL jobs, workflows, and data catalogs",
        "Optimize and tune ETL processes for improved performance and scalability, particularly with large data sets",
        "Apply methodologies and techniques for data matching, deduplication, and aggregation to ensure data accuracy and quality",
        "Implement and maintain data governance practices to ensure compliance, data security, and privacy",
        "Collaborate with the data engineering team to explore and adopt new technologies and tools that enhance the efficiency and effectiveness of data processing"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Google Cloud Platform Data Engineer(Fulltime) for Remote",
    "employer_name": "Amaze Systems Inc",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ3j6-Rx9U3c7Gzc5qfkYkFeEVFSNq9iCBifDLC&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Full–time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs™ - DentaQuest",
        "apply_link": "https://dev-cm.dentaquest.com/job/work-from-home-system-data-analyst-google-cloud-platform-data-fn8ov.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://jobxpedia.com/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://buzzcloud.in/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Fulltime opportunity\n\nRole: Google Cloud Platform Data Engineer\n\nLocation: Remote - USA\n\nDuration: FTE only\n\nJob Description:\n• 10+ years' proven experience as a Data Engineer with a focus on Google Cloud Platform services.\n• Strong proficiency in Google Cloud Platform services such as GCS, Dataflow with Apache Beam (Batch & Stream data processing), BigQuery, cloud Composer and Pub/Sub.\n• Proficiency in SQL and Python for data manipulation and analysis is mandatory.\n• Solid understanding of data warehousing concepts and ETL processes.\n\nThanks &Regards\n\nRahul Sharma | Lead Technical Recruiter\nAmaze Systems Inc\n\nE: |",
    "job_is_remote": null,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1752883200,
    "job_posted_at_datetime_utc": "2025-07-19T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVSDv3_uV9S8qF8byAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - MLOps | Remote in the USA - 1725",
    "employer_name": "PlacingIT",
    "employer_logo": null,
    "employer_website": "https://www.placingit.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - MLOps | Remote in the USA - 1725\n\nLocation: Hybrid - Onsite 4 days a week in Dallas, TX\nEmployment Type 6-12+ months contract w/possible extensions\nInterview: In-Person interview is required for this role.\nResidency Requirements: Those authorized to work in the US are encouraged to apply.\n\nRequired Qualifications\n• 5+ years of software development experience building and delivering customer facing cloud-based analytic solutions.\n• 5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives.\n• 5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope.\n• 5+ years coding in Python/PySpark, SQL, and LLM modeling.\n• 5+ years in cloud-based platforms such as Azure, GCP and AWS.\n\nPreferred Qualifications\n• 5+ years of experience leading the design, development, and deployment of applications on Palantir Foundry Platform\n• Retail merchandising domain expertise including category management, assortment optimization, product clustering, product price sensitivity and promotion affinity evaluation, store clustering and localization.\n• Experience with Prompt Engineering developing solutions leveraging LLM models.\n\nResponsibilities include:\n• You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions.\n• You will solve complex problems and deliver decision support tools to improve customer experience.\n• Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses.\n• Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces.\n• Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses.\n• Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry.\n• Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle.\n• Provide technical guidance, mentorship, and support to junior developers and team members.\n• Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem.",
    "job_is_remote": null,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1752537600,
    "job_posted_at_datetime_utc": "2025-07-15T00:00:00.000Z",
    "job_location": "Dallas, TX, United States",
    "job_city": "Dallas",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7766642,
    "job_longitude": -96.79698789999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRxeJao3yHoNlD18FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Residency Requirements: Those authorized to work in the US are encouraged to apply",
        "5+ years of software development experience building and delivering customer facing cloud-based analytic solutions",
        "5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives",
        "5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope",
        "5+ years coding in Python/PySpark, SQL, and LLM modeling",
        "5+ years in cloud-based platforms such as Azure, GCP and AWS"
      ],
      "Responsibilities": [
        "You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions",
        "You will solve complex problems and deliver decision support tools to improve customer experience",
        "Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses",
        "Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces",
        "Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses",
        "Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry",
        "Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle",
        "Provide technical guidance, mentorship, and support to junior developers and team members",
        "Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": null,
    "job_title": "Senior Data Engineer - Full remote",
    "employer_name": "All European Careers",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsGyayz3Bns7zN430bb2TN0tCgmnP5cORT61YZ&s=0",
    "employer_website": "https://www.all-european-careers.com",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JOBITT",
        "apply_link": "https://jobitt.com/job-openings/external/senior-engineer-full-stack-7870835690680623803?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: Europe\n\nThis is a remote position.\n\nFor an International Institution in Geneva, we are urgently looking for an experienced Senior Data Engineer preferably with Azure experience in Geneva or Full remote.As Senior Data Engineer, you work with the Cloud Architect, Data Architect, Solution Engineer and other technical professionals to act as technical focal point for the development of prototypes, providing advice and technical feasibility view, and other technical activities to ensure the Cloud data Platform architecture and technical configuration will address business and IT objectives.\n\nCandidates need to be fluent in English. This positions is long-term. Work permit not required.Candidates need to be based in Europe.\n\nTasks and Responsibilities:\n• Plan the activites and concrete milestones to deliver the new Cloud Data Platform Architecture;\n• Lead the development and technical implementation aligned to the Data Platform Architecture;\n• Provide expertise and technical advice in the development of conceptual, logical and physical data models, in support of interoperability protocols, API design and management, cybersecurity aspects and business intelligence;\n• Create the data factory pipelines to ingest data, apply data transformations to curate data using databricks, and make data available for downstream applications (API) or reporting from SQL database, Cosmos DB or Synapse Analytics;\n• Automation and development lifecycle control by development of rbooks, fctions, devops projects, sync to repos, ARM templates and Azure Blueprints;\n• Working with the Senior Solution Engineer derstand and catalogue the current landscape of Data and systems;\n• Develop and maintain the to-be Cloud Data Platform with the required capabilities in conformance with security requirements, agreements-based, with user-driven configuration and through documented design and configuration;\n• Provide technical expertise regarding short terms solution options to leverage in the immediate future to meet urgent data related demands, with consideration of the organization’s wider needs and the potential of deploying enterprise data solution platforms at the enterprise level;\n• Provide data insights and best practices, ensuring they are reflected in the development;\n\nProfile:\n• Bachelor or Master degree;\n• +5 years of relevant experience as Data Engineer;\n• Experience with modern Data technologies such as Business Intelligence, Analytics, AI, and Big Data;\n• Experience programming multiple languages, e.g. Phyton, R, Java, Scala, etc; Professional level vendor certifications, such as Microsoft, ITIL, and others;\n• Extensive experience with Microsoft Azure and other cloud technologies and interoperable solutions,experience as a data analyst, engineer and developer;\n• Demonstrated experience in the design, development, and implementation of various integrations between diverse infrastructure services, data models and architecture;\n• Demonstrated experience with the IT systems development life cycle (SDLC), as well as Agile/Scrum methodologies and ITIL processes;\n• Ability to conduct requirements gathering, interpret needs, and design solutions and manage expectations;\n• Professional experience in technical design and support of global, distributed corporate information systems;\n• Understanding state of Azure, Google and AWS components and reference architectures;\n\nExperience in designing for Bigdata/data warehousing/business intelligence/reporting solutions for various physical environments, both on-premises as well as in the Microsoft cloud, while influencing for the most efficient approach based on business requirements;\n\nExcellent written and spoken English;\n\nInterested: Please send your resume to:\nresume@all-european-careers.com\n\nThis offer from \"All European Careers\" has been enriched by Jobgether.com and got a 77% flex score.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DSjsMVNnmi0YDQr4jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineers (Hybrid | DC Area) Remote / Telecommute Jobs",
    "employer_name": "Rackner",
    "employer_logo": null,
    "employer_website": "https://rackner.com",
    "job_publisher": "Security Clearance Jobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Security Clearance Jobs",
        "apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Data Engineer\n\nLocation: Falls Church, VA (Hybrid)\n\nClearance: Secret Clearance\n\nAbout this role:\n\nRackner is looking for a Data Engineer that will be working within an Agile DevSecOps team environment using latest cloud-native technologies to architect and implement containerized applications, CI/CD pipelines, and Kubernetes platforms using best practices and leading technologies.\n\nWe are seeking professionals with:\n\nB.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms\n\nModern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)\n\nDeveloping data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules\n\nMature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR\n\nNice to have:\n\nM.S in Computer Science or related\n\nAI/ML\n\nKubernetes (Rancher RKE2, AWS EKS) and microservice architectures\n\nData engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR\n\nNIST Risk Management Framework and security accreditation process and tasks\n\nWhat will make you successful:\n\nUsing DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer\n\nDeveloping software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes\n\nLeveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs\n\nApplying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders\n\nEmbracing a shared responsibility for system security\n\nPerforming threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle\n\nContinuously engaging with project teams to deliver quality products\n\nParticipating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions\n\nWho We Are:\n\nRackner is a software consultancy that builds cloud-native solutions for startups, enterprises, and the public sector.\n\nWe are an energetic, growing consultancy with a passion for solving big problems for both startups and enterprises.\n\nEach of us enable digital transformation for large organizations through the newest in distributed technologies as we are laser focused on end-to-end application development, DevSecOps, AI/ML and systems architecture and our methodology focuses on cloud-first and cost-effective innovation.\n\nOur customers hail from a diverse, ever-growing list of industries.\n\nBenefits/Additional Info:\n\nRackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided . Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!\n\n401K with 100% matching up to 6%\n\nHighly competitive PTO\n\nGreat health insurance with large network of providers\n\nMedical/Dental/Vision\n\nLife Insurance, and short & long term disability\n\nIndustry-Leading Weekly Pay Schedule\n\nHome office & equipment plan\n\n#DataEngineer #AWS #Topsecret #FDA #publictrust #DataIngesting #DataPipeline #Python #Terraform #ETL #AI #ML #dataintegration #bigdataanalyticspipeline #awsbigdata #hadoop #apachespark #RDBMS #awsdynamoDB #collaboration #diversity #equity #Inclusion",
    "job_is_remote": null,
    "job_posted_at": "12 hours ago",
    "job_posted_at_timestamp": 1753128000,
    "job_posted_at_datetime_utc": "2025-07-21T20:00:00.000Z",
    "job_location": "Falls Church, VA",
    "job_city": "Falls Church",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.882334,
    "job_longitude": -77.1710914,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DL2b_MhwdllUxOf0jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "B.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms",
        "Modern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)",
        "Developing data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules",
        "Mature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR",
        "M.S in Computer Science or related",
        "Kubernetes (Rancher RKE2, AWS EKS) and microservice architectures",
        "Data engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR",
        "NIST Risk Management Framework and security accreditation process and tasks"
      ],
      "Benefits": [
        "Rackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided ",
        "Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!",
        "401K with 100% matching up to 6%",
        "Highly competitive PTO",
        "Great health insurance with large network of providers",
        "Life Insurance, and short & long term disability",
        "Industry-Leading Weekly Pay Schedule",
        "Home office & equipment plan"
      ],
      "Responsibilities": [
        "Using DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer",
        "Developing software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes",
        "Leveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs",
        "Applying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders",
        "Embracing a shared responsibility for system security",
        "Performing threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle",
        "Continuously engaging with project teams to deliver quality products",
        "Participating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Principal Data Engineer *Remote - Most states eligible*",
    "employer_name": "Providence",
    "employer_logo": null,
    "employer_website": "http://www.psjhealth.org/",
    "job_publisher": "Teal",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the position\n\nThe Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making. This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics. The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St. Joseph Health system, focusing on quality and value of care metrics. The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights.\n\nResponsibilities\n• Collect and analyze clinical data from various sources.\n,\n• Create visualizations and storytelling using data to communicate findings.\n,\n• Develop novel reporting platforms to address healthcare challenges.\n,\n• Collaborate with clinical and administrative leaders to evaluate new measures.\n,\n• Identify impactful findings in large clinical data stores.\n,\n• Design strong data visualizations to communicate key information across the organization.\n,\n• Support team members with experience in computational methods.\n,\n• Develop innovative methods for outcome measures in the whole person care model.\n\nRequirements\n• Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience.\n,\n• 9 years of relevant data analysis experience, preferably in a biomedical setting.\n,\n• Experience with database query and analysis languages (e.g., SQL, R, SAS, Python).\n,\n• Proficiency in data visualization tools (e.g., Tableau, D3).\n\nNice-to-haves\n• Ph.D. in a related field or equivalent experience.\n,\n• Certification in an IT discipline, application, or tool upon hire.\n,\n• Lean certification or Green Belt, Black Belt upon hire.\n,\n• Certification in Data Science upon hire.\n,\n• 4+ years of development experience in data exchange using FHIR.\n\nBenefits\n• 401(k) retirement savings plan with employer matching.\n,\n• Health care benefits (medical, dental, vision).\n,\n• Life insurance.\n,\n• Disability insurance.\n,\n• Paid parental leave.\n,\n• Vacation and holiday time off.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Renton, WA",
    "job_city": "Renton",
    "job_state": "Washington",
    "job_country": "US",
    "job_latitude": 47.4796927,
    "job_longitude": -122.2079218,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXswWukx0iLqtemRoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience",
        "9 years of relevant data analysis experience, preferably in a biomedical setting",
        "Experience with database query and analysis languages (e.g., SQL, R, SAS, Python)",
        "Proficiency in data visualization tools (e.g., Tableau, D3)",
        "Ph.D. in a related field or equivalent experience",
        "Certification in an IT discipline, application, or tool upon hire",
        "Lean certification or Green Belt, Black Belt upon hire",
        "Certification in Data Science upon hire",
        "4+ years of development experience in data exchange using FHIR"
      ],
      "Benefits": [
        "401(k) retirement savings plan with employer matching",
        "Health care benefits (medical, dental, vision)",
        "Life insurance",
        "Disability insurance",
        "Paid parental leave",
        "Vacation and holiday time off"
      ],
      "Responsibilities": [
        "The Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making",
        "This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics",
        "The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St",
        "The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights",
        "Collect and analyze clinical data from various sources",
        "Create visualizations and storytelling using data to communicate findings",
        "Develop novel reporting platforms to address healthcare challenges",
        "Collaborate with clinical and administrative leaders to evaluate new measures",
        "Identify impactful findings in large clinical data stores",
        "Design strong data visualizations to communicate key information across the organization",
        "Support team members with experience in computational methods",
        "Develop innovative methods for outcome measures in the whole person care model"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Indeed",
        "employer_name": "Providence",
        "score": 3.8,
        "num_stars": 4,
        "review_count": 3699,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Providence-959155fe/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Glassdoor",
        "employer_name": "Providence",
        "score": 3.5,
        "num_stars": 3.5,
        "review_count": 6699,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Providence-Reviews-E4651.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "AWS Data Engineer - Fully Remote - US Only",
    "employer_name": "Scalepex",
    "employer_logo": null,
    "employer_website": "https://scalepex.com",
    "job_publisher": "Jobs By Workable",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://apply.workable.com/scalepex/j/F5ED805D80?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobs By Workable",
        "apply_link": "https://apply.workable.com/scalepex/j/F5ED805D80?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Insight Global Jobs",
        "apply_link": "https://jobs.insightglobal.com/find_a_job/connecticut/job-313330/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ApplicantSite",
        "apply_link": "https://applicants.bairesdev.com/job/190/252655/apply?utm_source=trampedecasa&utm_medium=jobposting&utm_campaign=Remote-20240530?utm_source%3Dtrampedecasautm_medium%3Dreferral&utm_campaign=vaga-publicada-na-trampedecasa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "DailyRemote",
        "apply_link": "https://dailyremote.com/remote-job/aws-data-engineer-fully-remote-us-only-3512285?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Ladders",
        "apply_link": "https://www.theladders.com/job/aws-data-engineer-remote-nava-software-solutions-virtual-travel_82025315?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "❋ Why Scalepex?\n\nScalepex is a dynamic services firm specializing in providing solutions for premium brands like Nike, Pepsi, Toyota, Virgin and Walgreens. Our mission is to connect prominent market leaders with top-tier professionals from around the world, fostering collaboration, efficiency, and growth.\n\n❋ Take your portfolio to the next level by working with one of our fastest growing clients.\n\nJoin the Innovation Frontier at Scalepex!\n\nAbout the Role\n\nWe are seeking an experienced AWS Data Engineer with a strong background in building scalable data solutions and expertise in utilities-related datasets. The ideal candidate will have at least 5 years of experience in data engineering, a deep understanding of distributed systems, and proficiency with AWS services and tools like Step Functions, Lambda, Glue, and Redshift. This role will focus on designing, developing, and optimizing data pipelines to support analytics and decision-making in the utilities industry.\n\nKey Responsibilities\n• Design and Build Data Pipelines: Develop scalable, reliable data pipelines using AWS services (e.g., Glue, S3, Redshift) to process and transform large datasets from utility systems like smart meters or energy grids.\n• Workflow Orchestration: Use AWS Step Functions to orchestrate workflows across data pipelines; experience with Airflow is acceptable but Step Functions is preferred.\n• Data Integration and Transformation: Implement ETL/ELT processes using PySpark, Python, and Pandas to clean, transform, and integrate data from multiple sources into unified datasets.\n• Distributed Systems Expertise: Leverage experience with complex distributed systems to ensure reliability, scalability, and performance in handling large-scale utility data.\n• Serverless Application Development: Use AWS Lambda functions to build serverless solutions for automating data processing tasks.\n• Data Modeling for Analytics: Design data models tailored for utilities use cases (e.g., energy consumption forecasting) to enable advanced analytics\n• Optimize Data Pipelines: Continuously monitor and improve the performance of data pipelines to reduce latency, enhance throughput, and ensure high availability.\n• Ensure Data Security and Compliance: Implement robust security measures to protect sensitive utility data and ensure compliance with industry regulations.\n\nRequired Qualifications\n• Minimum of 5 years of experience in data engineering\n• Proficiency in AWS services such as Step Functions, Lambda, Glue, S3, DynamoDB, and Redshift.\n• Strong programming skills in Python with experience using PySpark and Pandas for large-scale data processing.\n• Hands-on experience with distributed systems and scalable architectures.\n• Knowledge of ETL/ELT processes for integrating diverse datasets into centralized systems.\n• Familiarity with utilities-specific datasets (e.g., smart meters, energy grids) is highly desirable.\n• Strong analytical skills with the ability to work on unstructured datasets.\n• Knowledge of data governance practices to ensure accuracy, consistency, and security of data.\n• Strong experience in AWS data engineering\n• Ability to work independently\n• Ability to work with a cross-functional teams, including interfacing and communicating with business stakeholders\n• Professional oral and written communication skills\n• Strong problem solving and troubleshooting skills with experience exercising mature judgement\n• Excellent teamwork and interpersonal skills\n• Ability to obtain and maintain the required clearance for this role",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DbMI3tTg8CkVNRcoxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Senior Data Engineer, Customer Facing  (US Remote)",
    "employer_name": "LeanTaaS",
    "employer_logo": null,
    "employer_website": "http://leantaas.com/",
    "job_publisher": "Built In",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://builtin.com/job/senior-data-engineer-customer-facing-us-remote/3060713?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/senior-data-engineer-customer-facing-us-remote/3060713?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs In USA",
        "apply_link": "https://todayremotejobs.com/job/582664?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://distantgigsio.com/job/556721?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talenttundra",
        "apply_link": "https://talenttundra.com/job/sr-sql-engineer-healthcare-us-remote-236249?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We are a growth stage company that creates software solutions combining lean principles, predictive and prescriptive analytics, and machine learning to transform hospital and infusion center operations. More than 180 health systems and over 1,000 hospitals and centers across 49 states rely on our award-winning products to increase patient access, decrease wait times, and reduce healthcare delivery costs. We have raised more than $300 million from top-tier investors such as Bain Capital, Insight Partners, and Goldman Sachs. We have been named among the top 100 AI companies in the world.\n\nPlease note that while this role is listed as available for remote, we are currently employing in the following states: AK, AZ, CA, CO, CT, DC, FL, GA, IL, IN, KS, LA, MA, MD, ME, MI, MO, MT, NC, NH, NJ, NY, OH, OR, PA, SC, TN, TX, UT, VA, WA, WI, WV. If your state is not listed, we may not be able to proceed with your application. We have offices in Santa Clara, CA and Charlotte, NC for employees who prefer to work regularly or occasionally from an office.\n\nWe are seeking a Senior Data Engineer to join the team responsible for managing the data integrations of our hospital inpatient SaaS product. Data is what powers our platform and empowers our users. This is a hands-on role that requires skills in engineering, data wrangling, and good old-fashioned customer charm to engage both technical and clinical staff at hospitals.\n\nWHAT YOU’LL DO\n• Manage, write, and maintain hospital data ingestion SQL queries and configuration in our custom system\n• Work with hospital IT teams to obtain data extracts and ensure timely integrations\n• Manage data warehouse fidelity, ensuring stakeholders trust the data and understand both its opportunities and limitations\n• Ensure client data feeds are operational and error-free\n• Oversee data specifications, technical designs, and customer-specific glossaries\n• Work with backend engineering and product to set technical direction for the data ingestion pipeline for performance, scale, and reliability\n• Assist sales team with pre-sales technical requirements and phased onboarding plans\n\nWHAT YOU’LL BRING\n• 3+ years of SQL experience\n• Expertise in building relationships and trust with clients\n• Excellent written and verbal communication skills\n• Knowledge of hospital data protocols such as HL7 and FHIR\n• Experience with one or more EHR vendors\n• Experience working in or with hospital IT teams\n• Empathy for our users\n\nBONUS POINTS IF YOU HAVE\n• Clinical or direct patient care experience\n• Python or similar programming languages\n\nWHAT YOU'LL GET\n• Intellectual and emotional satisfaction of solving tough operational problems in healthcare while improving patient access and saving lives!\n• Competitive compensation package that includes base salary, target bonus, and stock options\n• 401(k) Match\n• Comprehensive healthcare benefits\n• Generous Paid Time Off and Parental Leave\n• Monthly reimbursement for Skill Building\n• Monthly reimbursement for Wellness, Transportation, and/or Home Office\n• Education Reimbursement for select courses/programs\n\nVaccination policy\n\nWe have an obligation to protect our employees, our customers, and the patients of our customers. Therefore, COVID-19 vaccination is required to work from the office, attend in-person company events, or to travel on behalf of the company.\n\nCandidates must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position.\n\nLeanTaaS is an equal opportunity employer committed to promoting an inclusive work environment free of discrimination and harassment. We value diversity, inclusion, and aim to provide a sense of belonging for everyone. All qualified applicants for employment will be considered without regard to race, color, sex, gender identity, gender expression, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other basis protected by federal, state or local laws. If you require assistance during the application process, please reach out to accommodations@leantaas.com. LeanTaaS will reasonably accommodate qualified individuals with disabilities to the extent required by applicable law.\n\nPlease note: LeanTaaS is not accepting agency resumes at this time, and we are not responsible for any fees related to unsolicited resumes. Thank you.",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DGg8QqwU2vYyezjEoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 85000,
    "job_max_salary": 145000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "3+ years of SQL experience",
        "Expertise in building relationships and trust with clients",
        "Excellent written and verbal communication skills",
        "Knowledge of hospital data protocols such as HL7 and FHIR",
        "Experience with one or more EHR vendors",
        "Experience working in or with hospital IT teams",
        "Empathy for our users",
        "Clinical or direct patient care experience",
        "Python or similar programming languages",
        "Candidates must be legally authorized to work in the United States",
        "Verification of employment eligibility will be required at the time of hire"
      ],
      "Benefits": [
        "Intellectual and emotional satisfaction of solving tough operational problems in healthcare while improving patient access and saving lives!",
        "Competitive compensation package that includes base salary, target bonus, and stock options",
        "401(k) Match",
        "Comprehensive healthcare benefits",
        "Generous Paid Time Off and Parental Leave",
        "Monthly reimbursement for Skill Building",
        "Monthly reimbursement for Wellness, Transportation, and/or Home Office",
        "Education Reimbursement for select courses/programs"
      ],
      "Responsibilities": [
        "Data is what powers our platform and empowers our users",
        "This is a hands-on role that requires skills in engineering, data wrangling, and good old-fashioned customer charm to engage both technical and clinical staff at hospitals",
        "Manage, write, and maintain hospital data ingestion SQL queries and configuration in our custom system",
        "Work with hospital IT teams to obtain data extracts and ensure timely integrations",
        "Manage data warehouse fidelity, ensuring stakeholders trust the data and understand both its opportunities and limitations",
        "Ensure client data feeds are operational and error-free",
        "Oversee data specifications, technical designs, and customer-specific glossaries",
        "Work with backend engineering and product to set technical direction for the data ingestion pipeline for performance, scale, and reliability",
        "Assist sales team with pre-sales technical requirements and phased onboarding plans"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "LeanTaaS",
        "score": 3.8,
        "num_stars": 4,
        "review_count": 142,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/LeanTaaS-Reviews-E431771.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Comparably",
        "employer_name": "LeanTaaS",
        "score": 2.4,
        "num_stars": 2.5,
        "review_count": 106,
        "max_score": 5,
        "reviews_link": "https://www.comparably.com/companies/leantaas?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "RepVue",
        "employer_name": "LeanTaaS",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 2,
        "max_score": 5,
        "reviews_link": "https://www.repvue.com/companies/leantaas?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Lead Data Engineer (Data Management & Governance)",
    "employer_name": "Temus",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRCeKPOyYXp1LSpntg3JXC6rHqoKW_MlcoF-NO_&s=0",
    "employer_website": "https://temus.com",
    "job_publisher": "LinkedIn Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.linkedin.com/jobs/view/lead-data-engineer-data-management-governance-at-temus-4268385166?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/lead-data-engineer-data-management-governance-at-temus-4268385166?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/data-engineer-data-management-governance-jobs/41361185AB795804?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Temus was established by Temasek in partnership with UST, to provide digital transformation solutions for the private and public sectors as we aspire to be a strategic partner in realising the Singapore Government’s Smart Nation vision. We are headquartered in Singapore and have more than 400 employees across a wide range of disciplines in strategy, design, architecture, technology, data & AI.\n\nYour objectives\n• Solve complex data management challenges and deliver measurable business value through governance transformation\n• Lead data maturity assessments and develop transformation roadmaps for enterprise clients across various industries\n• Guide clients in establishing and scaling data analytics teams, from organizational design to capability building\n• Design and implement comprehensive data governance frameworks, policies, and operating models aligned with business objectives\n• Build and deploy metadata management solutions, data catalogues, and lineage tracking systems\n• Facilitate executive workshops and present data strategy recommendations to C-suite stakeholders\n• Drive adoption of DAMA-DMBOK principles and industry best practices across client organizations\n• Lead pre-sales engagements, solution architecture, and technical due diligence for data governance initiatives\n• Mentor clients' data teams on governance practices, data quality management, and stewardship models\n\nYour background\n• Bachelor's or master's degree in science, technology, engineering, mathematics, or information management\n• Deep expertise in data governance frameworks, with hands-on implementation experience using tools like Collibra, Alation, Informatica, Purview, or DataHub\n• Strong practical knowledge of DAMA-DMBOK framework and ability to adapt it to different organizational contexts\n• Proven track record conducting data environment assessments and maturity evaluations for large enterprises\n• Hands-on experience implementing metadata management, data lineage, and data cataloguing solutions at scale\n• Technical proficiency with data profiling, data quality tools, and master data management platforms\n• Experience with at least one major data platforms (Databricks, Snowflake, AWS, Azure, DataFabric) and understanding of their governance capabilities\n• Working knowledge of SQL, Python, or similar for data analysis and quality validation\n• Strong experience designing data operating models and organizational structures for analytics teams\n• 8+ years in data management consulting with direct client engagement and delivery accountability\n• Demonstrated ability to influence and advise senior executives on data strategy and governance investments\n• Excellence in creating and delivering executive presentations, turning technical concepts into business value stories\n• Experience managing complex stakeholder environments and driving consensus across business and IT teams\n• Track record of hands-on governance tool implementation, not just strategy documentation\n• Active engagement in data governance communities, certifications (CDMP, DCAM), and industry forums\n\nTemus is an equal opportunities employer. We welcome applications from all. We do not discriminate by race, religion, belief, ethnicity, origin, disability, age, partnership status, sexual orientation, or gender identity.\n\nWe see the diversity of our team as a strategic advantage, and we work actively to maintain it.\n\nBy applying for this role, you have read and acknowledge the data privacy statement via this link - temus.com/job-applicant-data-protection/",
    "job_is_remote": null,
    "job_posted_at": "6 hours ago",
    "job_posted_at_timestamp": 1753149600,
    "job_posted_at_datetime_utc": "2025-07-22T02:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DB4AEepGQz_M1JeavAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - QuantumBlack",
    "employer_name": "McKinsey & Company",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRZPbw9ZL0xF1gbrBe4t_LgmplpG25qKZ1Ey5Q8&s=0",
    "employer_website": "http://www.mckinsey.com/",
    "job_publisher": "McKinsey",
    "job_employment_type": "Full–time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.mckinsey.com/careers/search-jobs/jobs/dataengineer-quantumblack-94882?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "McKinsey",
        "apply_link": "https://www.mckinsey.com/careers/search-jobs/jobs/dataengineer-quantumblack-94882?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://id.indeed.com/viewjob?jk=4bdbf65a3c0c050c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://id.linkedin.com/jobs/view/data-engineer-contract-based-at-pt-astra-international-tbk-4118012549?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.mx/job-listing/data-engineer-quantumblack-mckinsey-company-JV_IC2709872_KO0,26_KE27,43.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.br/job-listing/data-engineer-quantumblack-mckinsey-company-JV_IC2709872_KO0,26_KE27,43.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-quantumblack-mckinsey-and-company-JV_IC2709872_KO0,26_KE27,47.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.es/job-listing/data-engineer-quantumblack-mckinsey-company-JV_IC2709872_KO0,26_KE27,43.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.co.nz/job-listing/data-engineer-quantumblack-mckinsey-company-JV_IC2709872_KO0,26_KE27,43.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Your Growth Driving lasting impact and building long-term capabilities with our clients is not easy work. You are the kind of person who thrives in a high performance/high reward culture - doing hard things, picking yourself up when you stumble, and having the resilience to try another way forward.\n\nIn return for your drive, determination, and curiosity, we'll provide the resources, mentorship, and opportunities you need to become a stronger leader faster than you ever thought possible. Your colleagues—at all levels—will invest deeply in your development, just as much as they invest in delivering exceptional results for clients. Every day, you'll receive apprenticeship, coaching, and exposure that will accelerate your growth in ways you won’t find anywhere else.\n\nWhen you join us, you will have:\n• Continuous learning: Our learning and apprenticeship culture, backed by structured programs, is all about helping you grow while creating an environment where feedback is clear, actionable, and focused on your development. The real magic happens when you take the input from others to heart and embrace the fast-paced learning experience, owning your journey.\n• A voice that matters: From day one, we value your ideas and contributions. You’ll make a tangible impact by offering innovative ideas and practical solutions. We not only encourage diverse perspectives, but they are critical in driving us toward the best possible outcomes.\n• Global community: With colleagues across 65+ countries and over 100 different nationalities, our firm’s diversity fuels creativity and helps us come up with the best solutions for our clients. Plus, you’ll have the opportunity to learn from exceptional colleagues with diverse backgrounds and experiences.\n• World-class benefits: On top of a competitive salary (based on your location, experience, and skills), we provide a comprehensive benefits package to enable holistic well-being for you and your family.\n\nYour ImpactYou will work on real-world, high-impact projects across a variety of industries. You will have the opportunity to collaborate with QB/Labs teams and build complex and innovative ML systems to accelerate our work in AI and help solve business problems at speed and scale.\n\nYou will experience the best environment to grow as a technologist and a leader. You will develop a sought-after perspective connecting technology and business value by working on real-life problems across a variety of industries and technical challenges to serve our clients on their changing needs.\n\nYou will be surrounded by inspiring individuals as part of diverse and multidisciplinary teams. You will develop a holistic perspective of AI by partnering with the best design, technical, and business talent in the world as your team members.\n\nWhile we advocate for using the right tech for the right task, we often leverage the following technologies: Python, PySpark, the PyData stack, SQL, Airflow, Databricks, our own open-source data pipelining framework called Kedro, Dask/RAPIDS, container technologies such as Docker and Kubernetes, cloud solutions such as AWS, GCP, and Azure, and more.\n\nAs a Data Engineer, you will:\n• Contribute to cross-functional problem-solving sessions with your team and our clients, from data owners and users to C-level executives, to address their needs and build impactful analytics solutions\n• Have the opportunity to contribute to R&D projects and internal asset development\n• Design and build GenAI applications (RAG, Agentic AI, etc) collaboratively with data scientists\n• Map data fields to hypotheses and curate, wrangle, and prepare data for use in advanced analytics models\n• Apply knowledge about clients data landscape and assess data quality\n• Create and manage data environments and ensure information security standards are maintained at all times\n• Design and build data pipelines for machine learning that are robust, modular, scalable, deployable, reproducible, and versioned\n• Help to build and maintain the technical platform for advanced analytics engagements, spanning data science and data engineering work\n\nYou will be part of our global Data Engineering community and you will work in cross-functional Agile project teams alongside Data Scientists, Machine Learning Engineers, other Data Engineers, Project Managers, and industry experts.\n\nYou will work hand-in-hand with our clients, from data owners, users, and fellow engineers to C-level executives.\n\nWho you are: You are a highly collaborative individual who wants to solve problems that drive business value. You have a strong sense of ownership and enjoy hands-on technical work. Our values resonate with yours.\n\nYour qualifications and skills\n• Degree in computer science, engineering, mathematics, or equivalent experience  \n• 2+ years of relevant professional experience\n• Ability to write clean, maintainable, scalable and robust code in an object-oriented language, e.g., Python, Scala, Java, in a professional setting\n• Proven experience building data pipelines in production for advanced analytics use cases\n• Experience working across structured, semi-structured and unstructured data\n• Exposure to software engineering concepts and best practices, inc. DevOps, DataOps and MLOps would be considered a plus\n• Familiarity with distributed computing frameworks, cloud platforms, containerization, and analytics libraries (e.g. pandas, numpy, matplotlib)\n• Experienced on Big Data platforms and tools like AWS, Azure, GCP and tools like Spark, Kafka, Snowflake, GCS Data Proc, Azure DataFactory, AWS Glue, Apache Beam/Flink, Python/PySpark, etc.\n• Commercial client-facing or senior stakeholder management experience would be beneficial",
    "job_is_remote": null,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Jakarta, Indonesia",
    "job_city": null,
    "job_state": "Jakarta",
    "job_country": "ID",
    "job_latitude": -6.1944491,
    "job_longitude": 106.8229198,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DQI7t3m021DbcFcjSAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "McKinsey & Company",
        "score": 4,
        "num_stars": 4,
        "review_count": 14355,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/McKinsey-and-Company-Reviews-E2893.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "McKinsey & Company",
        "score": 4.2,
        "num_stars": 4,
        "review_count": 913,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Mckinsey-&-Company/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "McKinsey & Company",
        "score": 3.8,
        "num_stars": 4,
        "review_count": 659,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/mckinsey-and-company-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Associate, Full Stack Data Engineer (Singapore) | Singapore, SG",
    "employer_name": "Nomura Asia",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTrygXvXOtQonteAgk3YEoo7ZPiO5GSwB3OD7QM&s=0",
    "employer_website": "https://www.nomuraholdings.com",
    "job_publisher": "EFinancialCareers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.efinancialcareers.sg/jobs-Singapore-Singapore-Associate_Full_Stack_Data_Engineer_Singapore.id22996447?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "EFinancialCareers",
        "apply_link": "https://www.efinancialcareers.sg/jobs-Singapore-Singapore-Associate_Full_Stack_Data_Engineer_Singapore.id22996447?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job title: Full Stack Data Engineer\nCorporate Title: Associate\nDepartment: Chief Data Office\nLocation: Singapore\nCompany overview\nNomura is an Asia-based financial services group with an integrated global network spanning over 30 countries. By connecting markets East & West, Nomura services the needs of individuals, institutions, corporates and governments through its three business divisions: Retail, Asset Management, and Wholesale (Global Markets and Investment Banking). Founded in 1925, the firm is built on a tradition of disciplined entrepreneurship, serving clients with creative solutions and considered thought leadership. For further information about Nomura, visit www.nomura.com\nDepartment overview:\nThe Chief Data Office plays a key role in defining and implementing the firm's data, cloud and AI strategy, driving change through these capabilities, enforcing data, cloud and AI governance for the firm, and elevating Nomura's data culture. Governance remains a critical focus area, and the Chief Data Office, in partnership with Business and Corporate functions, is responsible for ensuring that the firm's data assets are managed in line with the firm's data management framework, policy and standards.\nRole Description:\nJob Responsibilities:\n• Information delivery & analytics. State-of-the-art expertise across, data/information preparation, data insight & visualization using BI (or similar tools), and advanced data prediction using AI, ML, DL, etc.\n• AI/ML Ops. Responsible for integration, deployment and monitoring of AI/ML products and solutions,\n• Data management. Demonstrate expertise in data management to ensure the analytics products are appropriate/ethical and well-controlled. Enabling data architecture and delivery of data-analytics platforms and Solutions- on-premises, cloud, and hybrid ensuring adherence and conformance to Nomura standards and policies\n• Be a trusted partner. Shape the information & analytics agenda at Nomura, and work with all of Nomura's businesses in laying out their information & analytics adoption roadmaps.\n• Risk Mindset: Familiar with risk and controls frameworks and ability to operate with a control mindset\nSkills, experience, qualifications and knowledge required:\nCore Skills requirement:\n• Designing and developing scalable data pipelines to collect and process large volumes of data from multiple sources.\n• Building physical data models and ETL processes to ensure data quality, integrity, and accessibility.\n• Microservices Development: Building and maintaining highly scalable and fault tolerant microservice, including efficient server-side APIs.\n• Deployment: Hands on with CI/CD, Jenkins, Ansible, DevOps process, Enterprise integration patterns.\n• Hands-on with programming languages (Python, SQL, Java, Unix scripting etc.) and with orchestration tools like Airflow or Autosys\n• Experience with cloud technologies such as EC2, EMR, Snowflake or similar tools with ability to drive design and data model discussions, hybrid data architecture.\n• Proficiency in React with hands-on experience in UI development a plus.\n• Ability to understand and integrate cultural differences and work effectively with virtual cross-cultural, cross-border teams.\n• Flexibility to adjust to multiple demands, shifting priorities, ambiguity, and rapid change.\n• Experience with senior stakeholder management will be an added advantage.\n• Excellent communication (verbal, written, listening), presentation, and interpersonal skills.\n• Able to analyze complex situations and derive workable actions.\n• Able to constructively challenge requirements and current state to increase overall value to the firm.\nEducation and experience\nWide variety of degrees will be considered, however work experience will be of equal, if not greater importance\n• At least 4-year Bachelor's degree in quantitative fields with minimum of 5 years of relevant data experience in data engineering / MLOps, full stack engineering, preferably in financial organizations or Masters in quantitative fields (Computer Science, Statistics or similar)\n• Experience of working with a multi-cultural, multi-disciplined, globally dispersed teams\n• Certifications in relevant technologies or frameworks are a plus.\nDiversity Statement\nNomura is committed to an employment policy of equal opportunities, and is fundamentally opposed to any less favourable treatment accorded to existing or potential members of staff on the grounds of race, creed, colour, nationality, disability, marital status, pregnancy, gender or sexual orientation.\n\nDISCLAIMER: This Job Description is for reference only, and whilst this is intended to be an accurate reflection of the current job, it is not necessarily an exhaustive list of all responsibilities, duties, skills, efforts, requirements or working conditions associated with the job. The management reserves the right to revise the job and may, at his or her discretion, assign or reassign duties and responsibilities to this job at any time.\nNomura is an Equal Opportunity Employer",
    "job_is_remote": null,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DTwOHvv0eFL6EkyyFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113300",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Senior Software Engineer (Data Engineer)",
    "employer_name": "EPAM Systems",
    "employer_logo": null,
    "employer_website": "http://www.epam.com/",
    "job_publisher": "EPAM",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.epam.com/careers/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "EPAM",
        "apply_link": "https://www.epam.com/careers/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EPAM",
        "apply_link": "https://careers.epam.ua/vacancies/job-listings/job-detail.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EPAM",
        "apply_link": "https://careers.epam-poland.pl/careers/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "MyCareersFuture",
        "apply_link": "https://www.mycareersfuture.gov.sg/job/information-technology/senior-software-engineer-epam-systems-4fcdb9d5bd3be6f260b5f750d4eb993b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EPAM",
        "apply_link": "https://careers.epam.hu/careers/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Careers.epam.cn",
        "apply_link": "https://careers.epam.cn/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit",
        "apply_link": "https://www.foundit.sg/job/senior-software-engineer-data-engineer-epam-systems-pte-ltd-singapore-35437150?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/senior-software-engineer-data-engineer-at-epam-systems-4258451865?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We are seeking a skilled Data Engineer/Reporting Analyst to analyze the organization’s data needs, design efficient data pipelines, and develop reporting solutions. You will migrate data collection to optimized channels, implement ETL processes (Python, PySpark, Informatica), and build dashboards (Tableau/SAP BO). The role involves collaborating with technical teams to deploy solutions, ensuring system reliability, and supporting production issues. Experience in Agile/Waterfall methodologies, SQL, big data (Cloudera/Hive), and DevOps (Denodo) is essential.\n\nRESPONSIBILITIES\n• Analyse the Authority’s data needs and document the requirements\n• Refine data collection/consumption by migrating data collection to more efficient channels\n• Plan, design and implement data engineering jobs and reporting solutions to meet the analytical needs\n• Develop a test plan and scripts for system testing, and support user acceptance testing\n• Build reports and dashboards according to user requirements\n• Work with the Authority’s technical teams to ensure smooth deployment and adoption of the new solution\n• Ensure the smooth operations and service level of IT solutions\n• Support production issues\n\nREQUIREMENTS\n• Minimum 5 years of experience\n• Good understanding and completion of projects using Waterfall/Agile methodologies\n• Strong SQL, data modeling, and data analysis skills\n• Hands-on experience in big data engineering jobs using Python, PySpark, Linux, and ETL tools like Informatica\n• Hands-on experience in a reporting or visualization tool like SAP BO and Tableau\n• Good understanding of analytics and data warehouse implementations\n• Ability to troubleshoot complex issues ranging from system resources to application stack traces\n• Passion for automation, standardization, and best practices\n• No visa sponsorship is available\n\nNICE TO HAVE\n• Hands-on experience in DevOps deployment and data virtualization tools like Denodo\n• Track record in implementing systems using Hive, Impala, and Cloudera Data Platform\n• Track record in implementing systems with high availability, high performance, and high security hosted at various data centres or hybrid cloud environments\n\nWE OFFER\n• By choosing EPAM, you're getting a job at one of the most loved workplaces according to Newsweek 2021 & 2022&2023\n• Employee ideas are the main driver of our business. We have a very supportive environment where your voice matters\n• You will be challenged while working side-by-side with the best talent globally. We work with top-notch technologies, constantly seeking new industry trends and best practices\n• We offer a transparent career path and an individual roadmap to engineer your future & accelerate your journey\n• At EPAM, you can find vast opportunities for self-development: online courses and libraries, mentoring programs, partial grants of certification, and experience exchange with colleagues around the world. You will learn, contribute, and grow with us",
    "job_is_remote": null,
    "job_posted_at": "25 days ago",
    "job_posted_at_timestamp": 1750982400,
    "job_posted_at_datetime_utc": "2025-06-27T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DvgTNNr_ZkBb3Of6BAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "EPAM Systems",
        "score": 4.1,
        "num_stars": 4,
        "review_count": 13555,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/EPAM-Systems-Reviews-E15544.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "EPAM Systems",
        "score": 3.8,
        "num_stars": 4,
        "review_count": 489,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Epam-Systems/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "EPAM Systems",
        "score": 3.7,
        "num_stars": 3.5,
        "review_count": 1714,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/epam-systems-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - Growth",
    "employer_name": "TikTok",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSTRJE7dgz0c5ALgv5JPJvQUCXCLLRcSLRvRJlr&s=0",
    "employer_website": "https://www.tiktok.com",
    "job_publisher": "LinkedIn Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-growth-at-tiktok-4268168199?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-growth-at-tiktok-4268168199?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/data-engineer-growth-jobs/985E7896AA93B761?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Responsibilities\nThe Growth team plays a core role in acquisition, activation, and retention of billions of users, through our globally popular products such as TikTok, Lemon8, etc. We are building platform foundations, leveraging data and ML models, and providing end-to-end solutions to power global growth of products.\n\nYou will:\n- Build data pipelines to portray business status, based on a deep understanding of our fast changing business and data-driven approach;\n- Extract information and signals from a broad range of data and build hierarchies to accomplish analytical and mining goals for “Packaged Business Capability” such as user-growth, gaming and searching;\n- Keep improving the integrity of data pipelines to provide a comprehensive data service.\n\nQualifications\nMinimum Qualifications：\n- Bachelor's degree in Computer Science, Statistic, Data Science or a related field;\n- Skilled in SQL and additional object-oriented programming language (e.g. Scala, Java, or Python);\n- Experience in issue tracking and problem solving on data pipelines;\n- Fast business understanding and collaborative in teamwork.\n\nPreferred Qualification：\n- Industry experience working with user growth.\n\nAbout TikTok\nTikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.\n\nWhy Join Us\nInspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect – and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.\nWe strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.\n\nDiversity & Inclusion\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.",
    "job_is_remote": null,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D57oJKe0o8V6CkydyAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Data Engineer (Oracle) - SPVL",
    "employer_name": "SCIENTEC CONSULTING PTE. LTD.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSD3tysIrWi4kH4Q8UGyfSY7ZbYku3brcjJF5yo&s=0",
    "employer_website": null,
    "job_publisher": "MyCareersFuture",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-spvl-scientec-consulting-329e54c1ffef9899bd51a5491cf921ab?source=MCF&event=Search&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "MyCareersFuture",
        "apply_link": "https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-spvl-scientec-consulting-329e54c1ffef9899bd51a5491cf921ab?source=MCF&event=Search&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/singapore/job/full-time/technology/data-engineer-oracle-spvl-145489038?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs In Singapore. New Jobs, New Recruitment And Fastest 2025",
        "apply_link": "https://jobsingapore247.com/fr/data-engineer-seekbetter-urgent-job282470?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs In Singapore. Find Jobs Now, Find New Jobs, Recruit New Jobs 2025",
        "apply_link": "https://singaporejob24h.com/ja/data-engineer-integrated-e-services-job17460?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We are looking for Data Engineer to join our team of analytics experts, you'll be responsible for expanding and optimising data and data pipeline architecture, as well as optimising data flow and collect for cross-function teams\n• Exciting opportunity to join one of the fast growing IT company\n• Permanent position, positive working environment\n• Salary up to $7,000 + AWS + Bonuses\n\nResponsibilities\n• Develop, construct, test and maintain data architectures such as databases, data warehouses and large-scale data processing systems\n• Design and develop data pipelines/systems for data modelling, mining and production\n• Ensure the data architecture is in place to support routine and ad-hoc requirements of data analytics team, stakeholders and the business\n• Leverage on variety of programming languages and data crawling/processing tools to make raw data clean and highly available for use in descriptive and predictive modelling\n• Recommend and implement ways to improve data quality, reliability, flexibility and efficiency\n• Ensure data assets and data catalogs are organized and stored in an efficient way so that information is easy to access and retrieve\n• PL/SQL and SQL Tuning and optimization of newly develop and existing applications\n\nRequirements\n• At least 3 years' working experience in data architecture, data warehousing, data processing, data modelling and ETL/ELT, familiarity with real-time streaming solutions.\n• Working experience in Kubernetes-based DevOps practices, with experience in container orchestration, CI/CD pipelines, and microservices deployment.\n• Working experience in database development (Oracle SQL/PLSQL)\n• Working experience in AWS cloud environment, familiar with solutions such as EC2, S3, EMR, Redshift, Athena, Kinesis\n• Programming knowledge in Python, R, SQL for data cleaning, processing and aggregation\n\nBy submitting your resume, you consent to the collection, use, and disclosure of your personal information per ScienTec’s Privacy Policy (scientecconsulting.com/privacy-policy).\n\nThis authorizes us to:\n\nContact you about potential opportunities.\n\nDelete personal data not required at this application stage.\n\nTo withdraw consent, email dpo@scientecconsulting.com.\n\nAll applications will be processed with strict confidence. Only shortlisted candidates will be contacted.\n\nLiew Chien Hui - R2090138\n\nScienTec Consulting Pte Ltd – 11C5781",
    "job_is_remote": null,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dt7K78VJEBsgBxWmFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "MONTH",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Lead Data Engineer",
    "employer_name": "Thoughtworks",
    "employer_logo": null,
    "employer_website": "http://www.thoughtworks.com/",
    "job_publisher": "Thoughtworks",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.thoughtworks.com/careers/jobs/7007052?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Thoughtworks",
        "apply_link": "https://www.thoughtworks.com/careers/jobs/7007052?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://sg.indeed.com/viewjob?jk=64f93427a708fccc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/lead-data-engineer/6183183?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.sg/job-listing/lead-data-engineer-kyndryl-JV_IC3235921_KO0,18_KE19,26.htm?jl=1009716968816&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs By Workable",
        "apply_link": "https://jobs.workable.com/view/dzHg4JhGANf315FGXJNAcB/data-engineer-lead-in-singapore-at-unison-consulting-pte-ltd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In Singapore",
        "apply_link": "https://builtinsingapore.com/job/data-engineer-lead/6542350?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Smart Recruiters Jobs",
        "apply_link": "https://jobs.smartrecruiters.com/Grab/744000030786095?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/lead-data-engineer-at-temus-4268071512?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Lead data engineers at Thoughtworks develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. They might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On projects, they will be leading the design of technical solutions, or perhaps overseeing a program inception to build a new product. Alongside hands-on coding, they are leading the team to implement the solution.",
    "job_is_remote": null,
    "job_posted_at": "15 days ago",
    "job_posted_at_timestamp": 1751846400,
    "job_posted_at_datetime_utc": "2025-07-07T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DpItNW5ktrvI-6J5vAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Thoughtworks",
        "score": 3.2,
        "num_stars": 3,
        "review_count": 5433,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Thoughtworks-Reviews-E38334.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "Thoughtworks",
        "score": 4,
        "num_stars": 4,
        "review_count": 152,
        "max_score": 5,
        "reviews_link": "https://www.indeed.com/cmp/Thoughtworks/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Thoughtworks",
        "score": 3.9,
        "num_stars": 4,
        "review_count": 568,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/thoughtworks-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer, GovTech Anti Scam Products (GASP)",
    "employer_name": "Government Technology Agency",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT8QDYvtl0VCwTSBElbQL7ffJg24h3kJ3PTeR0q&s=0",
    "employer_website": "http://www.tech.gov.sg/",
    "job_publisher": "Careers - Careers@Gov",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.careers.gov.sg/jobs/hrp/16207454/992bab65-4819-1fe0-98d0-4bc31378c24e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers - Careers@Gov",
        "apply_link": "https://jobs.careers.gov.sg/jobs/hrp/16207454/992bab65-4819-1fe0-98d0-4bc31378c24e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-govtech-anti-scam-products-gasp-at-govtech-singapore-4255673317?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit",
        "apply_link": "https://www.foundit.sg/job/data-engineer-govtech-anti-scam-products-gasp-gvt-government-technology-agency-singapore-35393196?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs Trabajo.org",
        "apply_link": "https://sg.trabajo.org/job-3820-e0125065d3bfad0721cf63192b9f3acc?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "The BIG Jobsite | Singapore",
        "apply_link": "https://sg.thebigjobsite.com/details/C6BDC3C05A6A1BA7408F3A1F8A19D1F8/data-engineer--govtech-anti-scam-products--gasp?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "What the role is:\n\nThe Government Technology Agency (GovTech) is the lead agency driving Singapore’s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government’s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.\n\nAt GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.\n\nGovTech aims to transform the delivery of Government digital services by taking an \"outside-in\" view, putting citizens and businesses at the heart of everything we do.\n\nPlay a part in Singapore’s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!\n\nLearn more about GovTech at tech.gov.sg.\n\nWhat you will be working on:\n\nThe GovTech Anti Scam Products Team’s mission is to build tech solutions to detect, disrupt and deter scammers at scale. The work spans data infrastructure, developing and deploying AI & ML to scale proactive detection, API connections to partners, as well as front-end systems for enforcement agencies.\n\nThe Data Engineer will be solving abstract problems to tackling scams using data: \n• Design and implement ETL processes\n• Manage data warehousing solutions\n• Expose and deploy machine learning models to production\n• Ensure data quality and consistency across various sources\n• Collaborate with data scientists and software engineers to develop anti-scam tech products\n\n \n\nMore senior engineers will also be expected to:\n• Establish best practices for developer operations\n• Provide technical leadership across multiple product teams\n• Share your expertise and mentor other engineers\n• Help with recruiting\n\nWhat we are looking for:\n• Proficient in Python \n• Experience with SQL/NoSQL/Graph databases\n• Glue, Presto, Apache Spark and/or Hive\n• Experience working with CI/CD setups\n• Strong knowledge of algorithms and database structures\n• Strong knowledge of database integration and migration strategy\n• Strong knowledge in designing and implementing scalable data infrastructure\n\nPreferred\n• Experience in AWS Cloud\n• Experience building high scale, low latency APIs & deploying scaled AI / ML solutions\n• Experience with data intensive frameworks & data warehousing solutions, bonus if you’re familiar with graph data structures\n• Understanding of event driven architectures involving SQS / kafka, bonus if you’re familiar with Flink!\n\nWe're here to improve how we live as a society through what we can offer as a government. Because our team focuses on pushing new initiatives, you will also have to:\n• Identify potential projects that improve the public good\n• Design novel systems that work around bureaucratic constraints\n• Advocate and explain these technical ideas to other government agencies\n\n \n\nYou're not just here to write code, but also to figure out what we should be building and how we should build it. You will work on meaningful projects to reduce scams and make a big impact on people’s lives. \n\nOur employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. These include leave benefits to meet your work-life needs and employee wellness programmes.\n\nWe champion flexible work arrangements (subject to your job role) and trust that you will manage your own time to deliver your best, wherever you are, and whatever works best for you.\n\nLearn more about life inside GovTech at go.gov.sg/GovTechCareers.\n\nStay connected with us on social media at go.gov.sg/ConnectWithGovTech.\n\nAbout Government Technology Agency\nThe Government Technology Agency (GovTech) is the lead agency driving Singapore’s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government’s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.\n\nAt GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.\n\nGovTech aims to transform the delivery of Government digital services by taking an \"outside-in\" view, putting citizens and businesses at the heart of everything we do.\n\nPlay a part in Singapore’s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!\n\nLearn more about GovTech at tech.gov.sg.",
    "job_is_remote": null,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DHGgV0-Xe7pagLjspAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Government Technology Agency",
        "score": 3.5,
        "num_stars": 3.5,
        "review_count": 1363,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Overview/Working-at-GovTech-EI_IE1371268.11,18.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "Government Technology Agency",
        "score": 4.3,
        "num_stars": 4.5,
        "review_count": 27,
        "max_score": 5,
        "reviews_link": "https://sg.indeed.com/cmp/Government-Technology-Agency/reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "NodeFlair",
        "employer_name": "Government Technology Agency",
        "score": 4,
        "num_stars": 4,
        "review_count": 126,
        "max_score": 5,
        "reviews_link": "https://nodeflair.com/companies/govtech?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer (must have Oracle)",
    "employer_name": "Randstad Singapore",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQwmN82IdF-azillsg5hgpCpL-WeqAiw3OPC-uk&s=0",
    "employer_website": "https://www.randstad.com.sg",
    "job_publisher": "Randstad Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.randstad.com.sg/jobs/data-engineer-must-have-oracle_singapore_45548391/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Randstad Singapore",
        "apply_link": "https://www.randstad.com.sg/jobs/data-engineer-must-have-oracle_singapore_45548391/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Randstad",
        "apply_link": "https://www.randstad.com/jobs/data-engineer-must-have-oracle_singapore_45548391/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-must-have-oracle-at-randstad-singapore-4266486378?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "MyCareersFuture",
        "apply_link": "https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-randstad-78c115d0db514360f3809728030ca440?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobsDB",
        "apply_link": "https://sg.jobsdb.com/job/Data-Engineer-fdb205247cd4241273fd6349777c4431?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://en-sg.whatjobs.com/jobs/database-management?id=56832899&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "about company\n\nI am currently working with a software consultancy firm that provides premium software development and strategy expertise to a wide spectrum of clients. Projects are about Gaming/Gambling/Blockchain/Government Project.\n\nSalary structure: Base + AWS + variable bonus. 2 rounds of interview process. Hybrid working arrangement. Office at CBD.\n\nabout job\n\n• Develop, construct, test and maintain data architectures such as databases, data warehouses and large-scale data processing systems\n\n• Design and develop data pipelines/systems for data modelling, mining and production\n\n• Make raw data clean and highly available for use in descriptive and predictive modelling\n\n• Recommend and implement ways to improve data quality, reliability, flexibility and efficiency\n\n• Ensure data assets and data catalogs are organized and stored in an efficient way\n\n• PL/SQL and SQL Tuning and optimization\n\nskills and requirements\n\n• Min 3 years in data architecture, data warehousing, data processing, data modelling and ETL/ELT, familiarity with real-time streaming solutions\n\n• Experience in Kubernetes-based DevOps practices, with experience in container orchestration, CI/CD pipelines, and microservices deployment.\n\n• Experience in database development (Oracle SQL/PLSQL)\n\n• Working experience in AWS cloud environment, familiar with solutions such as EC2, S3, EMR, Redshift, Athena, Kinesis • Programming knowledge in Python, R, SQL for data cleaning, processing and aggregation\n\n• Mandarin speaking required as you need to liaise with Chinese counterparts who can only speak and write in Mandarin\n\nTo apply online please use the 'apply' function, alternatively you may contact Stella at 96554170 (EA: 94C3609 /R1875382)\n\nskills\n\nno additional skills required\n\nqualifications\n\nno additional qualifications required\n\neducation\n\nBachelor Degree",
    "job_is_remote": null,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D-Zlw5zo4rsFh8aCFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "MONTH",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Randstad Singapore",
        "score": 4.2,
        "num_stars": 4,
        "review_count": 12228,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Randstad-Singapore-Reviews-EI_IE302306.0,8_IL.9,18_IM1123.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "Randstad Singapore",
        "score": 3.7,
        "num_stars": 3.5,
        "review_count": 19258,
        "max_score": 5,
        "reviews_link": "https://sg.indeed.com/cmp/Randstad/reviews?ftopic=culture&utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "Data Engineer - ETL (MOH ITDG)",
    "employer_name": "Synapxe",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSw0jV-APEssGxuQHgooZgQ4xWnz778jKZZBknb&s=0",
    "employer_website": "http://www.synapxe.sg/",
    "job_publisher": "HealthTech Career | Tech Jobs At Synapxe And Public Healthcare - Synapxe",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers-public-healthtech-jobs.synapxe.sg/job/Data-Engineer-ETL-%28MOH-ITDG%29/43267444/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "HealthTech Career | Tech Jobs At Synapxe And Public Healthcare - Synapxe",
        "apply_link": "https://careers-public-healthtech-jobs.synapxe.sg/job/Data-Engineer-ETL-%28MOH-ITDG%29/43267444/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-etl-moh-itdg-at-synapxe-4267780519?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/singapore/job/part-time/technology/etl-data-engineer-part-time-140913043?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Overview\n\nRole & Responsibilities\n\nDevelop TRUST data strategy:\n• Work with stakeholders to understand data analytics needs, data structure requirements (both in terms of scalability and accessibility), and translate this into a coherent near to long term data strategy for TRUST\n• Support translation of data business needs into technical system requirements for MCDR, in terms of collection, storage, batch -time processing, as well as analysis of information from structured and unstructured sources in a scalable, repeatable, and secure manner\n• Identify opportunities for improvements and optimisation e.g., Implement best practices and performance optimization on Big Data and Cloud to achieve the best data engineering outcomes\n\nOversee data preparation and data provisioning for TRUST:\n• Collaborate with data engineers to organise and prepare anonymised datasets in MCDR according to TRUST standards, and then providing the data in accordance with the approved TRUST Data Request. This involves working with the data engineers closely to ensure that the datasets meet the required standards and are made available as per the specific data request guidelines set by TRUST\n• Oversee implementation of common data model and data quality programme in TRUST and MCDR\n• Work with data analysts, data scientists, clinicians and other stakeholders to implement common data models to support analytics use cases\n• Design and implement tools to enhance the data strategy and enable seamless integration with the data, potentially leveraging API calls for efficient integration\n• Implement data management standards and practices\n\nRequirements\n• Degree/master’s in computer science, Information Technology, Computer Engineering or equivalent\n• At least ten (10) years of relevant working experience in Data management / Integration / Modelling the data warehouse or advanced analytics solutions\n• Demonstrate good, in-depth knowledge in relevant Extract-Transform-Load (ETL) hardware/software products, frameworks, and methodologies\n• Experience in designing and implementing cloud-based data solutions using cloud platforms (e.g., AWS cloud native tools)\n• Databases (e.g., Oracle, MS SQL, MySQL, Teradata)\n• Big data (e.g., Hadoop ecosystem)\n• ETL development using ETL tools (e.g., Informatica, IBM DataStage, Talend)\n• Data repository design (e.g., operational data stores, dimensional data stores, data marts)\n• Experience in interacting with analytics stakeholders (economists, statisticians, clinicians, policy makers) on a business or domain level\n• Comfortable working independently to carry out data analysis, estimate data quality and sufficiency\n• Good interpersonal skills, a detail-oriented & flexible person who can work across different areas within the team\n• The following will be preferred: Some understanding of Singapore Healthcare System and healthcare data governance, management; and/or familiarity with health informatics\n\nApply Now\n\nNOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!\n\n#LI-SYNX40",
    "job_is_remote": null,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DkvYQr9pAcrfhoccgAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113300",
    "job_onet_job_zone": "4",
    "employer_reviews": [
      {
        "publisher": "Glassdoor",
        "employer_name": "Synapxe",
        "score": 3.1,
        "num_stars": 3,
        "review_count": 1142,
        "max_score": 5,
        "reviews_link": "https://www.glassdoor.com/Reviews/Synapxe-Reviews-E454697.htm?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "Indeed",
        "employer_name": "Synapxe",
        "score": 3.3,
        "num_stars": 3.5,
        "review_count": 6,
        "max_score": 5,
        "reviews_link": "https://sg.indeed.com/cmp/Synapxe-Pte.-Ltd./reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      },
      {
        "publisher": "AmbitionBox",
        "employer_name": "Synapxe",
        "score": 2.3,
        "num_stars": 2.5,
        "review_count": 7,
        "max_score": 5,
        "reviews_link": "https://www.ambitionbox.com/reviews/synapxe-reviews?utm_campaign=google_jobs_reviews&utm_source=google_jobs_reviews&utm_medium=organic"
      }
    ]
  },
  {
    "job_id": null,
    "job_title": "AWS Data Engineer",
    "employer_name": "Unison Consulting",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQGU2djMRaKedNmQA3nRllDv_rAKICtlpJaRgGi&s=0",
    "employer_website": "http://www.unison-ucg.com/",
    "job_publisher": "LinkedIn Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.linkedin.com/jobs/view/aws-data-engineer-at-unison-consulting-4269382253?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/aws-data-engineer-at-unison-consulting-4269382253?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://sg.jooble.org/jdp/7638434842567790994?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs Trabajo.org",
        "apply_link": "https://sg.trabajo.org/job-3088-4c3dfbe626aba6cbd1a938ec263b05ad?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Hirewand",
        "apply_link": "https://www.hirewand.com/apply/job/detail?jid=1709102sid%3D5af414cf6ed33ca5454ecbdb&src=jobpost&cpid=1709&uid=88869&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://en-sg.whatjobs.com/jobs/data-management?id=56864579&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/singapore/singapore/info_technology/4383965951/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "• Design and build scalable ETL pipelines and data integration workflows using AWS services and Python\n• Develop and optimize data lake and data warehouse solutions for structured and unstructured data\n• Leverage Apache Spark for large-scale data processing and transformation tasks\n• Collaborate with cross-functional teams to gather requirements and deliver clean, usable datasets for analytics and reporting\n• Ensure high data quality, security, and compliance in all stages of the data lifecycle\n\nRequirements\n• 5+ years of hands-on experience in data engineering roles\n• Proficiency in Python for building and automating data pipelines\n• Strong experience with AWS services (S3, Glue, Redshift, Lambda, etc.)\n• Solid understanding of ETL processes and modern data warehousing concepts\n• Experience with big data tools, especially Apache Spark (PySpark preferred)\n• Familiarity with DevOps and CI/CD practices for data pipeline deployment\n• Knowledge of data governance and cataloging tools\n• Strong problem-solving, communication, and collaboration skills",
    "job_is_remote": null,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXYhI4KEG-YaTLDBoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": null,
    "job_title": "Sr. Data Engineer",
    "employer_name": "VISA WORLDWIDE PTE. LIMITED",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRahbyP0z1nWG4Sth9655HWfnYWbVJVkrhkGGTe&s=0",
    "employer_website": "http://www.visa.com.sg/",
    "job_publisher": "Jobstreet",
    "job_employment_type": "Full–time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.jobstreet.com/job/85840488?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobstreet",
        "apply_link": "https://sg.jobstreet.com/job/85840488?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido.com",
        "apply_link": "https://sg.jobrapido.com/jobpreview/182079774099767296?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Expertini",
        "apply_link": "https://sg.expertini.com/jobs/job/sr-data-engineer-singapore-r-systems-singapore-pte-limited-2502-13920920/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Company Description\n\nVisa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n\nMake an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n\nJob Description\n\nVisa’s Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world’s most sophisticated processing networks, capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. You’ll work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cybersecurity, and B2C platforms.\n\nIn addition, Value Added Services (VAS) - VAS Digital Marketing is a key growth strategy for Visa globally, aimed at diversifying Visa’s revenue with products and solutions that differentiate its network and deliver valuable solutions across other networks.\n\nThe Opportunity:\n\nWe are developing and executing a shared strategic vision for Digital Marketing platforms and products that enable Visa to be the world-leading data-driven payments company. As a Senior Data Engineer, you will be part of a world-class team of Engineers to define, drive and execute on this vision. We are looking for a self-motivated, versatile and energetic individual with software engineering skills and expertise with Java, Big data & Web technologies, who embraces solving complex challenges on a global scale. The candidate will be extensively involved in hands-on activities including POCs, design, development, testing, and managing applications globally used by Visa cardholders. Candidate must be flexible and willing to switch tasks based on team's needs.\n\nYou will use your Java skills and experience with various technologies to design, develop, test, and deploy high-quality code that meets stringent business, security, and resiliency requirements. You will collaborate with other teams, vendors, and stakeholders to ensure the smooth delivery and operation of the application. You will have the opportunity to learn and apply new technologies and frameworks, such as AI and generative AI, to enhance the functionality and performance of the application.\n\nPrimary responsibilities will include:\n• Design, develop, test, document, and implement new applications and enhance existing systems to ensure high performance and reliability.\n• Write secure, maintainable, and efficient code that adheres to Java/J2EE best practices, organizational and security standards.\n• Create and maintain comprehensive technical documentation, including design changes and architectural decisions, using Wiki or similar tools.\n• Participate in code and design review sessions to ensure high-quality deliverables and adherence to development standards.\n• Collaborate with architects, product owners, and technical stakeholders to deliver products that meet business requirements and leverage modern technologies.\n• Identify and recommend opportunities for process improvements, enhancements, and adoption of best practices within the development team.\n• Mentor and support junior developers, fostering knowledge sharing and contributing to the development of departmental procedures and standards.\n• Coordinate and contribute to Continuous Integration (CI) activities and the implementation of automated testing frameworks.\n• Develop proof-of-concepts (POCs) and prototypes to validate ideas and quickly iterate new features or enhancements.\n• Communicate technical solutions, project status, issues, and risks effectively to both technical and non-technical stakeholders.\n• Ensure the delivery of high-quality, defect-free code and take accountability for meeting project timelines and quality standards.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\nQualifications\n\nPreferred Qualifications\n\n•3 or more years of work experience with a Bachelor’s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\n\n•4–7 years of relevant experience in Java/J2EE enterprise applications.\n\n•Strong skills in Core Java, J2EE, Spring Framework, Spring Boot, Hibernate, and Web services.\n\n•Proficiency in object-oriented design and software design principles.\n\n•Experience with secure coding practices.\n\n•Strong SQL skills with experience in relational (MySQL, PostgreSQL) and NoSQL (MongoDB) databases.\n\n•Understanding of data warehousing concepts and tools.\n\n•Exposure to data engineering frameworks such as Apache Spark, Hadoop, or Kafka is an advantage.\n\n•Basic understanding of ETL processes and data pipeline development.\n\n•Hands-on experience with containerization and orchestration tools (Docker, Kubernetes).\n\n•Proficiency in version control systems (Git/Stash), build tools (Maven), and CI/CD tools (Jenkins).\n\n•Familiarity with Unix/Linux operating systems and shell scripting.\n\n•Experience with UI frameworks and frontend development using Angular or React, Next.js, JavaScript, HTML, and CSS.\n\n•AI and generative AI skills are highly desirable.\n\n•Experience working in all phases of the software development life cycle.\n\n•Experience with Agile methodologies (Scrum, sprints) and tools (Jira).\n\n•Understanding of DevOps practices.\n\n•Solid foundation in computer science, including data structures and algorithms.\n\n•Willingness to learn and improve coding skills, especially in Java or Scala.\n\nAdditional Information:\n\nSkills/Abilities\n\n•Strong analytical and problem-solving abilities.\n\n•Quick to learn and adapt to new technologies and challenges.\n\n•Excellent organizational skills with the ability to manage multiple tasks and deadlines in a fast-paced environment.\n\n•Outstanding written and verbal communication skills for conveying ideas and implementation plans to team members and stakeholders.\n\n•Highly detail-oriented, resourceful, and results-driven.\n\n•Self-motivated with a demonstrated ability to work independently and meet commitments.\n\n•Comfortable collaborating in dynamic, fast-paced, and highly interactive team settings.\n\n•Eager to learn new skills, embrace new initiatives, and contribute to team success.\n\n•Proven ability to maintain a positive attitude and have fun while working as part of a team.\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
    "job_is_remote": null,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DzlIHUsDVBSxg5HpJAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  }
]