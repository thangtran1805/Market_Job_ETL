[
  {
    "job_id": "00eCRolQcTBw_16xAAAAAA==",
    "job_title": "Data Engineer - Consultant (Remote)",
    "employer_name": "Releady",
    "employer_logo": null,
    "employer_website": "https://www.releady.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/consultant-data-engineer_93b50b06-e8ad-4459-ac66-68a2c9229a93?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Pangian",
        "apply_link": "https://pangian.com/remote/job/data-engineer-consultant-remote-1o?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EarnBetter",
        "apply_link": "https://earnbetter.com/app/job/01JM148F98JPMEKGYAMF1HY8HZ/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "OVERVIEW\n\nThis Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team. You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies. Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure.\n\u2022 Duration: 6+ months contract\n\u2022 Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada. Working hours will be PST. Preference for California.\n\u2022 Rate: $70/hr - $85/hr DOE\n\u2022 **Must be able to work in the United States without sponsorship***\n\nRESPONSIBILITIES\n\u2022 Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)\n\u2022 Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability\n\u2022 Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling\n\u2022 Work with Collibra for data governance, quality assurance, and metadata management\n\u2022 Leverage Refuel.ai for data mastering and Striim for data validation processes\n\u2022 Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows\n\u2022 Collaborate with client stakeholders to translate requirements into effective data solutions\n\u2022 Support data visualization and reporting needs through Tableau\n\u2022 Implement CI/CD practices using Git repositories and modern DevOps tools\n\u2022 Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners\n\nQUALIFICATIONS\n\u2022 Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)\n\u2022 10+ years of experience in data engineering or related roles\n\u2022 Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server\n\u2022 Hands-on experience with dbt Cloud for data transformation and pipeline development\n\u2022 Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse\n\u2022 Knowledge of Data Vault modeling principles and implementation techniques\n\u2022 Experience with data governance and data quality tools, particularly Collibra\n\u2022 Familiarity with data visualization platforms, especially Tableau\n\u2022 Understanding of version control systems (Git, Bitbucket) and CI/CD practices\n\u2022 Experience with scheduling systems like Tidal or Control-M\n\u2022 Working knowledge of Agile methodologies and DevOps principles applied to data pipelines\n\u2022 Preferred Skills:\n\u2022 Experience with data observability platforms and data quality monitoring\n\u2022 Knowledge of Python, R, KNIME, or Alteryx for data science applications\n\u2022 Experience with Refuel.ai and Striim technologies\n\u2022 Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms\n\u2022 Experience with enterprise scheduling tools like Tidal\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or other non-merit factor. We are committed to creating a diverse and inclusive environment for all employees.",
    "job_is_remote": true,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Sacramento, CA",
    "job_city": "Sacramento",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 38.5781342,
    "job_longitude": -121.4944209,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D00eCRolQcTBw_16xAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 70,
    "job_max_salary": 85,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "**Must be able to work in the United States without sponsorship***",
        "Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)",
        "10+ years of experience in data engineering or related roles",
        "Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server",
        "Hands-on experience with dbt Cloud for data transformation and pipeline development",
        "Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse",
        "Knowledge of Data Vault modeling principles and implementation techniques",
        "Experience with data governance and data quality tools, particularly Collibra",
        "Familiarity with data visualization platforms, especially Tableau",
        "Understanding of version control systems (Git, Bitbucket) and CI/CD practices",
        "Experience with scheduling systems like Tidal or Control-M",
        "Working knowledge of Agile methodologies and DevOps principles applied to data pipelines",
        "Experience with data observability platforms and data quality monitoring",
        "Knowledge of Python, R, KNIME, or Alteryx for data science applications",
        "Experience with Refuel.ai and Striim technologies",
        "Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms",
        "Experience with enterprise scheduling tools like Tidal"
      ],
      "Benefits": [
        "Rate: $70/hr - $85/hr DOE"
      ],
      "Responsibilities": [
        "This Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team",
        "You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies",
        "Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure",
        "Duration: 6+ months contract",
        "Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada",
        "Working hours will be PST. Preference for California",
        "Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)",
        "Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability",
        "Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling",
        "Work with Collibra for data governance, quality assurance, and metadata management",
        "Leverage Refuel.ai for data mastering and Striim for data validation processes",
        "Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows",
        "Collaborate with client stakeholders to translate requirements into effective data solutions",
        "Support data visualization and reporting needs through Tableau",
        "Implement CI/CD practices using Git repositories and modern DevOps tools",
        "Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "uINRzsWvsKKLdrOtAAAAAA==",
    "job_title": "URGENT by 6/25:REMOTE:Certified Data Engineer-Databricks, PySpark/Scala, ADFactory, W2 Only",
    "employer_name": "Solitsys",
    "employer_logo": null,
    "employer_website": "http://www.solitsys.com",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time and Contractor",
    "job_employment_types": [
      "FULLTIME",
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "URGENTLY NEEDED: REMOTE DATA ENGINEER- DATA BRICKS , PySpark/Scala, AZURE DATA FACTORY [W2 ONLY, ALL Corp-to-Corp WILL BE REJECTED]\n\nWe cannot offer Corp-to-Corp arrangement. This position is being offered on W2 basis only (no Corp-to-Corp or 1099), we are NOT a head-hunting agency. Please respond ASAP with your detailed resume in Word format. Resume must address the minimum qualifications listed below.\n\nDATA ENGINEER - DATABRICKS SPECIALIST\n\nAre you a skilled Data Engineer with a passion for modernizing data solutions? We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity. This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity. Time-sensitive, project-based with defined deliverables. Apply now!\n\nYOUR RESUME MUST SHOW THESE MINIMUM QUALIFICATIONS:\n\u2022 3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling.\n\u2022 5+ years background: PySpark or Scala for data engineering tasks.\n\u2022 5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning.\n\u2022 5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF).\n\u2022 3+ years of: Delta Lake architecture and implementation for data warehousing.\n\u2022 3+ years of experience with: BI tools such as Power BI or Tableau for reporting.\n\u2022 Familiarity with: Azure Cloud environment (Blob Storage, ADLS).\n\u2022 Proficiency in: Python scripting for data manipulation and automation.\n\nEducation & Certifications:\n\u2022 Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or related field.\n\u2022 Databricks Certified Data Engineer Associate or relevant industry certifications.\n\u2022 Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields.\n\nJob Types: Full-time, Contract\n\nPay: $60,000.00 per year\n\nCompensation Package:\n\u2022 Hourly pay\n\nSchedule:\n\u2022 8 hour shift\n\nEducation:\n\u2022 Bachelor's (Required)\n\nExperience:\n\u2022 Databricks: 3 years (Required)\n\u2022 Python: 5 years (Required)\n\u2022 PySpark: 5 years (Required)\n\u2022 Scala: 5 years (Required)\n\u2022 Azure Data Lake: 3 years (Required)\n\u2022 Azure Data Factory: 3 years (Required)\n\u2022 Power BI: 5 years (Preferred)\n\u2022 Tableau: 5 years (Preferred)\n\nWork Location: Remote",
    "job_is_remote": true,
    "job_posted_at": "27 days ago",
    "job_posted_at_timestamp": 1750809600,
    "job_posted_at_datetime_utc": "2025-06-25T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuINRzsWvsKKLdrOtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Resume must address the minimum qualifications listed below",
        "3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling",
        "5+ years background: PySpark or Scala for data engineering tasks",
        "5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning",
        "5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF)",
        "3+ years of: Delta Lake architecture and implementation for data warehousing",
        "3+ years of experience with: BI tools such as Power BI or Tableau for reporting",
        "Familiarity with: Azure Cloud environment (Blob Storage, ADLS)",
        "Proficiency in: Python scripting for data manipulation and automation",
        "Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or related field",
        "Databricks Certified Data Engineer Associate or relevant industry certifications",
        "Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields",
        "Bachelor's (Required)",
        "Databricks: 3 years (Required)",
        "Python: 5 years (Required)",
        "PySpark: 5 years (Required)",
        "Scala: 5 years (Required)",
        "Azure Data Lake: 3 years (Required)",
        "Azure Data Factory: 3 years (Required)"
      ],
      "Benefits": [
        "Pay: $60,000.00 per year",
        "Compensation Package:",
        "Hourly pay",
        "8 hour shift"
      ],
      "Responsibilities": [
        "We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity",
        "This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity",
        "Time-sensitive, project-based with defined deliverables"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "VSDv3_uV9S8qF8byAAAAAA==",
    "job_title": "Google Cloud Platform Data Engineer(Fulltime) for Remote",
    "employer_name": "Amaze Systems Inc",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7sj9aM48iM-XElhKT8EMmjwhDrfGkFy-kqO6e&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Remote Jobs\u2122 - DentaQuest",
        "apply_link": "https://dev-cm.dentaquest.com/job/work-from-home-system-data-analyst-google-cloud-platform-data-fn8ov.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://jobxpedia.com/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://buzzcloud.in/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Fulltime opportunity\n\nRole: Google Cloud Platform Data Engineer\n\nLocation: Remote - USA\n\nDuration: FTE only\n\nJob Description:\n\u2022 10+ years' proven experience as a Data Engineer with a focus on Google Cloud Platform services.\n\u2022 Strong proficiency in Google Cloud Platform services such as GCS, Dataflow with Apache Beam (Batch & Stream data processing), BigQuery, cloud Composer and Pub/Sub.\n\u2022 Proficiency in SQL and Python for data manipulation and analysis is mandatory.\n\u2022 Solid understanding of data warehousing concepts and ETL processes.\n\nThanks &Regards\n\nRahul Sharma | Lead Technical Recruiter\nAmaze Systems Inc\n\nE: |",
    "job_is_remote": true,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1752883200,
    "job_posted_at_datetime_utc": "2025-07-19T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVSDv3_uV9S8qF8byAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "PrsQLoxA4Q6wEGlDAAAAAA==",
    "job_title": "Senior/Lead Data Engineer , Remote, $63/HR",
    "employer_name": "FASTRA LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4AyPlflVLMXHqcj8rlWeGkNh8rDl0qeg-V8eO&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Quality Contracts",
        "apply_link": "https://qualitycontracts.co.uk/contract-jobs/remote-senior-lead-data-engineer-remote-63-hr-in-usa-1752741389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior/Lead Data Engineer\n\nLocation: Remote\n\nDuration: 12 Months\n\nResponsibilities:\n\u2022 Collaborate with cross-functional stakeholders (including data scientists, analysts, and business teams) to define data requirements, processing needs, and analytics solutions.\n\u2022 Create and sustain scalable data models and efficient extraction processes.\n\u2022 Establish and monitor data quality checks and validation systems.\n\u2022 Construct and enhance business intelligence dashboards.\n\u2022 Produce comprehensive documentation for data models and processes.\n\u2022 Adapt to shifting priorities and ad-hoc requests.\n\u2022 Drive continuous improvement in data practices and visualization techniques.\n\nRequirements:\n\u2022 5+ years of proven experience in database engineering and software development.\n\u2022 Advanced skills in SQL for complex query development and database management.\n\u2022 Strong Python programming skills for automation and data processing workflows.\n\u2022 Ability to handle large-scale data processing tasks using Spark.\n\u2022 Knowledge of data visualization tools, Qlik is preferred.\n\u2022 Familiarity with cloud platforms, particularly AWS (Glue and Athena desired).\n\u2022 Growth mindset with enthusiasm to learn new technologies.",
    "job_is_remote": true,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPrsQLoxA4Q6wEGlDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "6UkieosGdj3DK2hqAAAAAA==",
    "job_title": "Data Engineer - Growth Insights and Foundations",
    "employer_name": "Netflix",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQljIThV_aNQmMjwmu4kEPJEPGF3VYelRwpmpum&s=0",
    "employer_website": "https://about.netflix.com",
    "job_publisher": "Remote Rocketship",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Remote Rocketship",
        "apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description:\n\u2022 Partner closely with data scientists and other engineers to build low-latency data products\n\u2022 Ensure the availability of critical data to enhance \"in-the-moment\" experiences\n\u2022 Develop highly available and reliable distributed data systems and services\n\u2022 Optimize for the best customer experience with data insights\n\u2022 Ensure timely delivery of high-quality data for Netflix product\n\nRequirements:\n\u2022 Proficient in at least one major language preferably on the JVM stack (e.g., Java, Scala) and SQL (any variant)\n\u2022 Strive to write elegant and maintainable code\n\u2022 Comfortable with picking up new technologies\n\u2022 Have a product mindset and are curious to understand the business's needs\n\u2022 Naturally collaborative style to work with product management, data science, engineering, etc.\n\u2022 Strong data intuition and know how to apply analytical skills to support building high quality data products\n\u2022 Experience building applications that use large-scale distributed systems and data processing frameworks (batch and real-time)\n\u2022 Passionate about making data available for self-service and wider integration\n\u2022 Knowledge about transport protocols and building APIs/services and frameworks (e.g. Spring, gRPC)\n\u2022 Experience in supporting and maintaining products that run 24x7\n\u2022 Can craft scalable systems and solutions to realize a range of product and engineering goals\n\u2022 Strong operational awareness and design multi-tenant systems handling high-scale demands\n\u2022 Prioritize observability in designs with comprehensive monitoring, logging, and alerting\n\u2022 Own what you build and have a passion for quality\n\u2022 Comfortable working in agile environments with vague requirements\n\u2022 Nimble and can pivot easily when needed\n\u2022 Unafraid to take smart risks\n\nBenefits:\n\u2022 Health Plans\n\u2022 Mental Health support\n\u2022 401(k) Retirement Plan with employer match\n\u2022 Stock Option Program\n\u2022 Disability Programs\n\u2022 Health Savings and Flexible Spending Accounts\n\u2022 Family-forming benefits\n\u2022 Life and Serious Injury Benefits\n\u2022 Paid leave of absence programs\n\u2022 Paid time off",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6UkieosGdj3DK2hqAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 170000,
    "job_max_salary": 720000,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "j0uGbWgrvId1fahdAAAAAA==",
    "job_title": "Data Engineer (Full-Time, Remote, North Carolina Based)",
    "employer_name": "Alliance Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRhPb_eu7pJl1gzkeLNRLOVuqLa0EIv62YtXd0S&s=0",
    "employer_website": "https://www.alliancehealthplan.org",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Alliance/Job/Data-Engineer-(Full-Time,-Remote,-North-Carolina-Based)/-in-Morrisville,NC?jid=63c088f9feb31195&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-full-time-remote-north-carolina-based-alliance-health-JV_KO0,51_KE52,67.htm?jl=1009695680692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-full-time-remote-north-carolina-based-at-alliance-health-4256300763?refId=ZZ8HCMbeJKzEURTwY97AHQ%3D%3D&trackingId=Ty0o5ZD5tUD13LeFLBMwKA%3D%3D&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/685c8729de1ebd49c8be30eb-data-engineer-full-time-remote-north-carolina-based?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5274727037?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://vividhireio.com/job/606346?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kinetichires",
        "apply_link": "https://kinetichires.net/job/data-analyst-diversity-equity-inclusion-and-health-equity-full-time-remote-north-carolina-based-264200?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages. The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities.\n\nThis position is fulltime remote. Selected candidate must reside in North Carolina. Some travel for onsite meetings to the Home office at Morrisville may be required.\n\nResponsibilities & Duties\n\u2022 Analyze business and technical requirements for the design of data integration solutions\n\u2022 Define the overall data integration and dataflow architectures to support data integration projects\n\u2022 Design and develop SQL and SSIS processes to support data integration projects\n\u2022 Design and develop APIs to consume and distribute healthcare data\n\u2022 Design, develop and execute unit testing plans\n\u2022 Ensure data quality and integrity in all data integration projects\n\u2022 Develop technical and business process documentation for data integration projects\n\u2022 Maintain and continually improve data integration projects\n\u2022 Assist in establishing standards for the design, development, implementation and support of data integration projects\n\u2022 Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders\n\u2022 Any other tasks as reasonably required\n\nMinimum Requirements\n\nEducation & Experience:\n\u2022 Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position.\n\u2022 Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis.\n\nPreferred\n\u2022 Bachelor\u2019s degree plus five (5) years of experience in a computer science related field including experience in a data integration or ETL development position including developing complex data integration software applications.\n\u2022 Microsoft Certified Solutions Expert, MuleSoft Certified Developer and/or HL7 Certifications.\n\nKnowledge, Skills, & Abilities\n\u2022 Expert programming in SQL\n\u2022 Proficient designing and developing ETL processes, preferably using SSIS\n\u2022 Proficient designing and developing APIs, preferably using .NET Framework\n\u2022 Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI\n\u2022 Experience working with API management and data integration platforms such as Apigee or MuleSoft\n\u2022 Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP\n\u2022 Experience working with HIEs and/or HISPs\n\u2022 Strong communication and organizational skills\n\u2022 Ability to access and analyze large data sets for completeness and quality\n\u2022 Ability to work independently and in a team setting\n\nEmployment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date.\n\nSalary Range\n\n$102,424-$130,591/Annually\n\nExact compensation will be determined based on the candidate's education, experience, external market data and consideration of internal equity.\n\nAn excellent fringe benefit package accompanies the salary, which includes:\n\u2022 Medical, Dental, Vision, Life, Long Term Disability\n\u2022 Generous retirement savings plan\n\u2022 Flexible work schedules including hybrid/remote options\n\u2022 Paid time off including vacation, sick leave, holiday, management leave\n\u2022 Dress flexibility\n\nEqual Opportunity Employer\nThis employer is required to notify all applicants of their rights pursuant to federal employment laws. For further information, please review the Know Your Rights notice from the Department of Labor.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "North Carolina",
    "job_city": null,
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.7595731,
    "job_longitude": -79.01929969999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dj0uGbWgrvId1fahdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 102424,
    "job_max_salary": 130591,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Selected candidate must reside in North Carolina",
        "Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position",
        "Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis",
        "Knowledge, Skills, & Abilities",
        "Expert programming in SQL",
        "Proficient designing and developing ETL processes, preferably using SSIS",
        "Proficient designing and developing APIs, preferably using .NET Framework",
        "Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI",
        "Experience working with API management and data integration platforms such as Apigee or MuleSoft",
        "Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP",
        "Experience working with HIEs and/or HISPs",
        "Strong communication and organizational skills",
        "Ability to access and analyze large data sets for completeness and quality",
        "Ability to work independently and in a team setting",
        "Employment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date"
      ],
      "Benefits": [
        "$102,424-$130,591/Annually",
        "An excellent fringe benefit package accompanies the salary, which includes:",
        "Medical, Dental, Vision, Life, Long Term Disability",
        "Generous retirement savings plan",
        "Flexible work schedules including hybrid/remote options",
        "Paid time off including vacation, sick leave, holiday, management leave",
        "Dress flexibility"
      ],
      "Responsibilities": [
        "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages",
        "The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities",
        "This position is fulltime remote",
        "Analyze business and technical requirements for the design of data integration solutions",
        "Define the overall data integration and dataflow architectures to support data integration projects",
        "Design and develop SQL and SSIS processes to support data integration projects",
        "Design and develop APIs to consume and distribute healthcare data",
        "Design, develop and execute unit testing plans",
        "Ensure data quality and integrity in all data integration projects",
        "Develop technical and business process documentation for data integration projects",
        "Maintain and continually improve data integration projects",
        "Assist in establishing standards for the design, development, implementation and support of data integration projects",
        "Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders",
        "Any other tasks as reasonably required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "NEO12lrRiYzpcopxAAAAAA==",
    "job_title": "Junior Data Engineer, Entry Level, (Remote)",
    "employer_name": "Jobright.ai",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcReKYi9Ou9Of_UDMPu0za4gYQx6jmIH5nEXq2dI&s=0",
    "employer_website": "https://jobright.ai",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/687c8ddbfd9af3ce40360fa6-junior-data-engineer-entry-level-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/687c8ddbfd9af3ce40360fa6-junior-data-engineer-entry-level-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://prowlremote.com/job/131094?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Flexremote003.Kesug.com",
        "apply_link": "https://flexremote003.kesug.com/job/parttime-remote-specialist-data-entry-junior-remote/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: United States\n\nJob Summary:\n\nSegmed, Inc. is a fast-growing startup focused on revolutionizing healthcare research through a medical imaging data platform. They are seeking a Junior Data Engineer to streamline and scale data operations, ensuring data integrity and compliance while collaborating with various teams.\n\nResponsibilities:\n\n\u2022 Operating and supporting internal data workflows using a range of tools and platforms\n\n\u2022 Proactively monitoring and improving data operation pipelines as well as debugging and resolving data related issues\n\n\u2022 Collaborating with cross-functional teams \u2014 clinical, engineering, product, and commercial \u2014 to support project delivery\n\n\u2022 Performing quality checks and supporting de-identification efforts to ensure patient privacy\n\n\u2022 Following and reinforcing Segmed's compliance policies and data security protocols\n\n\u2022 Managing multiple projects and shifting priorities in a dynamic environment\n\nQualifications:\n\nRequired:\n\n\u2022 1\u20134 years of experience in a data engineering, technical operations, or software support role\n\n\u2022 Proficiency in Python, SQL and familiarity with bash, Git, file formats, and data wrangling\n\n\u2022 Comfort working with healthcare data types (or interest in learning about DICOM, ECG, video, and pathology data)\n\n\u2022 Detail-oriented mindset, especially when working with sensitive or regulated data\n\n\u2022 A self-starter who takes ownership and thrives in a collaborative startup environment. A proactive, ownership-driven attitude \u2014 you like to take initiative and figure things out\n\n\u2022 Comfortable working in a fast-paced, startup-style environment with evolving processes\n\n\u2022 Strong communication and collaboration skills across technical and non-technical teams\n\nPreferred:\n\n\u2022 Experience with healthcare, imaging, or clinical data\n\n\u2022 Familiarity with privacy regulations (e.g., HIPAA) or de-identification practices\n\n\u2022 Exposure to cloud platforms like Google Cloud or AWS\n\n\u2022 Prior experience in a startup or fast-paced operational role\n\nCompany:\n\nSegmed provides at-scale access to novel clinical data globally, offering millions of diagnostic-grade medical images to accelerate innovation and mature the evidence needs of life sciences and technology industry. Founded in 2019, the company is headquartered in Palo Alto, California, USA, with a team of 11-50 employees. The company is currently Early Stage. Segmed, Inc. has a track record of offering H1B sponsorships.\n\nThis offer from \"Jobright.ai\" has been enriched by Jobgether.com and got a 72% flex score.",
    "job_is_remote": true,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Washington, DC",
    "job_city": "Washington",
    "job_state": "District of Columbia",
    "job_country": "US",
    "job_latitude": 38.9071923,
    "job_longitude": -77.0368707,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DNEO12lrRiYzpcopxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "1\u20134 years of experience in a data engineering, technical operations, or software support role",
        "Proficiency in Python, SQL and familiarity with bash, Git, file formats, and data wrangling",
        "Comfort working with healthcare data types (or interest in learning about DICOM, ECG, video, and pathology data)",
        "Detail-oriented mindset, especially when working with sensitive or regulated data",
        "A self-starter who takes ownership and thrives in a collaborative startup environment",
        "A proactive, ownership-driven attitude \u2014 you like to take initiative and figure things out",
        "Comfortable working in a fast-paced, startup-style environment with evolving processes",
        "Strong communication and collaboration skills across technical and non-technical teams"
      ],
      "Responsibilities": [
        "They are seeking a Junior Data Engineer to streamline and scale data operations, ensuring data integrity and compliance while collaborating with various teams",
        "Operating and supporting internal data workflows using a range of tools and platforms",
        "Proactively monitoring and improving data operation pipelines as well as debugging and resolving data related issues",
        "Collaborating with cross-functional teams \u2014 clinical, engineering, product, and commercial \u2014 to support project delivery",
        "Performing quality checks and supporting de-identification efforts to ensure patient privacy",
        "Following and reinforcing Segmed's compliance policies and data security protocols",
        "Managing multiple projects and shifting priorities in a dynamic environment"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "3KUvi58ta8OiVWqPAAAAAA==",
    "job_title": "W2 Contract || Title- Azure Data Engineer in 100% Remote || USC & GC only on W2 || 12 Years Exp.",
    "employer_name": "JS Consulting",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ5srw1SEg2aFYK1ozh4gD1IIysdoiTgxUfEKho&s=0",
    "employer_website": "https://www.jsconsultingco.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Note: Candidate must be Work on W2 and USC & GC Visa only.\n\nTitle: Azure Data Engineer (12+ Years Experinece)\n\nLocation: Kansas City, MO 100% Remote\n\nDuration: 6-12+ Months (W2 Contract)\n\nInterview: Video\n\nVisa: USC/GC (must work on our w2) (need strong communication)\n\nJob Description\n\nMake sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager.\n\nDon't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes.\n\nCommunication must be flawless\n\nResume should not be more than 6 pages\n\nMust have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020.\n\nJob Description-\n\nMust have strong Azure, ADF and Databricks experience.\n\nThe purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks. It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes. This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files.\n\nEssential Duties And Responsibilities\n\u2022 Work with a highly dynamic team focused on Digital Transformation.\n\u2022 Understand the domain and business processes to implement successful data pipelines.\n\u2022 Provide work status and coordinate with Data Engineers.\n\u2022 Manage customer deliverables and regularly report the status via Weekly/Monthly reviews.\n\u2022 Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views\n\u2022 Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server.\n\u2022 Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot.\n\u2022 Design normalized database tables with proper indexing and constraints.\n\u2022 Perform SQL query tuning and performance optimization on complex and inefficient queries.\n\u2022 Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets.\n\u2022 Collaborate with DBA on database design and performance enhancements.\n\u2022 Leading in all phases of the software development life cycle in a team environment.\n\u2022 Debug existing code and troubleshoot for issues.\n\u2022 Design and provide a framework for maintaining existing data warehouse for reporting and data analytics.\n\u2022 Follow best practices, design, develop, test and document ETL processes.",
    "job_is_remote": true,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Kansas City, MO",
    "job_city": "Kansas City",
    "job_state": "Missouri",
    "job_country": "US",
    "job_latitude": 39.099726499999996,
    "job_longitude": -94.5785667,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3KUvi58ta8OiVWqPAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Note: Candidate must be Work on W2 and USC & GC Visa only",
        "Title: Azure Data Engineer (12+ Years Experinece)",
        "Visa: USC/GC (must work on our w2) (need strong communication)",
        "Communication must be flawless",
        "Resume should not be more than 6 pages",
        "Must have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020",
        "Must have strong Azure, ADF and Databricks experience",
        "Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server"
      ],
      "Responsibilities": [
        "Make sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager",
        "Don't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes",
        "The purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks",
        "It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes",
        "This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files",
        "Work with a highly dynamic team focused on Digital Transformation",
        "Understand the domain and business processes to implement successful data pipelines",
        "Provide work status and coordinate with Data Engineers",
        "Manage customer deliverables and regularly report the status via Weekly/Monthly reviews",
        "Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views",
        "Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot",
        "Design normalized database tables with proper indexing and constraints",
        "Perform SQL query tuning and performance optimization on complex and inefficient queries",
        "Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets",
        "Collaborate with DBA on database design and performance enhancements",
        "Leading in all phases of the software development life cycle in a team environment",
        "Debug existing code and troubleshoot for issues",
        "Design and provide a framework for maintaining existing data warehouse for reporting and data analytics",
        "Follow best practices, design, develop, test and document ETL processes"
      ]
    },
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "eCMK1KxoFvtFAEKKAAAAAA==",
    "job_title": "Data Engineer 5 - Playback",
    "employer_name": "Netflix",
    "employer_logo": null,
    "employer_website": "https://about.netflix.com",
    "job_publisher": "Careers At Netflix",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At Netflix",
        "apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://netflix.wd1.myworkdayjobs.com/netflix/job/usa---remote/data-engineer-5---playback_jr33081?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=949db8d2e2fdda48&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ShowbizJobs",
        "apply_link": "https://www.showbizjobs.com/jobs/netflix-data-engineer-games-in-los-gatos/jid-23obn2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-5-playback-netflix-JV_KO0,24_KE25,32.htm?jl=1009715302012&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/freelancer-data-engineer-developer-at-norconsulting-global-recruitment-4267468273?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6803ddfc9501a3a17fa1d905-data-engineer-5---playback?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Taro",
        "apply_link": "https://www.jointaro.com/jobs/netflix/data-engineer-5-playback/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\n\nWe are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.\n\nThe Playback pod within the Consumer Engineering Product Data Engineering team partners with client and edge systems to produce playback datasets, a source of truth for Title, Live and Ads performance, title popularity (Netflix top 10), and member quality of experience. This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP. It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network.\n\nThis role is focused on supporting Netflix\u2019s core datasets, delivering high-quality business metrics, and building systems to process batch and real-time data at a large scale. Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques. Since data engineers are responsible for their pipelines at Netflix, this role requires engineers to take ownership of the operational excellence in their domain. In addition, the ideal candidate will have excellent data intuition and share our passion for continuously improving how we handle streaming data at Netflix.\n\nWho are you?\n\u2022 You have strong product ownership and good intuition on how data is used to drive business decisions\n\u2022 You strive to write elegant code and are comfortable with independently picking up new technologies.\n\u2022 You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL.\n\u2022 You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models.\n\u2022 You have a strong background in at least one of the following: distributed data processing or software engineering of data services.\n\u2022 You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets.\n\u2022 You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation.\n\u2022 At Netflix, you own your code, services, and pipelines. You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain.\n\u2022 You have an eye for detail, good data intuition, and a passion for data quality.\n\u2022 You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.\n\u2022 You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks.\n\nWhat you will do:\n\u2022 Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences.\n\u2022 Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data\n\u2022 Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets.\n\u2022 Maintain and rethink existing pipelines to improve scalability and maintainability.\n\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\n\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\n\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\n\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\nJob is open for no less than 7 days and will be removed when the position is filled.",
    "job_is_remote": true,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1752364800,
    "job_posted_at_datetime_utc": "2025-07-13T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DeCMK1KxoFvtFAEKKAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques",
        "You have strong product ownership and good intuition on how data is used to drive business decisions",
        "You strive to write elegant code and are comfortable with independently picking up new technologies",
        "You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL",
        "You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models",
        "You have a strong background in at least one of the following: distributed data processing or software engineering of data services",
        "You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets",
        "You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation",
        "At Netflix, you own your code, services, and pipelines",
        "You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain",
        "You have an eye for detail, good data intuition, and a passion for data quality",
        "You relate to and embody many of the aspects of the Netflix Culture",
        "You love working independently while also collaborating and giving/receiving candid feedback",
        "You are comfortable working in a rapidly changing environment with ambiguous requirements",
        "You are nimble and take intelligent risks"
      ],
      "Benefits": [
        "Our compensation structure consists solely of an annual salary; we do not have bonuses",
        "You choose each year how much of your compensation you want in salary versus stock options",
        "To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range",
        "The range for this role is $170,000 - $720,000",
        "Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits",
        "We also offer paid leave of absence programs",
        "Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off",
        "Full-time salaried employees are immediately entitled to flexible time off"
      ],
      "Responsibilities": [
        "This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP",
        "It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network",
        "Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences",
        "Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data",
        "Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets",
        "Maintain and rethink existing pipelines to improve scalability and maintainability"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "RxeJao3yHoNlD18FAAAAAA==",
    "job_title": "Data Engineer - MLOps | Remote in the USA - 1725",
    "employer_name": "PlacingIT",
    "employer_logo": null,
    "employer_website": "https://www.placingit.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - MLOps | Remote in the USA - 1725\n\nLocation: Hybrid - Onsite 4 days a week in Dallas, TX\nEmployment Type 6-12+ months contract w/possible extensions\nInterview: In-Person interview is required for this role.\nResidency Requirements: Those authorized to work in the US are encouraged to apply.\n\nRequired Qualifications\n\u2022 5+ years of software development experience building and delivering customer facing cloud-based analytic solutions.\n\u2022 5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives.\n\u2022 5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope.\n\u2022 5+ years coding in Python/PySpark, SQL, and LLM modeling.\n\u2022 5+ years in cloud-based platforms such as Azure, GCP and AWS.\n\nPreferred Qualifications\n\u2022 5+ years of experience leading the design, development, and deployment of applications on Palantir Foundry Platform\n\u2022 Retail merchandising domain expertise including category management, assortment optimization, product clustering, product price sensitivity and promotion affinity evaluation, store clustering and localization.\n\u2022 Experience with Prompt Engineering developing solutions leveraging LLM models.\n\nResponsibilities include:\n\u2022 You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions.\n\u2022 You will solve complex problems and deliver decision support tools to improve customer experience.\n\u2022 Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses.\n\u2022 Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces.\n\u2022 Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses.\n\u2022 Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry.\n\u2022 Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle.\n\u2022 Provide technical guidance, mentorship, and support to junior developers and team members.\n\u2022 Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem.",
    "job_is_remote": true,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": "Dallas, TX",
    "job_city": "Dallas",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7766642,
    "job_longitude": -96.79698789999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRxeJao3yHoNlD18FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Residency Requirements: Those authorized to work in the US are encouraged to apply",
        "5+ years of software development experience building and delivering customer facing cloud-based analytic solutions",
        "5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives",
        "5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope",
        "5+ years coding in Python/PySpark, SQL, and LLM modeling",
        "5+ years in cloud-based platforms such as Azure, GCP and AWS"
      ],
      "Responsibilities": [
        "You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions",
        "You will solve complex problems and deliver decision support tools to improve customer experience",
        "Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses",
        "Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces",
        "Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses",
        "Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry",
        "Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle",
        "Provide technical guidance, mentorship, and support to junior developers and team members",
        "Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": "PrsQLoxA4Q6wEGlDAAAAAA==",
    "job_title": "Senior/Lead Data Engineer , Remote, $63/HR",
    "employer_name": "FASTRA LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4AyPlflVLMXHqcj8rlWeGkNh8rDl0qeg-V8eO&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Quality Contracts",
        "apply_link": "https://qualitycontracts.co.uk/contract-jobs/remote-senior-lead-data-engineer-remote-63-hr-in-usa-1752741389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior/Lead Data Engineer\n\nLocation: Remote\n\nDuration: 12 Months\n\nResponsibilities:\n\u2022 Collaborate with cross-functional stakeholders (including data scientists, analysts, and business teams) to define data requirements, processing needs, and analytics solutions.\n\u2022 Create and sustain scalable data models and efficient extraction processes.\n\u2022 Establish and monitor data quality checks and validation systems.\n\u2022 Construct and enhance business intelligence dashboards.\n\u2022 Produce comprehensive documentation for data models and processes.\n\u2022 Adapt to shifting priorities and ad-hoc requests.\n\u2022 Drive continuous improvement in data practices and visualization techniques.\n\nRequirements:\n\u2022 5+ years of proven experience in database engineering and software development.\n\u2022 Advanced skills in SQL for complex query development and database management.\n\u2022 Strong Python programming skills for automation and data processing workflows.\n\u2022 Ability to handle large-scale data processing tasks using Spark.\n\u2022 Knowledge of data visualization tools, Qlik is preferred.\n\u2022 Familiarity with cloud platforms, particularly AWS (Glue and Athena desired).\n\u2022 Growth mindset with enthusiasm to learn new technologies.",
    "job_is_remote": true,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPrsQLoxA4Q6wEGlDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "eCMK1KxoFvtFAEKKAAAAAA==",
    "job_title": "Data Engineer 5 - Playback",
    "employer_name": "Netflix",
    "employer_logo": null,
    "employer_website": "https://about.netflix.com",
    "job_publisher": "Careers At Netflix",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At Netflix",
        "apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://netflix.wd1.myworkdayjobs.com/netflix/job/usa---remote/data-engineer-5---playback_jr33081?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=949db8d2e2fdda48&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ShowbizJobs",
        "apply_link": "https://www.showbizjobs.com/jobs/netflix-data-engineer-games-in-los-gatos/jid-23obn2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-5-playback-netflix-JV_KO0,24_KE25,32.htm?jl=1009715302012&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/freelancer-data-engineer-developer-at-norconsulting-global-recruitment-4267468273?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6803ddfc9501a3a17fa1d905-data-engineer-5---playback?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Taro",
        "apply_link": "https://www.jointaro.com/jobs/netflix/data-engineer-5-playback/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\n\nWe are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.\n\nThe Playback pod within the Consumer Engineering Product Data Engineering team partners with client and edge systems to produce playback datasets, a source of truth for Title, Live and Ads performance, title popularity (Netflix top 10), and member quality of experience. This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP. It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network.\n\nThis role is focused on supporting Netflix\u2019s core datasets, delivering high-quality business metrics, and building systems to process batch and real-time data at a large scale. Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques. Since data engineers are responsible for their pipelines at Netflix, this role requires engineers to take ownership of the operational excellence in their domain. In addition, the ideal candidate will have excellent data intuition and share our passion for continuously improving how we handle streaming data at Netflix.\n\nWho are you?\n\u2022 You have strong product ownership and good intuition on how data is used to drive business decisions\n\u2022 You strive to write elegant code and are comfortable with independently picking up new technologies.\n\u2022 You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL.\n\u2022 You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models.\n\u2022 You have a strong background in at least one of the following: distributed data processing or software engineering of data services.\n\u2022 You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets.\n\u2022 You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation.\n\u2022 At Netflix, you own your code, services, and pipelines. You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain.\n\u2022 You have an eye for detail, good data intuition, and a passion for data quality.\n\u2022 You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.\n\u2022 You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks.\n\nWhat you will do:\n\u2022 Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences.\n\u2022 Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data\n\u2022 Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets.\n\u2022 Maintain and rethink existing pipelines to improve scalability and maintainability.\n\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\n\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\n\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\n\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\nJob is open for no less than 7 days and will be removed when the position is filled.",
    "job_is_remote": true,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1752364800,
    "job_posted_at_datetime_utc": "2025-07-13T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DeCMK1KxoFvtFAEKKAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques",
        "You have strong product ownership and good intuition on how data is used to drive business decisions",
        "You strive to write elegant code and are comfortable with independently picking up new technologies",
        "You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL",
        "You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models",
        "You have a strong background in at least one of the following: distributed data processing or software engineering of data services",
        "You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets",
        "You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation",
        "At Netflix, you own your code, services, and pipelines",
        "You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain",
        "You have an eye for detail, good data intuition, and a passion for data quality",
        "You relate to and embody many of the aspects of the Netflix Culture",
        "You love working independently while also collaborating and giving/receiving candid feedback",
        "You are comfortable working in a rapidly changing environment with ambiguous requirements",
        "You are nimble and take intelligent risks"
      ],
      "Benefits": [
        "Our compensation structure consists solely of an annual salary; we do not have bonuses",
        "You choose each year how much of your compensation you want in salary versus stock options",
        "To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range",
        "The range for this role is $170,000 - $720,000",
        "Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits",
        "We also offer paid leave of absence programs",
        "Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off",
        "Full-time salaried employees are immediately entitled to flexible time off"
      ],
      "Responsibilities": [
        "This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP",
        "It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network",
        "Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences",
        "Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data",
        "Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets",
        "Maintain and rethink existing pipelines to improve scalability and maintainability"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "RxeJao3yHoNlD18FAAAAAA==",
    "job_title": "Data Engineer - MLOps | Remote in the USA - 1725",
    "employer_name": "PlacingIT",
    "employer_logo": null,
    "employer_website": "https://www.placingit.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - MLOps | Remote in the USA - 1725\n\nLocation: Hybrid - Onsite 4 days a week in Dallas, TX\nEmployment Type 6-12+ months contract w/possible extensions\nInterview: In-Person interview is required for this role.\nResidency Requirements: Those authorized to work in the US are encouraged to apply.\n\nRequired Qualifications\n\u2022 5+ years of software development experience building and delivering customer facing cloud-based analytic solutions.\n\u2022 5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives.\n\u2022 5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope.\n\u2022 5+ years coding in Python/PySpark, SQL, and LLM modeling.\n\u2022 5+ years in cloud-based platforms such as Azure, GCP and AWS.\n\nPreferred Qualifications\n\u2022 5+ years of experience leading the design, development, and deployment of applications on Palantir Foundry Platform\n\u2022 Retail merchandising domain expertise including category management, assortment optimization, product clustering, product price sensitivity and promotion affinity evaluation, store clustering and localization.\n\u2022 Experience with Prompt Engineering developing solutions leveraging LLM models.\n\nResponsibilities include:\n\u2022 You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions.\n\u2022 You will solve complex problems and deliver decision support tools to improve customer experience.\n\u2022 Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses.\n\u2022 Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces.\n\u2022 Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses.\n\u2022 Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry.\n\u2022 Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle.\n\u2022 Provide technical guidance, mentorship, and support to junior developers and team members.\n\u2022 Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem.",
    "job_is_remote": true,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": "Dallas, TX",
    "job_city": "Dallas",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7766642,
    "job_longitude": -96.79698789999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRxeJao3yHoNlD18FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Residency Requirements: Those authorized to work in the US are encouraged to apply",
        "5+ years of software development experience building and delivering customer facing cloud-based analytic solutions",
        "5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives",
        "5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope",
        "5+ years coding in Python/PySpark, SQL, and LLM modeling",
        "5+ years in cloud-based platforms such as Azure, GCP and AWS"
      ],
      "Responsibilities": [
        "You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions",
        "You will solve complex problems and deliver decision support tools to improve customer experience",
        "Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses",
        "Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces",
        "Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses",
        "Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry",
        "Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle",
        "Provide technical guidance, mentorship, and support to junior developers and team members",
        "Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": "SjsMVNnmi0YDQr4jAAAAAA==",
    "job_title": "Senior Data Engineer - Full remote",
    "employer_name": "All European Careers",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTfGAmX7q4Hokut7-XE9J8D2NsW-8elqZh_HaX4&s=0",
    "employer_website": "https://www.all-european-careers.com",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JOBITT",
        "apply_link": "https://jobitt.com/job-openings/external/senior-engineer-full-stack-7870835690680623803?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: Europe\n\nThis is a remote position.\n\nFor an International Institution in Geneva, we are urgently looking for an experienced Senior Data Engineer preferably with Azure experience in Geneva or Full remote.As Senior Data Engineer, you work with the Cloud Architect, Data Architect, Solution Engineer and other technical professionals to act as technical focal point for the development of prototypes, providing advice and technical feasibility view, and other technical activities to ensure the Cloud data Platform architecture and technical configuration will address business and IT objectives.\n\nCandidates need to be fluent in English. This positions is long-term. Work permit not required.Candidates need to be based in Europe.\n\nTasks and Responsibilities:\n\u2022 Plan the activites and concrete milestones to deliver the new Cloud Data Platform Architecture;\n\u2022 Lead the development and technical implementation aligned to the Data Platform Architecture;\n\u2022 Provide expertise and technical advice in the development of conceptual, logical and physical data models, in support of interoperability protocols, API design and management, cybersecurity aspects and business intelligence;\n\u2022 Create the data factory pipelines to ingest data, apply data transformations to curate data using databricks, and make data available for downstream applications (API) or reporting from SQL database, Cosmos DB or Synapse Analytics;\n\u2022 Automation and development lifecycle control by development of rbooks, fctions, devops projects, sync to repos, ARM templates and Azure Blueprints;\n\u2022 Working with the Senior Solution Engineer derstand and catalogue the current landscape of Data and systems;\n\u2022 Develop and maintain the to-be Cloud Data Platform with the required capabilities in conformance with security requirements, agreements-based, with user-driven configuration and through documented design and configuration;\n\u2022 Provide technical expertise regarding short terms solution options to leverage in the immediate future to meet urgent data related demands, with consideration of the organization\u2019s wider needs and the potential of deploying enterprise data solution platforms at the enterprise level;\n\u2022 Provide data insights and best practices, ensuring they are reflected in the development;\n\nProfile:\n\u2022 Bachelor or Master degree;\n\u2022 +5 years of relevant experience as Data Engineer;\n\u2022 Experience with modern Data technologies such as Business Intelligence, Analytics, AI, and Big Data;\n\u2022 Experience programming multiple languages, e.g. Phyton, R, Java, Scala, etc; Professional level vendor certifications, such as Microsoft, ITIL, and others;\n\u2022 Extensive experience with Microsoft Azure and other cloud technologies and interoperable solutions,experience as a data analyst, engineer and developer;\n\u2022 Demonstrated experience in the design, development, and implementation of various integrations between diverse infrastructure services, data models and architecture;\n\u2022 Demonstrated experience with the IT systems development life cycle (SDLC), as well as Agile/Scrum methodologies and ITIL processes;\n\u2022 Ability to conduct requirements gathering, interpret needs, and design solutions and manage expectations;\n\u2022 Professional experience in technical design and support of global, distributed corporate information systems;\n\u2022 Understanding state of Azure, Google and AWS components and reference architectures;\n\nExperience in designing for Bigdata/data warehousing/business intelligence/reporting solutions for various physical environments, both on-premises as well as in the Microsoft cloud, while influencing for the most efficient approach based on business requirements;\n\nExcellent written and spoken English;\n\nInterested: Please send your resume to:\nresume@all-european-careers.com\n\nThis offer from \"All European Careers\" has been enriched by Jobgether.com and got a 77% flex score.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DSjsMVNnmi0YDQr4jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "3KUvi58ta8OiVWqPAAAAAA==",
    "job_title": "W2 Contract || Title- Azure Data Engineer in 100% Remote || USC & GC only on W2 || 12 Years Exp.",
    "employer_name": "JS Consulting",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ5srw1SEg2aFYK1ozh4gD1IIysdoiTgxUfEKho&s=0",
    "employer_website": "https://www.jsconsultingco.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Note: Candidate must be Work on W2 and USC & GC Visa only.\n\nTitle: Azure Data Engineer (12+ Years Experinece)\n\nLocation: Kansas City, MO 100% Remote\n\nDuration: 6-12+ Months (W2 Contract)\n\nInterview: Video\n\nVisa: USC/GC (must work on our w2) (need strong communication)\n\nJob Description\n\nMake sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager.\n\nDon't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes.\n\nCommunication must be flawless\n\nResume should not be more than 6 pages\n\nMust have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020.\n\nJob Description-\n\nMust have strong Azure, ADF and Databricks experience.\n\nThe purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks. It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes. This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files.\n\nEssential Duties And Responsibilities\n\u2022 Work with a highly dynamic team focused on Digital Transformation.\n\u2022 Understand the domain and business processes to implement successful data pipelines.\n\u2022 Provide work status and coordinate with Data Engineers.\n\u2022 Manage customer deliverables and regularly report the status via Weekly/Monthly reviews.\n\u2022 Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views\n\u2022 Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server.\n\u2022 Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot.\n\u2022 Design normalized database tables with proper indexing and constraints.\n\u2022 Perform SQL query tuning and performance optimization on complex and inefficient queries.\n\u2022 Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets.\n\u2022 Collaborate with DBA on database design and performance enhancements.\n\u2022 Leading in all phases of the software development life cycle in a team environment.\n\u2022 Debug existing code and troubleshoot for issues.\n\u2022 Design and provide a framework for maintaining existing data warehouse for reporting and data analytics.\n\u2022 Follow best practices, design, develop, test and document ETL processes.",
    "job_is_remote": true,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Kansas City, MO",
    "job_city": "Kansas City",
    "job_state": "Missouri",
    "job_country": "US",
    "job_latitude": 39.099726499999996,
    "job_longitude": -94.5785667,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3KUvi58ta8OiVWqPAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Note: Candidate must be Work on W2 and USC & GC Visa only",
        "Title: Azure Data Engineer (12+ Years Experinece)",
        "Visa: USC/GC (must work on our w2) (need strong communication)",
        "Communication must be flawless",
        "Resume should not be more than 6 pages",
        "Must have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020",
        "Must have strong Azure, ADF and Databricks experience",
        "Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server"
      ],
      "Responsibilities": [
        "Make sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager",
        "Don't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes",
        "The purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks",
        "It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes",
        "This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files",
        "Work with a highly dynamic team focused on Digital Transformation",
        "Understand the domain and business processes to implement successful data pipelines",
        "Provide work status and coordinate with Data Engineers",
        "Manage customer deliverables and regularly report the status via Weekly/Monthly reviews",
        "Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views",
        "Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot",
        "Design normalized database tables with proper indexing and constraints",
        "Perform SQL query tuning and performance optimization on complex and inefficient queries",
        "Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets",
        "Collaborate with DBA on database design and performance enhancements",
        "Leading in all phases of the software development life cycle in a team environment",
        "Debug existing code and troubleshoot for issues",
        "Design and provide a framework for maintaining existing data warehouse for reporting and data analytics",
        "Follow best practices, design, develop, test and document ETL processes"
      ]
    },
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "L2b_MhwdllUxOf0jAAAAAA==",
    "job_title": "Data Engineers (Hybrid | DC Area) Remote / Telecommute Jobs",
    "employer_name": "Rackner",
    "employer_logo": null,
    "employer_website": "https://rackner.com",
    "job_publisher": "Security Clearance Jobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Security Clearance Jobs",
        "apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Title: Data Engineer\n\nLocation: Falls Church, VA (Hybrid)\n\nClearance: Secret Clearance\n\nAbout this role:\n\nRackner is looking for a Data Engineer that will be working within an Agile DevSecOps team environment using latest cloud-native technologies to architect and implement containerized applications, CI/CD pipelines, and Kubernetes platforms using best practices and leading technologies.\n\nWe are seeking professionals with:\n\nB.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms\n\nModern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)\n\nDeveloping data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules\n\nMature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR\n\nNice to have:\n\nM.S in Computer Science or related\n\nAI/ML\n\nKubernetes (Rancher RKE2, AWS EKS) and microservice architectures\n\nData engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR\n\nNIST Risk Management Framework and security accreditation process and tasks\n\nWhat will make you successful:\n\nUsing DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer\n\nDeveloping software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes\n\nLeveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs\n\nApplying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders\n\nEmbracing a shared responsibility for system security\n\nPerforming threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle\n\nContinuously engaging with project teams to deliver quality products\n\nParticipating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions\n\nWho We Are:\n\nRackner is a software consultancy that builds cloud-native solutions for startups, enterprises, and the public sector.\n\nWe are an energetic, growing consultancy with a passion for solving big problems for both startups and enterprises.\n\nEach of us enable digital transformation for large organizations through the newest in distributed technologies as we are laser focused on end-to-end application development, DevSecOps, AI/ML and systems architecture and our methodology focuses on cloud-first and cost-effective innovation.\n\nOur customers hail from a diverse, ever-growing list of industries.\n\nBenefits/Additional Info:\n\nRackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided . Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!\n\n401K with 100% matching up to 6%\n\nHighly competitive PTO\n\nGreat health insurance with large network of providers\n\nMedical/Dental/Vision\n\nLife Insurance, and short & long term disability\n\nIndustry-Leading Weekly Pay Schedule\n\nHome office & equipment plan\n\n#DataEngineer #AWS #Topsecret #FDA #publictrust #DataIngesting #DataPipeline #Python #Terraform #ETL #AI #ML #dataintegration #bigdataanalyticspipeline #awsbigdata #hadoop #apachespark #RDBMS #awsdynamoDB #collaboration #diversity #equity #Inclusion",
    "job_is_remote": true,
    "job_posted_at": "17 hours ago",
    "job_posted_at_timestamp": 1753102800,
    "job_posted_at_datetime_utc": "2025-07-21T13:00:00.000Z",
    "job_location": "Falls Church, VA",
    "job_city": "Falls Church",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.882334,
    "job_longitude": -77.1710914,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DL2b_MhwdllUxOf0jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "B.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms",
        "Modern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)",
        "Developing data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules",
        "Mature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR",
        "M.S in Computer Science or related",
        "Kubernetes (Rancher RKE2, AWS EKS) and microservice architectures",
        "Data engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR",
        "NIST Risk Management Framework and security accreditation process and tasks"
      ],
      "Benefits": [
        "Rackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided ",
        "Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!",
        "401K with 100% matching up to 6%",
        "Highly competitive PTO",
        "Great health insurance with large network of providers",
        "Life Insurance, and short & long term disability",
        "Industry-Leading Weekly Pay Schedule",
        "Home office & equipment plan"
      ],
      "Responsibilities": [
        "Using DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer",
        "Developing software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes",
        "Leveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs",
        "Applying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders",
        "Embracing a shared responsibility for system security",
        "Performing threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle",
        "Continuously engaging with project teams to deliver quality products",
        "Participating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "XswWukx0iLqtemRoAAAAAA==",
    "job_title": "Principal Data Engineer *Remote - Most states eligible*",
    "employer_name": "Providence",
    "employer_logo": null,
    "employer_website": "http://www.psjhealth.org/",
    "job_publisher": "Teal",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the position\n\nThe Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making. This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics. The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St. Joseph Health system, focusing on quality and value of care metrics. The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights.\n\nResponsibilities\n\u2022 Collect and analyze clinical data from various sources.\n,\n\u2022 Create visualizations and storytelling using data to communicate findings.\n,\n\u2022 Develop novel reporting platforms to address healthcare challenges.\n,\n\u2022 Collaborate with clinical and administrative leaders to evaluate new measures.\n,\n\u2022 Identify impactful findings in large clinical data stores.\n,\n\u2022 Design strong data visualizations to communicate key information across the organization.\n,\n\u2022 Support team members with experience in computational methods.\n,\n\u2022 Develop innovative methods for outcome measures in the whole person care model.\n\nRequirements\n\u2022 Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience.\n,\n\u2022 9 years of relevant data analysis experience, preferably in a biomedical setting.\n,\n\u2022 Experience with database query and analysis languages (e.g., SQL, R, SAS, Python).\n,\n\u2022 Proficiency in data visualization tools (e.g., Tableau, D3).\n\nNice-to-haves\n\u2022 Ph.D. in a related field or equivalent experience.\n,\n\u2022 Certification in an IT discipline, application, or tool upon hire.\n,\n\u2022 Lean certification or Green Belt, Black Belt upon hire.\n,\n\u2022 Certification in Data Science upon hire.\n,\n\u2022 4+ years of development experience in data exchange using FHIR.\n\nBenefits\n\u2022 401(k) retirement savings plan with employer matching.\n,\n\u2022 Health care benefits (medical, dental, vision).\n,\n\u2022 Life insurance.\n,\n\u2022 Disability insurance.\n,\n\u2022 Paid parental leave.\n,\n\u2022 Vacation and holiday time off.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Renton, WA",
    "job_city": "Renton",
    "job_state": "Washington",
    "job_country": "US",
    "job_latitude": 47.4796927,
    "job_longitude": -122.2079218,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXswWukx0iLqtemRoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience",
        "9 years of relevant data analysis experience, preferably in a biomedical setting",
        "Experience with database query and analysis languages (e.g., SQL, R, SAS, Python)",
        "Proficiency in data visualization tools (e.g., Tableau, D3)",
        "Ph.D. in a related field or equivalent experience",
        "Certification in an IT discipline, application, or tool upon hire",
        "Lean certification or Green Belt, Black Belt upon hire",
        "Certification in Data Science upon hire",
        "4+ years of development experience in data exchange using FHIR"
      ],
      "Benefits": [
        "401(k) retirement savings plan with employer matching",
        "Health care benefits (medical, dental, vision)",
        "Life insurance",
        "Disability insurance",
        "Paid parental leave",
        "Vacation and holiday time off"
      ],
      "Responsibilities": [
        "The Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making",
        "This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics",
        "The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St",
        "The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights",
        "Collect and analyze clinical data from various sources",
        "Create visualizations and storytelling using data to communicate findings",
        "Develop novel reporting platforms to address healthcare challenges",
        "Collaborate with clinical and administrative leaders to evaluate new measures",
        "Identify impactful findings in large clinical data stores",
        "Design strong data visualizations to communicate key information across the organization",
        "Support team members with experience in computational methods",
        "Develop innovative methods for outcome measures in the whole person care model"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "bMI3tTg8CkVNRcoxAAAAAA==",
    "job_title": "AWS Data Engineer - Fully Remote - US Only",
    "employer_name": "Scalepex",
    "employer_logo": null,
    "employer_website": "https://scalepex.com",
    "job_publisher": "Jobs By Workable",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://apply.workable.com/scalepex/j/F5ED805D80?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Jobs By Workable",
        "apply_link": "https://apply.workable.com/scalepex/j/F5ED805D80?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Insight Global Jobs",
        "apply_link": "https://jobs.insightglobal.com/find_a_job/connecticut/job-313330/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ApplicantSite",
        "apply_link": "https://applicants.bairesdev.com/job/190/252655/apply?utm_source=trampedecasa&utm_medium=jobposting&utm_campaign=Remote-20240530?utm_source%3Dtrampedecasautm_medium%3Dreferral&utm_campaign=vaga-publicada-na-trampedecasa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "DailyRemote",
        "apply_link": "https://dailyremote.com/remote-job/aws-data-engineer-fully-remote-us-only-3512285?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Ladders",
        "apply_link": "https://www.theladders.com/job/aws-data-engineer-remote-nava-software-solutions-virtual-travel_82025315?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "\u274b Why Scalepex?\n\nScalepex is a dynamic services firm specializing in providing solutions for premium brands like Nike, Pepsi, Toyota, Virgin and Walgreens. Our mission is to connect prominent market leaders with top-tier professionals from around the world, fostering collaboration, efficiency, and growth.\n\n\u274b Take your portfolio to the next level by working with one of our fastest growing clients.\n\nJoin the Innovation Frontier at Scalepex!\n\nAbout the Role\n\nWe are seeking an experienced AWS Data Engineer with a strong background in building scalable data solutions and expertise in utilities-related datasets. The ideal candidate will have at least 5 years of experience in data engineering, a deep understanding of distributed systems, and proficiency with AWS services and tools like Step Functions, Lambda, Glue, and Redshift. This role will focus on designing, developing, and optimizing data pipelines to support analytics and decision-making in the utilities industry.\n\nKey Responsibilities\n\u2022 Design and Build Data Pipelines: Develop scalable, reliable data pipelines using AWS services (e.g., Glue, S3, Redshift) to process and transform large datasets from utility systems like smart meters or energy grids.\n\u2022 Workflow Orchestration: Use AWS Step Functions to orchestrate workflows across data pipelines; experience with Airflow is acceptable but Step Functions is preferred.\n\u2022 Data Integration and Transformation: Implement ETL/ELT processes using PySpark, Python, and Pandas to clean, transform, and integrate data from multiple sources into unified datasets.\n\u2022 Distributed Systems Expertise: Leverage experience with complex distributed systems to ensure reliability, scalability, and performance in handling large-scale utility data.\n\u2022 Serverless Application Development: Use AWS Lambda functions to build serverless solutions for automating data processing tasks.\n\u2022 Data Modeling for Analytics: Design data models tailored for utilities use cases (e.g., energy consumption forecasting) to enable advanced analytics\n\u2022 Optimize Data Pipelines: Continuously monitor and improve the performance of data pipelines to reduce latency, enhance throughput, and ensure high availability.\n\u2022 Ensure Data Security and Compliance: Implement robust security measures to protect sensitive utility data and ensure compliance with industry regulations.\n\nRequired Qualifications\n\u2022 Minimum of 5 years of experience in data engineering\n\u2022 Proficiency in AWS services such as Step Functions, Lambda, Glue, S3, DynamoDB, and Redshift.\n\u2022 Strong programming skills in Python with experience using PySpark and Pandas for large-scale data processing.\n\u2022 Hands-on experience with distributed systems and scalable architectures.\n\u2022 Knowledge of ETL/ELT processes for integrating diverse datasets into centralized systems.\n\u2022 Familiarity with utilities-specific datasets (e.g., smart meters, energy grids) is highly desirable.\n\u2022 Strong analytical skills with the ability to work on unstructured datasets.\n\u2022 Knowledge of data governance practices to ensure accuracy, consistency, and security of data.\n\u2022 Strong experience in AWS data engineering\n\u2022 Ability to work independently\n\u2022 Ability to work with a cross-functional teams, including interfacing and communicating with business stakeholders\n\u2022 Professional oral and written communication skills\n\u2022 Strong problem solving and troubleshooting skills with experience exercising mature judgement\n\u2022 Excellent teamwork and interpersonal skills\n\u2022 Ability to obtain and maintain the required clearance for this role",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DbMI3tTg8CkVNRcoxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "Dt3-pBfk2JtUPqzdAAAAAA==",
    "job_title": "Data Engineer (Remote)",
    "employer_name": "Authority Brands",
    "employer_logo": null,
    "employer_website": "http://www.authoritybrands.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Authority-Brands/Job/Data-Engineer-(Remote)/-in-Columbia,MD?jid=4654298a59de84ef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Authority-Brands/Job/Data-Engineer-(Remote)/-in-Columbia,MD?jid=4654298a59de84ef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-remote-at-authority-brands-4266961170?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Experteer GmbH",
        "apply_link": "https://us.experteer.com/career/view-jobs/data-engineer-remote-columbia-md-usa-52992124?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/QizhCwUPqC5y2cREZFac5D84cSac8fO3fugLwCJXdQCkfBECS4Xrmg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Authority Brands Inc. headquartered in Columbia, MD is a leading provider of home services, building brands that support the success of our franchisees, as well as bettering the lives of the homeowners we serve and the people we employ.\n\nAbout Us: Authority Brands is a leading provider of home service franchises, dedicated to supporting franchise owners with elite marketing, advanced technology, and strong operational support. Founded in 2017, Authority Brands has grown to include 15 industry-leading home service franchisors, providing a wide range of services from plumbing and electrical work to lawn care and pest control.\n\nMission: Our mission is to empower franchise owners to succeed by offering best-in-class training, operational support, and marketing systems. We aim to deliver exceptional home services that customers trust and rely on.\n\nCulture: At Authority Brands, we foster a collaborative and supportive work environment where innovation and excellence are encouraged. We believe in the power of teamwork and are committed to helping our franchise owners achieve their personal and professional goals.\n\nJob Overview: The Data Analytics team at Authority Brands is seeking a skilled Data Engineer with at least 4 years of experience, strong SQL expertise, Python programming skills, and hands-on experience with AWS, including AWS Glue. You will collaborate with our data analytics and development teams to build, maintain, and optimize our data pipelines and infrastructure. We are looking for a candidate who enjoys solving complex data challenges and is eager to contribute to a dynamic team.\n\nKey Responsibilities:\n\u2022 Develop and optimize complex SQL queries to ensure efficient data retrieval, manipulation, and reporting.\n\u2022 Collaborate with data analysts, data scientists, and cross-functional teams to understand their data requirements and deliver solutions.\n\u2022 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.\n\u2022 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\n\u2022 Analyze and solve problems at their root, stepping back to understand the broader context in a timely manner.\n\u2022 Design, build, and maintain efficient and reliable data pipelines that support the company's data platform.\n\u2022 Leverage Python for ETL processes, automation scripts, and data pipeline development.\n\u2022 Utilize AWS services, including S3, RDS, Redshift, Lambda, and AWS Glue, to design and maintain cloud-based data infrastructure.\n\u2022 Work with AWS Glue for ETL transformations, data cataloging, and automation of data processes.\n\u2022 Monitor and improve the performance of data pipelines and data processing systems.\n\nQualifications:\n\u2022 Bachelor's degree in Computer Science, Engineering, or a related field.\n\u2022 Minimum of 4 years of experience in data engineering or a related role.\n\u2022 Strong proficiency in SQL for querying, optimizing, and managing large datasets.\n\u2022 Experience with data modeling and schema design.\n\u2022 Solid programming skills in Python, with experience in building and optimizing ETL pipelines.\n\u2022 Hands-on experience with AWS cloud services, including data-related tools such as S3, Redshift, RDS, Lambda, and AWS Glue.\n\u2022 Familiarity with version control (e.g., Git), CI/CD pipelines, and agile methodologies.\n\u2022 Experience working with large datasets and building scalable solutions.\n\u2022 Strong problem-solving skills and ability to work independently or within a team.\n\u2022 Excellent communication skills with the ability to explain technical concepts to non-technical stakeholders.\n\nWe believe our greatest assets are our employees, we offer competitive salaries and a full benefits package to include, PTO, paid holidays, 401(k) and more.\n\nAuthority Brands Inc. conducts drug screens and background checks on applicants who accept employment offers. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions however we do not sponsor Visa's at this time.\n\nAuthority Brands Inc. is an Equal Opportunity Employer",
    "job_is_remote": true,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Columbia, MD",
    "job_city": "Columbia",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.203714399999996,
    "job_longitude": -76.86104619999999,
    "job_benefits": [
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDt3-pBfk2JtUPqzdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Job Overview: The Data Analytics team at Authority Brands is seeking a skilled Data Engineer with at least 4 years of experience, strong SQL expertise, Python programming skills, and hands-on experience with AWS, including AWS Glue",
        "We are looking for a candidate who enjoys solving complex data challenges and is eager to contribute to a dynamic team",
        "Bachelor's degree in Computer Science, Engineering, or a related field",
        "Minimum of 4 years of experience in data engineering or a related role",
        "Strong proficiency in SQL for querying, optimizing, and managing large datasets",
        "Experience with data modeling and schema design",
        "Solid programming skills in Python, with experience in building and optimizing ETL pipelines",
        "Hands-on experience with AWS cloud services, including data-related tools such as S3, Redshift, RDS, Lambda, and AWS Glue",
        "Familiarity with version control (e.g., Git), CI/CD pipelines, and agile methodologies",
        "Experience working with large datasets and building scalable solutions",
        "Strong problem-solving skills and ability to work independently or within a team",
        "Excellent communication skills with the ability to explain technical concepts to non-technical stakeholders",
        "conducts drug screens and background checks on applicants who accept employment offers"
      ],
      "Benefits": [
        "We believe our greatest assets are our employees, we offer competitive salaries and a full benefits package to include, PTO, paid holidays, 401(k) and more"
      ],
      "Responsibilities": [
        "You will collaborate with our data analytics and development teams to build, maintain, and optimize our data pipelines and infrastructure",
        "Develop and optimize complex SQL queries to ensure efficient data retrieval, manipulation, and reporting",
        "Collaborate with data analysts, data scientists, and cross-functional teams to understand their data requirements and deliver solutions",
        "Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc",
        "to drive key business decisions",
        "Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation",
        "Analyze and solve problems at their root, stepping back to understand the broader context in a timely manner",
        "Design, build, and maintain efficient and reliable data pipelines that support the company's data platform",
        "Leverage Python for ETL processes, automation scripts, and data pipeline development",
        "Utilize AWS services, including S3, RDS, Redshift, Lambda, and AWS Glue, to design and maintain cloud-based data infrastructure",
        "Work with AWS Glue for ETL transformations, data cataloging, and automation of data processes",
        "Monitor and improve the performance of data pipelines and data processing systems"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "FWriH--HRVnwoB8JAAAAAA==",
    "job_title": "Data Engineer | Remote | Strong on AWS Glue/AWS Services |   GC-EAD & -EAD(Only Genuine) | W2 Position",
    "employer_name": "URSI Technologies Inc.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS2a9CliPK7OMiRF0tz_mpWj4sKVrMcEj0UMyWG&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/fa29282b-81e1-49d4-9386-188f2360713f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/fa29282b-81e1-49d4-9386-188f2360713f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Title: Data EngineerLocation: Remote RoleDuration: 6+ Months ContractVisa Status: GC-EAD & -EAD ONLY!! (Need Passport Copy f & Passport Number f-EAD & EAD) No H1's/OPT/CPT!! Note: This is a Remote role, but potential office for the quarterly planning meets it s in Pittsburgh or Philadelphia, PA. Roles and Responsibilities:Collaborate closely with cross-functional teams including Data Scientists, Analysts, and Software Engineers to understand data requirements and translate them into efficient solutions using AWS Glue and AWS services.Develop and maintain ETL processes using AWS Glue to facilitate seamless and reliable data extraction, transformation, and loading.Implement robust data security measures and access controls in alignment with company policies and industry best practices.Monitor, troubleshoot, and enhance data pipelines, identifying and resolving performance bottlenecks, data quality issues, and other challenges.Stay up-to-date with the latest advancements in AWS Glue and AWS services, and advocate for their effective utilization within the organization Experience/Minimum RequirementsProven experience 8+ years as a Data Engineer, with a strong emphasis on AWS Glue and AWS services.In-depth understanding of architecture, performance optimization techniques, and best practices.Proficiency in SQL and experience with database design principles.Hands-on expertise in designing, building, and maintaining complex ETL pipelines using and AWS Glue.Familiarity with data warehousing concepts and methodologies.Competence in cloud computing and AWS services, with a focus on data-related services such as S3, Redshift, and Lambda.Proficiency in scripting and programming languages such as Python, Java, or similar.",
    "job_is_remote": true,
    "job_posted_at": "13 hours ago",
    "job_posted_at_timestamp": 1753117200,
    "job_posted_at_datetime_utc": "2025-07-21T17:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DFWriH--HRVnwoB8JAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "VuP-kAKuKrvGJDw8AAAAAA==",
    "job_title": "Data Engineer III (Associate) (Remote)",
    "employer_name": "Jobright.ai",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRc11c46XEF7nUag98qauGWM3tBuQH8ELcz8Cbg&s=0",
    "employer_website": "https://jobright.ai",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-associate-remote-at-jobright-ai-4270055641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-associate-remote-at-jobright-ai-4270055641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs - Alooba",
        "apply_link": "https://jobs.alooba.com/us/job/everi-holdings-inc-data-engineer-iii-remote-842416/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Truemote",
        "apply_link": "https://truemote.com/remote-job/data-engineer-iii-associate-remote-b2435c5f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "USAJobCareer.com",
        "apply_link": "https://usajobcareer.com/jobs/view/data-engineer-iii-associate-remote-1672550.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Jobs",
        "apply_link": "https://store.joblagii.com/blogs/news/data-engineer-iii-remote-id-15864?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job_Summary:\n\nAstrana Health is seeking a highly motivated Data Engineer III to join their Data - Analytics department. The role involves collaborating with product teams and clinical leaders to manage and enhance data products, ensuring high-quality data models and addressing data quality issues.\n\nResponsibilities:\n\n\u2022 Use data engineering best practices to produce high quality, maximally available data models which are intuitive and trusted by stakeholders.\n\n\u2022 Scope and implement new entities for Astrana\u2019s unified data model.\n\n\u2022 Interfacing with business customers, gathering requirements and developing new datasets in data platform\n\n\u2022 Identifying the data quality issues to address them immediately to provide great user experience\n\n\u2022 Extracting and combining data from various heterogeneous data sources\n\n\u2022 Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets\n\n\u2022 Modelling data and metadata to support machine learning and AI\n\n\u2022 Support integration efforts from acquisitions as necessary.\n\nQualifications:\n\n-Required:\n\n\u2022 Bachelor's degree required in computer science, information technology, or related field\n\n\u2022 Strong understanding of database structures, theories, principles, and practices\n\n\u2022 Working knowledge with programming or scripting languages such as Python, Spark, and SQL\n\n\u2022 Demonstrated track record executing best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, deployment processes, testing, and operations\n\n\u2022 Familiarity with normalized, dimensional, star schema and snowflake schematic models\n\n\u2022 Healthcare domain and data experience\n\n\u2022 Strong written and oral communication skills\n\n\u2022 4+ years of experience working with diverse healthcare data sources, for example, claims, encounters, ADT data, FHIR, EHR data etc.\n\n\u2022 3+ years\u2019 using cloud-based services from AWS, GCP, or Azure\n\n\u2022 2+ years serving data sets to both BI tools and SE applications\n\n-Preferred:\n\n\u2022 Master\u2019s degree in healthcare-related field\n\n\u2022 Working experience with Databricks\n\n\u2022 Databricks/Microsoft Azure Certification is a plus\n\nCompany:\n\nLeading physician-centric, technology-powered, risk-bearing healthcare mgmt. company delivering high quality care in a cost-effective manner Astrana Health has a track record of offering H1B sponsorships.",
    "job_is_remote": true,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Alhambra, CA",
    "job_city": "Alhambra",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.092754899999996,
    "job_longitude": -118.1268211,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVuP-kAKuKrvGJDw8AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree required in computer science, information technology, or related field",
        "Strong understanding of database structures, theories, principles, and practices",
        "Working knowledge with programming or scripting languages such as Python, Spark, and SQL",
        "Demonstrated track record executing best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, deployment processes, testing, and operations",
        "Familiarity with normalized, dimensional, star schema and snowflake schematic models",
        "Healthcare domain and data experience",
        "Strong written and oral communication skills",
        "4+ years of experience working with diverse healthcare data sources, for example, claims, encounters, ADT data, FHIR, EHR data etc",
        "3+ years\u2019 using cloud-based services from AWS, GCP, or Azure",
        "2+ years serving data sets to both BI tools and SE applications",
        "Master\u2019s degree in healthcare-related field",
        "Working experience with Databricks"
      ],
      "Responsibilities": [
        "The role involves collaborating with product teams and clinical leaders to manage and enhance data products, ensuring high-quality data models and addressing data quality issues",
        "Use data engineering best practices to produce high quality, maximally available data models which are intuitive and trusted by stakeholders",
        "Scope and implement new entities for Astrana\u2019s unified data model",
        "Interfacing with business customers, gathering requirements and developing new datasets in data platform",
        "Identifying the data quality issues to address them immediately to provide great user experience",
        "Extracting and combining data from various heterogeneous data sources",
        "Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets",
        "Modelling data and metadata to support machine learning and AI",
        "Support integration efforts from acquisitions as necessary"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "00eCRolQcTBw_16xAAAAAA==",
    "job_title": "Data Engineer - Consultant (Remote)",
    "employer_name": "Releady",
    "employer_logo": null,
    "employer_website": "https://www.releady.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/consultant-data-engineer_93b50b06-e8ad-4459-ac66-68a2c9229a93?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Pangian",
        "apply_link": "https://pangian.com/remote/job/data-engineer-consultant-remote-1o?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EarnBetter",
        "apply_link": "https://earnbetter.com/app/job/01JM148F98JPMEKGYAMF1HY8HZ/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "OVERVIEW\n\nThis Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team. You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies. Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure.\n\u2022 Duration: 6+ months contract\n\u2022 Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada. Working hours will be PST. Preference for California.\n\u2022 Rate: $70/hr - $85/hr DOE\n\u2022 **Must be able to work in the United States without sponsorship***\n\nRESPONSIBILITIES\n\u2022 Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)\n\u2022 Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability\n\u2022 Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling\n\u2022 Work with Collibra for data governance, quality assurance, and metadata management\n\u2022 Leverage Refuel.ai for data mastering and Striim for data validation processes\n\u2022 Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows\n\u2022 Collaborate with client stakeholders to translate requirements into effective data solutions\n\u2022 Support data visualization and reporting needs through Tableau\n\u2022 Implement CI/CD practices using Git repositories and modern DevOps tools\n\u2022 Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners\n\nQUALIFICATIONS\n\u2022 Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)\n\u2022 10+ years of experience in data engineering or related roles\n\u2022 Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server\n\u2022 Hands-on experience with dbt Cloud for data transformation and pipeline development\n\u2022 Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse\n\u2022 Knowledge of Data Vault modeling principles and implementation techniques\n\u2022 Experience with data governance and data quality tools, particularly Collibra\n\u2022 Familiarity with data visualization platforms, especially Tableau\n\u2022 Understanding of version control systems (Git, Bitbucket) and CI/CD practices\n\u2022 Experience with scheduling systems like Tidal or Control-M\n\u2022 Working knowledge of Agile methodologies and DevOps principles applied to data pipelines\n\u2022 Preferred Skills:\n\u2022 Experience with data observability platforms and data quality monitoring\n\u2022 Knowledge of Python, R, KNIME, or Alteryx for data science applications\n\u2022 Experience with Refuel.ai and Striim technologies\n\u2022 Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms\n\u2022 Experience with enterprise scheduling tools like Tidal\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or other non-merit factor. We are committed to creating a diverse and inclusive environment for all employees.",
    "job_is_remote": true,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Sacramento, CA",
    "job_city": "Sacramento",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 38.5781342,
    "job_longitude": -121.4944209,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D00eCRolQcTBw_16xAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 70,
    "job_max_salary": 85,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "**Must be able to work in the United States without sponsorship***",
        "Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)",
        "10+ years of experience in data engineering or related roles",
        "Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server",
        "Hands-on experience with dbt Cloud for data transformation and pipeline development",
        "Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse",
        "Knowledge of Data Vault modeling principles and implementation techniques",
        "Experience with data governance and data quality tools, particularly Collibra",
        "Familiarity with data visualization platforms, especially Tableau",
        "Understanding of version control systems (Git, Bitbucket) and CI/CD practices",
        "Experience with scheduling systems like Tidal or Control-M",
        "Working knowledge of Agile methodologies and DevOps principles applied to data pipelines",
        "Experience with data observability platforms and data quality monitoring",
        "Knowledge of Python, R, KNIME, or Alteryx for data science applications",
        "Experience with Refuel.ai and Striim technologies",
        "Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms",
        "Experience with enterprise scheduling tools like Tidal"
      ],
      "Benefits": [
        "Rate: $70/hr - $85/hr DOE"
      ],
      "Responsibilities": [
        "This Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team",
        "You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies",
        "Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure",
        "Duration: 6+ months contract",
        "Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada",
        "Working hours will be PST. Preference for California",
        "Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)",
        "Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability",
        "Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling",
        "Work with Collibra for data governance, quality assurance, and metadata management",
        "Leverage Refuel.ai for data mastering and Striim for data validation processes",
        "Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows",
        "Collaborate with client stakeholders to translate requirements into effective data solutions",
        "Support data visualization and reporting needs through Tableau",
        "Implement CI/CD practices using Git repositories and modern DevOps tools",
        "Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "uINRzsWvsKKLdrOtAAAAAA==",
    "job_title": "URGENT by 6/25:REMOTE:Certified Data Engineer-Databricks, PySpark/Scala, ADFactory, W2 Only",
    "employer_name": "Solitsys",
    "employer_logo": null,
    "employer_website": "http://www.solitsys.com",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time and Contractor",
    "job_employment_types": [
      "FULLTIME",
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "URGENTLY NEEDED: REMOTE DATA ENGINEER- DATA BRICKS , PySpark/Scala, AZURE DATA FACTORY [W2 ONLY, ALL Corp-to-Corp WILL BE REJECTED]\n\nWe cannot offer Corp-to-Corp arrangement. This position is being offered on W2 basis only (no Corp-to-Corp or 1099), we are NOT a head-hunting agency. Please respond ASAP with your detailed resume in Word format. Resume must address the minimum qualifications listed below.\n\nDATA ENGINEER - DATABRICKS SPECIALIST\n\nAre you a skilled Data Engineer with a passion for modernizing data solutions? We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity. This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity. Time-sensitive, project-based with defined deliverables. Apply now!\n\nYOUR RESUME MUST SHOW THESE MINIMUM QUALIFICATIONS:\n\u2022 3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling.\n\u2022 5+ years background: PySpark or Scala for data engineering tasks.\n\u2022 5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning.\n\u2022 5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF).\n\u2022 3+ years of: Delta Lake architecture and implementation for data warehousing.\n\u2022 3+ years of experience with: BI tools such as Power BI or Tableau for reporting.\n\u2022 Familiarity with: Azure Cloud environment (Blob Storage, ADLS).\n\u2022 Proficiency in: Python scripting for data manipulation and automation.\n\nEducation & Certifications:\n\u2022 Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or related field.\n\u2022 Databricks Certified Data Engineer Associate or relevant industry certifications.\n\u2022 Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields.\n\nJob Types: Full-time, Contract\n\nPay: $60,000.00 per year\n\nCompensation Package:\n\u2022 Hourly pay\n\nSchedule:\n\u2022 8 hour shift\n\nEducation:\n\u2022 Bachelor's (Required)\n\nExperience:\n\u2022 Databricks: 3 years (Required)\n\u2022 Python: 5 years (Required)\n\u2022 PySpark: 5 years (Required)\n\u2022 Scala: 5 years (Required)\n\u2022 Azure Data Lake: 3 years (Required)\n\u2022 Azure Data Factory: 3 years (Required)\n\u2022 Power BI: 5 years (Preferred)\n\u2022 Tableau: 5 years (Preferred)\n\nWork Location: Remote",
    "job_is_remote": true,
    "job_posted_at": "27 days ago",
    "job_posted_at_timestamp": 1750809600,
    "job_posted_at_datetime_utc": "2025-06-25T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuINRzsWvsKKLdrOtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Resume must address the minimum qualifications listed below",
        "3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling",
        "5+ years background: PySpark or Scala for data engineering tasks",
        "5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning",
        "5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF)",
        "3+ years of: Delta Lake architecture and implementation for data warehousing",
        "3+ years of experience with: BI tools such as Power BI or Tableau for reporting",
        "Familiarity with: Azure Cloud environment (Blob Storage, ADLS)",
        "Proficiency in: Python scripting for data manipulation and automation",
        "Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or related field",
        "Databricks Certified Data Engineer Associate or relevant industry certifications",
        "Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields",
        "Bachelor's (Required)",
        "Databricks: 3 years (Required)",
        "Python: 5 years (Required)",
        "PySpark: 5 years (Required)",
        "Scala: 5 years (Required)",
        "Azure Data Lake: 3 years (Required)",
        "Azure Data Factory: 3 years (Required)"
      ],
      "Benefits": [
        "Pay: $60,000.00 per year",
        "Compensation Package:",
        "Hourly pay",
        "8 hour shift"
      ],
      "Responsibilities": [
        "We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity",
        "This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity",
        "Time-sensitive, project-based with defined deliverables"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "PrsQLoxA4Q6wEGlDAAAAAA==",
    "job_title": "Senior/Lead Data Engineer , Remote, $63/HR",
    "employer_name": "FASTRA LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4AyPlflVLMXHqcj8rlWeGkNh8rDl0qeg-V8eO&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Quality Contracts",
        "apply_link": "https://qualitycontracts.co.uk/contract-jobs/remote-senior-lead-data-engineer-remote-63-hr-in-usa-1752741389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior/Lead Data Engineer\n\nLocation: Remote\n\nDuration: 12 Months\n\nResponsibilities:\n\u2022 Collaborate with cross-functional stakeholders (including data scientists, analysts, and business teams) to define data requirements, processing needs, and analytics solutions.\n\u2022 Create and sustain scalable data models and efficient extraction processes.\n\u2022 Establish and monitor data quality checks and validation systems.\n\u2022 Construct and enhance business intelligence dashboards.\n\u2022 Produce comprehensive documentation for data models and processes.\n\u2022 Adapt to shifting priorities and ad-hoc requests.\n\u2022 Drive continuous improvement in data practices and visualization techniques.\n\nRequirements:\n\u2022 5+ years of proven experience in database engineering and software development.\n\u2022 Advanced skills in SQL for complex query development and database management.\n\u2022 Strong Python programming skills for automation and data processing workflows.\n\u2022 Ability to handle large-scale data processing tasks using Spark.\n\u2022 Knowledge of data visualization tools, Qlik is preferred.\n\u2022 Familiarity with cloud platforms, particularly AWS (Glue and Athena desired).\n\u2022 Growth mindset with enthusiasm to learn new technologies.",
    "job_is_remote": true,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPrsQLoxA4Q6wEGlDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "GsDGmxGZgv_H39E5AAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Nike",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTBuieX_9ud1sNvwsldw07_VlE1vE0QqXp1Insm&s=0",
    "employer_website": "http://www.nike.com/",
    "job_publisher": "Nike Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.nike.com/data-engineer/job/R-61137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Nike Careers",
        "apply_link": "https://careers.nike.com/data-engineer/job/R-61137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Cardinal Health",
        "apply_link": "https://jobs.cardinalhealth.com/search/jobdetails/data-engineer/610d0c2e-65a6-4123-82b5-db5a51021d33?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://pnc.wd5.myworkdayjobs.com/External/job/OH---Strongsville/Technology-Engineer---Data-and-Automation--Python--R--Unix-_R193407-1/apply?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Wd3.Myworkdaysite.com",
        "apply_link": "https://wd3.myworkdaysite.com/recruiting/magna/Magna/job/Lowell-Massachusetts-US/Data-Engineer_R00164181?source=BuiltInNationwide&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "StarSevenSix",
        "apply_link": "https://starsevensix.com/careers/data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "TigerData",
        "apply_link": "https://www.tigerdata.com/careers/29788682-de5c-45d8-a0b8-9603c833a8d8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Viasat Careers",
        "apply_link": "https://careers.viasat.com/jobs/4560?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Jobvite",
        "apply_link": "https://jobs.jobvite.com/uplight/job/o5ilwfwY?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Nike Inc.- Beaverton, OR. Design and build simple reusable components of larger process or framework to support analytics products with mentorship from experienced peers; design and implement product features in collaboration with Business and Technology partners;; clean, prepare and optimize data at scale for ingestion and consumption; support the implementation of new data management projects and re-structure of the current data architecture; implement automated workflows and routines using workflow scheduling tools; understand and use continuous integration, test driven development and production deployment frameworks; Anticipate, identify and solve issues concerning data management to improve data quality. participate in design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards; analyze and profile data for the purpose of designing scalable solutions; and solve straightforward data issues and perform root cause analysis to proactively resolve product issues. Telecommuting is available from anywhere in the U.S., except from AK, AL, AR, DE, HI, IA, ID, IN, KS, KY, LA, MT, ND, NE, NH, NM, NV, OH, OK, RI, SD, VT, WV, and WY.\n\nMust have a Master's degree in Computer Science, Engineering, Electronics Engineering and 2 years of experience in the job offered or in a data-related occupation. Experience must include;\n\n\u2022 System development life cycle\n\n\u2022 Cloud platforms, such as AWS and Databricks\n\n\u2022 SQL\n\n\u2022 Version control and CI/CD pipelines\n\n\u2022 Extracting, transforming, and loading and data pipelines\n\n\u2022 Scripting and automation\n\n\u2022 Big data technologies, such as Hadoop, and Spark\n\n\u2022 Data Warehouse concepts and methodologies\n\n\u2022 Relational and nonrelational database design\n\n\u2022 Programming languages, such as Python, Java, and Scala\n\nApply at www.Nike.com/Careers (Job #R-61137)\n\n#LI-DNI\n\nWe offer a number of accommodations to complete our interview process including screen readers, sign language interpreters, accessible and single location for in-person interviews, closed captioning, and other reasonable modifications as needed. If you discover, as you navigate our application process, that you need assistance or an accommodation due to a disability, please complete the Candidate Accommodation Request Form.",
    "job_is_remote": true,
    "job_posted_at": "18 days ago",
    "job_posted_at_timestamp": 1751587200,
    "job_posted_at_datetime_utc": "2025-07-04T00:00:00.000Z",
    "job_location": "Beaverton, OR",
    "job_city": "Beaverton",
    "job_state": "Oregon",
    "job_country": "US",
    "job_latitude": 45.486928299999995,
    "job_longitude": -122.80403199999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DGsDGmxGZgv_H39E5AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Must have a Master's degree in Computer Science, Engineering, Electronics Engineering and 2 years of experience in the job offered or in a data-related occupation",
        "System development life cycle",
        "Cloud platforms, such as AWS and Databricks",
        "SQL",
        "Version control and CI/CD pipelines",
        "Extracting, transforming, and loading and data pipelines",
        "Scripting and automation",
        "Big data technologies, such as Hadoop, and Spark",
        "Data Warehouse concepts and methodologies",
        "Relational and nonrelational database design",
        "Programming languages, such as Python, Java, and Scala"
      ],
      "Responsibilities": [
        "Design and build simple reusable components of larger process or framework to support analytics products with mentorship from experienced peers; design and implement product features in collaboration with Business and Technology partners;; clean, prepare and optimize data at scale for ingestion and consumption; support the implementation of new data management projects and re-structure of the current data architecture; implement automated workflows and routines using workflow scheduling tools; understand and use continuous integration, test driven development and production deployment frameworks; Anticipate, identify and solve issues concerning data management to improve data quality",
        "participate in design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards; analyze and profile data for the purpose of designing scalable solutions; and solve straightforward data issues and perform root cause analysis to proactively resolve product issues"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "6UkieosGdj3DK2hqAAAAAA==",
    "job_title": "Data Engineer - Growth Insights and Foundations",
    "employer_name": "Netflix",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQljIThV_aNQmMjwmu4kEPJEPGF3VYelRwpmpum&s=0",
    "employer_website": "https://www.netflix.com/",
    "job_publisher": "Remote Rocketship",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Remote Rocketship",
        "apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description:\n\u2022 Partner closely with data scientists and other engineers to build low-latency data products\n\u2022 Ensure the availability of critical data to enhance \"in-the-moment\" experiences\n\u2022 Develop highly available and reliable distributed data systems and services\n\u2022 Optimize for the best customer experience with data insights\n\u2022 Ensure timely delivery of high-quality data for Netflix product\n\nRequirements:\n\u2022 Proficient in at least one major language preferably on the JVM stack (e.g., Java, Scala) and SQL (any variant)\n\u2022 Strive to write elegant and maintainable code\n\u2022 Comfortable with picking up new technologies\n\u2022 Have a product mindset and are curious to understand the business's needs\n\u2022 Naturally collaborative style to work with product management, data science, engineering, etc.\n\u2022 Strong data intuition and know how to apply analytical skills to support building high quality data products\n\u2022 Experience building applications that use large-scale distributed systems and data processing frameworks (batch and real-time)\n\u2022 Passionate about making data available for self-service and wider integration\n\u2022 Knowledge about transport protocols and building APIs/services and frameworks (e.g. Spring, gRPC)\n\u2022 Experience in supporting and maintaining products that run 24x7\n\u2022 Can craft scalable systems and solutions to realize a range of product and engineering goals\n\u2022 Strong operational awareness and design multi-tenant systems handling high-scale demands\n\u2022 Prioritize observability in designs with comprehensive monitoring, logging, and alerting\n\u2022 Own what you build and have a passion for quality\n\u2022 Comfortable working in agile environments with vague requirements\n\u2022 Nimble and can pivot easily when needed\n\u2022 Unafraid to take smart risks\n\nBenefits:\n\u2022 Health Plans\n\u2022 Mental Health support\n\u2022 401(k) Retirement Plan with employer match\n\u2022 Stock Option Program\n\u2022 Disability Programs\n\u2022 Health Savings and Flexible Spending Accounts\n\u2022 Family-forming benefits\n\u2022 Life and Serious Injury Benefits\n\u2022 Paid leave of absence programs\n\u2022 Paid time off",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6UkieosGdj3DK2hqAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 170000,
    "job_max_salary": 720000,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "j0uGbWgrvId1fahdAAAAAA==",
    "job_title": "Data Engineer (Full-Time, Remote, North Carolina Based)",
    "employer_name": "Alliance Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRhPb_eu7pJl1gzkeLNRLOVuqLa0EIv62YtXd0S&s=0",
    "employer_website": "https://www.alliancehealthplan.org",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Alliance/Job/Data-Engineer-(Full-Time,-Remote,-North-Carolina-Based)/-in-Morrisville,NC?jid=63c088f9feb31195&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-full-time-remote-north-carolina-based-alliance-health-JV_KO0,51_KE52,67.htm?jl=1009695680692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-full-time-remote-north-carolina-based-at-alliance-health-4256300763?refId=ZZ8HCMbeJKzEURTwY97AHQ%3D%3D&trackingId=Ty0o5ZD5tUD13LeFLBMwKA%3D%3D&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/685c8729de1ebd49c8be30eb-data-engineer-full-time-remote-north-carolina-based?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5274727037?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://vividhireio.com/job/606346?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kinetichires",
        "apply_link": "https://kinetichires.net/job/data-analyst-diversity-equity-inclusion-and-health-equity-full-time-remote-north-carolina-based-264200?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages. The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities.\n\nThis position is fulltime remote. Selected candidate must reside in North Carolina. Some travel for onsite meetings to the Home office at Morrisville may be required.\n\nResponsibilities & Duties\n\u2022 Analyze business and technical requirements for the design of data integration solutions\n\u2022 Define the overall data integration and dataflow architectures to support data integration projects\n\u2022 Design and develop SQL and SSIS processes to support data integration projects\n\u2022 Design and develop APIs to consume and distribute healthcare data\n\u2022 Design, develop and execute unit testing plans\n\u2022 Ensure data quality and integrity in all data integration projects\n\u2022 Develop technical and business process documentation for data integration projects\n\u2022 Maintain and continually improve data integration projects\n\u2022 Assist in establishing standards for the design, development, implementation and support of data integration projects\n\u2022 Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders\n\u2022 Any other tasks as reasonably required\n\nMinimum Requirements\n\nEducation & Experience:\n\u2022 Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position.\n\u2022 Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis.\n\nPreferred\n\u2022 Bachelor\u2019s degree plus five (5) years of experience in a computer science related field including experience in a data integration or ETL development position including developing complex data integration software applications.\n\u2022 Microsoft Certified Solutions Expert, MuleSoft Certified Developer and/or HL7 Certifications.\n\nKnowledge, Skills, & Abilities\n\u2022 Expert programming in SQL\n\u2022 Proficient designing and developing ETL processes, preferably using SSIS\n\u2022 Proficient designing and developing APIs, preferably using .NET Framework\n\u2022 Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI\n\u2022 Experience working with API management and data integration platforms such as Apigee or MuleSoft\n\u2022 Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP\n\u2022 Experience working with HIEs and/or HISPs\n\u2022 Strong communication and organizational skills\n\u2022 Ability to access and analyze large data sets for completeness and quality\n\u2022 Ability to work independently and in a team setting\n\nEmployment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date.\n\nSalary Range\n\n$102,424-$130,591/Annually\n\nExact compensation will be determined based on the candidate's education, experience, external market data and consideration of internal equity.\n\nAn excellent fringe benefit package accompanies the salary, which includes:\n\u2022 Medical, Dental, Vision, Life, Long Term Disability\n\u2022 Generous retirement savings plan\n\u2022 Flexible work schedules including hybrid/remote options\n\u2022 Paid time off including vacation, sick leave, holiday, management leave\n\u2022 Dress flexibility\n\nEqual Opportunity Employer\nThis employer is required to notify all applicants of their rights pursuant to federal employment laws. For further information, please review the Know Your Rights notice from the Department of Labor.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "North Carolina",
    "job_city": null,
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.7595731,
    "job_longitude": -79.01929969999999,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dj0uGbWgrvId1fahdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 102424,
    "job_max_salary": 130591,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Selected candidate must reside in North Carolina",
        "Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position",
        "Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis",
        "Knowledge, Skills, & Abilities",
        "Expert programming in SQL",
        "Proficient designing and developing ETL processes, preferably using SSIS",
        "Proficient designing and developing APIs, preferably using .NET Framework",
        "Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI",
        "Experience working with API management and data integration platforms such as Apigee or MuleSoft",
        "Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP",
        "Experience working with HIEs and/or HISPs",
        "Strong communication and organizational skills",
        "Ability to access and analyze large data sets for completeness and quality",
        "Ability to work independently and in a team setting",
        "Employment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date"
      ],
      "Benefits": [
        "$102,424-$130,591/Annually",
        "An excellent fringe benefit package accompanies the salary, which includes:",
        "Medical, Dental, Vision, Life, Long Term Disability",
        "Generous retirement savings plan",
        "Flexible work schedules including hybrid/remote options",
        "Paid time off including vacation, sick leave, holiday, management leave",
        "Dress flexibility"
      ],
      "Responsibilities": [
        "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages",
        "The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities",
        "This position is fulltime remote",
        "Analyze business and technical requirements for the design of data integration solutions",
        "Define the overall data integration and dataflow architectures to support data integration projects",
        "Design and develop SQL and SSIS processes to support data integration projects",
        "Design and develop APIs to consume and distribute healthcare data",
        "Design, develop and execute unit testing plans",
        "Ensure data quality and integrity in all data integration projects",
        "Develop technical and business process documentation for data integration projects",
        "Maintain and continually improve data integration projects",
        "Assist in establishing standards for the design, development, implementation and support of data integration projects",
        "Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders",
        "Any other tasks as reasonably required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "7jGiM9T-GjtGDCWIAAAAAA==",
    "job_title": "Principal Data Engineer - Remote US",
    "employer_name": "Seamless.AI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTVw7NbcCd2ZQwIRbtYPEgUPiVfGPYawTZcHVKg&s=0",
    "employer_website": "https://seamless.ai",
    "job_publisher": "Wellfound",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://wellfound.com/jobs/3227170-principal-data-engineer-remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Wellfound",
        "apply_link": "https://wellfound.com/jobs/3227170-principal-data-engineer-remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Seamless.AI/Job/Principal-Data-Engineer-Remote-US/-in-Columbus,OH?jid=d298d1770fe754ee&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=a0a3d5a80823c1a4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/principal-data-engineer-remote-us/4275987?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67c03123b965ace1d468e64f-principal-data-engineer---remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Startup Jobs",
        "apply_link": "https://startup.jobs/principal-data-engineer-remote-us-seamlessai-2-6223376?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-us_04bbb179-c9c8-4960-851f-91caa0a4f220?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://jooble.org/jdp/4971728553806184117?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Opportunity:\n\nAt Seamless.AI, we\u2019re seeking a highly skilled and experienced Principal Data Engineer with expertise in Python, Spark, AWS Glue, and other ETL (Extract, Transform, Load) technologies. The ideal candidate will have a proven track record in data acquisition and transformation, as well as experience working with large data sets and applying methodologies for data matching and aggregation. Strong organizational skills and the ability to work independently as a self-starter are essential for this role.\n\nResponsibilities:\n\u2022 Design, develop, and maintain robust and scalable ETL pipelines to acquire, transform, and load data from various sources into our data ecosystem.\n\u2022 Collaborate with cross-functional teams to understand data requirements and develop efficient data acquisition and integration strategies.\n\u2022 Implement data transformation logic using Python and other relevant programming languages and frameworks.\n\u2022 Utilize AWS Glue or similar tools to create and manage ETL jobs, workflows, and data catalogs.\n\u2022 Optimize and tune ETL processes for improved performance and scalability, particularly with large data sets.\n\u2022 Apply methodologies and techniques for data matching, deduplication, and aggregation to ensure data accuracy and quality.\n\u2022 Implement and maintain data governance practices to ensure compliance, data security, and privacy.\n\u2022 Collaborate with the data engineering team to explore and adopt new technologies and tools that enhance the efficiency and effectiveness of data processing.\n\nSkillset:\n\u2022 Strong proficiency in Python and experience with related libraries and frameworks (e.g., pandas, NumPy, PySpark).\n\u2022 Hands-on experience with AWS Glue or similar ETL tools and technologies.\n\u2022 Solid understanding of data modeling, data warehousing, and data architecture principles.\n\u2022 Expertise in working with large data sets, data lakes, and distributed computing frameworks.\n\u2022 Experience developing and training machine learning models.\n\u2022 Strong proficiency in SQL.\n\u2022 Familiarity with data matching, deduplication, and aggregation methodologies.\n\u2022 Experience with data governance, data security, and privacy practices.\n\u2022 Strong problem-solving and analytical skills, with the ability to identify and resolve data-related issues.\n\u2022 Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.\n\u2022 Highly organized and self-motivated, with the ability to manage multiple projects and priorities simultaneously.\n\nEducation and Requirements:\n\u2022 Bachelor's degree in Computer Science, Information Systems, related fields or equivalent years of work experience.\n\u2022 7+ years of experience as a Data Engineer, with a focus on ETL processes and data integration.\n\u2022 Professional experience with Spark and AWS pipeline development required.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Columbus, OH",
    "job_city": "Columbus",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.9625112,
    "job_longitude": -83.00322179999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D7jGiM9T-GjtGDCWIAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate will have a proven track record in data acquisition and transformation, as well as experience working with large data sets and applying methodologies for data matching and aggregation",
        "Strong organizational skills and the ability to work independently as a self-starter are essential for this role",
        "Strong proficiency in Python and experience with related libraries and frameworks (e.g., pandas, NumPy, PySpark)",
        "Hands-on experience with AWS Glue or similar ETL tools and technologies",
        "Solid understanding of data modeling, data warehousing, and data architecture principles",
        "Expertise in working with large data sets, data lakes, and distributed computing frameworks",
        "Experience developing and training machine learning models",
        "Strong proficiency in SQL",
        "Familiarity with data matching, deduplication, and aggregation methodologies",
        "Experience with data governance, data security, and privacy practices",
        "Strong problem-solving and analytical skills, with the ability to identify and resolve data-related issues",
        "Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams",
        "Highly organized and self-motivated, with the ability to manage multiple projects and priorities simultaneously",
        "Bachelor's degree in Computer Science, Information Systems, related fields or equivalent years of work experience",
        "7+ years of experience as a Data Engineer, with a focus on ETL processes and data integration",
        "Professional experience with Spark and AWS pipeline development required"
      ],
      "Responsibilities": [
        "Design, develop, and maintain robust and scalable ETL pipelines to acquire, transform, and load data from various sources into our data ecosystem",
        "Collaborate with cross-functional teams to understand data requirements and develop efficient data acquisition and integration strategies",
        "Implement data transformation logic using Python and other relevant programming languages and frameworks",
        "Utilize AWS Glue or similar tools to create and manage ETL jobs, workflows, and data catalogs",
        "Optimize and tune ETL processes for improved performance and scalability, particularly with large data sets",
        "Apply methodologies and techniques for data matching, deduplication, and aggregation to ensure data accuracy and quality",
        "Implement and maintain data governance practices to ensure compliance, data security, and privacy",
        "Collaborate with the data engineering team to explore and adopt new technologies and tools that enhance the efficiency and effectiveness of data processing"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "VSDv3_uV9S8qF8byAAAAAA==",
    "job_title": "Google Cloud Platform Data Engineer(Fulltime) for Remote",
    "employer_name": "Amaze Systems Inc",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7sj9aM48iM-XElhKT8EMmjwhDrfGkFy-kqO6e&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Remote Jobs\u2122 - DentaQuest",
        "apply_link": "https://dev-cm.dentaquest.com/job/work-from-home-system-data-analyst-google-cloud-platform-data-fn8ov.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://jobxpedia.com/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://buzzcloud.in/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Fulltime opportunity\n\nRole: Google Cloud Platform Data Engineer\n\nLocation: Remote - USA\n\nDuration: FTE only\n\nJob Description:\n\u2022 10+ years' proven experience as a Data Engineer with a focus on Google Cloud Platform services.\n\u2022 Strong proficiency in Google Cloud Platform services such as GCS, Dataflow with Apache Beam (Batch & Stream data processing), BigQuery, cloud Composer and Pub/Sub.\n\u2022 Proficiency in SQL and Python for data manipulation and analysis is mandatory.\n\u2022 Solid understanding of data warehousing concepts and ETL processes.\n\nThanks &Regards\n\nRahul Sharma | Lead Technical Recruiter\nAmaze Systems Inc\n\nE: |",
    "job_is_remote": true,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1752883200,
    "job_posted_at_datetime_utc": "2025-07-19T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVSDv3_uV9S8qF8byAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "eCMK1KxoFvtFAEKKAAAAAA==",
    "job_title": "Data Engineer 5 - Playback",
    "employer_name": "Netflix",
    "employer_logo": null,
    "employer_website": "https://about.netflix.com",
    "job_publisher": "Careers At Netflix",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At Netflix",
        "apply_link": "https://explore.jobs.netflix.net/careers/job/790302425499-data-engineer-5-playback-usa-remote?domain=netflix.com&microsite=netflix.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://netflix.wd1.myworkdayjobs.com/netflix/job/usa---remote/data-engineer-5---playback_jr33081?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=949db8d2e2fdda48&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ShowbizJobs",
        "apply_link": "https://www.showbizjobs.com/jobs/netflix-data-engineer-games-in-los-gatos/jid-23obn2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-5-playback-netflix-JV_KO0,24_KE25,32.htm?jl=1009715302012&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/freelancer-data-engineer-developer-at-norconsulting-global-recruitment-4267468273?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6803ddfc9501a3a17fa1d905-data-engineer-5---playback?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Taro",
        "apply_link": "https://www.jointaro.com/jobs/netflix/data-engineer-5-playback/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\n\nWe are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.\n\nThe Playback pod within the Consumer Engineering Product Data Engineering team partners with client and edge systems to produce playback datasets, a source of truth for Title, Live and Ads performance, title popularity (Netflix top 10), and member quality of experience. This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP. It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network.\n\nThis role is focused on supporting Netflix\u2019s core datasets, delivering high-quality business metrics, and building systems to process batch and real-time data at a large scale. Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques. Since data engineers are responsible for their pipelines at Netflix, this role requires engineers to take ownership of the operational excellence in their domain. In addition, the ideal candidate will have excellent data intuition and share our passion for continuously improving how we handle streaming data at Netflix.\n\nWho are you?\n\u2022 You have strong product ownership and good intuition on how data is used to drive business decisions\n\u2022 You strive to write elegant code and are comfortable with independently picking up new technologies.\n\u2022 You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL.\n\u2022 You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models.\n\u2022 You have a strong background in at least one of the following: distributed data processing or software engineering of data services.\n\u2022 You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets.\n\u2022 You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation.\n\u2022 At Netflix, you own your code, services, and pipelines. You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain.\n\u2022 You have an eye for detail, good data intuition, and a passion for data quality.\n\u2022 You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.\n\u2022 You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks.\n\nWhat you will do:\n\u2022 Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences.\n\u2022 Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data\n\u2022 Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets.\n\u2022 Maintain and rethink existing pipelines to improve scalability and maintainability.\n\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\n\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\n\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\n\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\nJob is open for no less than 7 days and will be removed when the position is filled.",
    "job_is_remote": true,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1752364800,
    "job_posted_at_datetime_utc": "2025-07-13T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DeCMK1KxoFvtFAEKKAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Additionally, a candidate should have a rich understanding of large distributed systems, modern big data technologies, and software development techniques",
        "You have strong product ownership and good intuition on how data is used to drive business decisions",
        "You strive to write elegant code and are comfortable with independently picking up new technologies",
        "You have mastery over at least one major programming language (e.g., Java, Scala, Python) and SQL",
        "You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine-learning models",
        "You have a strong background in at least one of the following: distributed data processing or software engineering of data services",
        "You are familiar with big data technologies like Spark and Flink and comfortable working with web-scale datasets",
        "You are passionate about the end-to-end software development lifecycle, focusing on automation, testing, CI/CD, and documentation",
        "At Netflix, you own your code, services, and pipelines",
        "You have practical, solid DevOps and Operation fundamentals, and you enjoy total ownership of your domain",
        "You have an eye for detail, good data intuition, and a passion for data quality",
        "You relate to and embody many of the aspects of the Netflix Culture",
        "You love working independently while also collaborating and giving/receiving candid feedback",
        "You are comfortable working in a rapidly changing environment with ambiguous requirements",
        "You are nimble and take intelligent risks"
      ],
      "Benefits": [
        "Our compensation structure consists solely of an annual salary; we do not have bonuses",
        "You choose each year how much of your compensation you want in salary versus stock options",
        "To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range",
        "The range for this role is $170,000 - $720,000",
        "Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits",
        "We also offer paid leave of absence programs",
        "Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off",
        "Full-time salaried employees are immediately entitled to flexible time off"
      ],
      "Responsibilities": [
        "This team empowers Netflix with data to drive content decisions and understand where content is being watched, on what devices, in what quality, or through which ISP",
        "It enables analytics about client behavior, server choices, or decisions on what content should be placed in which parts of our content network",
        "Help Netflix scale new and existing business efforts by driving data excellence and quality, supporting analytics, developing and maintaining metrics, and partnering with engineering to deliver the next generation of Netflix experiences",
        "Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data",
        "Partner with numerous engineering teams, analytics engineers, and data scientists to enhance playback-related datasets",
        "Maintain and rethink existing pipelines to improve scalability and maintainability"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "RxeJao3yHoNlD18FAAAAAA==",
    "job_title": "Data Engineer - MLOps | Remote in the USA - 1725",
    "employer_name": "PlacingIT",
    "employer_logo": null,
    "employer_website": "https://www.placingit.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - MLOps | Remote in the USA - 1725\n\nLocation: Hybrid - Onsite 4 days a week in Dallas, TX\nEmployment Type 6-12+ months contract w/possible extensions\nInterview: In-Person interview is required for this role.\nResidency Requirements: Those authorized to work in the US are encouraged to apply.\n\nRequired Qualifications\n\u2022 5+ years of software development experience building and delivering customer facing cloud-based analytic solutions.\n\u2022 5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives.\n\u2022 5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope.\n\u2022 5+ years coding in Python/PySpark, SQL, and LLM modeling.\n\u2022 5+ years in cloud-based platforms such as Azure, GCP and AWS.\n\nPreferred Qualifications\n\u2022 5+ years of experience leading the design, development, and deployment of applications on Palantir Foundry Platform\n\u2022 Retail merchandising domain expertise including category management, assortment optimization, product clustering, product price sensitivity and promotion affinity evaluation, store clustering and localization.\n\u2022 Experience with Prompt Engineering developing solutions leveraging LLM models.\n\nResponsibilities include:\n\u2022 You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions.\n\u2022 You will solve complex problems and deliver decision support tools to improve customer experience.\n\u2022 Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses.\n\u2022 Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces.\n\u2022 Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses.\n\u2022 Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry.\n\u2022 Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle.\n\u2022 Provide technical guidance, mentorship, and support to junior developers and team members.\n\u2022 Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem.",
    "job_is_remote": true,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": "Dallas, TX",
    "job_city": "Dallas",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7766642,
    "job_longitude": -96.79698789999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRxeJao3yHoNlD18FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Residency Requirements: Those authorized to work in the US are encouraged to apply",
        "5+ years of software development experience building and delivering customer facing cloud-based analytic solutions",
        "5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives",
        "5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope",
        "5+ years coding in Python/PySpark, SQL, and LLM modeling",
        "5+ years in cloud-based platforms such as Azure, GCP and AWS"
      ],
      "Responsibilities": [
        "You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions",
        "You will solve complex problems and deliver decision support tools to improve customer experience",
        "Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses",
        "Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces",
        "Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses",
        "Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry",
        "Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle",
        "Provide technical guidance, mentorship, and support to junior developers and team members",
        "Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": "3KUvi58ta8OiVWqPAAAAAA==",
    "job_title": "W2 Contract || Title- Azure Data Engineer in 100% Remote || USC & GC only on W2 || 12 Years Exp.",
    "employer_name": "JS Consulting",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ5srw1SEg2aFYK1ozh4gD1IIysdoiTgxUfEKho&s=0",
    "employer_website": "https://www.jsconsultingco.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/w2-contract-title-azure-data-engineer-in-100%25-remote-usc-gc-only-on-w2-12-years-exp-at-js-consulting-4270436999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Note: Candidate must be Work on W2 and USC & GC Visa only.\n\nTitle: Azure Data Engineer (12+ Years Experinece)\n\nLocation: Kansas City, MO 100% Remote\n\nDuration: 6-12+ Months (W2 Contract)\n\nInterview: Video\n\nVisa: USC/GC (must work on our w2) (need strong communication)\n\nJob Description\n\nMake sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager.\n\nDon't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes.\n\nCommunication must be flawless\n\nResume should not be more than 6 pages\n\nMust have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020.\n\nJob Description-\n\nMust have strong Azure, ADF and Databricks experience.\n\nThe purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks. It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes. This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files.\n\nEssential Duties And Responsibilities\n\u2022 Work with a highly dynamic team focused on Digital Transformation.\n\u2022 Understand the domain and business processes to implement successful data pipelines.\n\u2022 Provide work status and coordinate with Data Engineers.\n\u2022 Manage customer deliverables and regularly report the status via Weekly/Monthly reviews.\n\u2022 Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views\n\u2022 Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server.\n\u2022 Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot.\n\u2022 Design normalized database tables with proper indexing and constraints.\n\u2022 Perform SQL query tuning and performance optimization on complex and inefficient queries.\n\u2022 Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets.\n\u2022 Collaborate with DBA on database design and performance enhancements.\n\u2022 Leading in all phases of the software development life cycle in a team environment.\n\u2022 Debug existing code and troubleshoot for issues.\n\u2022 Design and provide a framework for maintaining existing data warehouse for reporting and data analytics.\n\u2022 Follow best practices, design, develop, test and document ETL processes.",
    "job_is_remote": true,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Kansas City, MO",
    "job_city": "Kansas City",
    "job_state": "Missouri",
    "job_country": "US",
    "job_latitude": 39.099726499999996,
    "job_longitude": -94.5785667,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3KUvi58ta8OiVWqPAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Note: Candidate must be Work on W2 and USC & GC Visa only",
        "Title: Azure Data Engineer (12+ Years Experinece)",
        "Visa: USC/GC (must work on our w2) (need strong communication)",
        "Communication must be flawless",
        "Resume should not be more than 6 pages",
        "Must have valid LinkedIn profile with profile pic and and good number of connection and must be crreated before 2020",
        "Must have strong Azure, ADF and Databricks experience",
        "Program in T-SQL with relational databases including currently supported versions of Microsoft SQL Server"
      ],
      "Responsibilities": [
        "Make sure candidates are exceptionally communicating well while speaking with my business partner over the video call because he will be recoding the video and sharing to the hiring manager",
        "Don't send any profile those who are not comfortable to do video call with my business partner, it will be for 15-20 minutes",
        "The purpose of this position is to perform Data Development functions which include: the design of new or enhancement of existing enterprise database systems; maintenance and/or development of critical data processes; unit and system testing; support and help desk tasks",
        "It also requires defining and adopting best practices for each of the data development functions as well as visualization and ETL processes",
        "This position is also responsible for architecting ETL functions between a multitude of relational databases and external data files",
        "Work with a highly dynamic team focused on Digital Transformation",
        "Understand the domain and business processes to implement successful data pipelines",
        "Provide work status and coordinate with Data Engineers",
        "Manage customer deliverables and regularly report the status via Weekly/Monthly reviews",
        "Design, develop and maintain ETL processes as well as Stored Procedures, Functions and Views",
        "Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot",
        "Design normalized database tables with proper indexing and constraints",
        "Perform SQL query tuning and performance optimization on complex and inefficient queries",
        "Provide guidance in the use of table variable, temporary table, CTE appropriately to deal with large datasets",
        "Collaborate with DBA on database design and performance enhancements",
        "Leading in all phases of the software development life cycle in a team environment",
        "Debug existing code and troubleshoot for issues",
        "Design and provide a framework for maintaining existing data warehouse for reporting and data analytics",
        "Follow best practices, design, develop, test and document ETL processes"
      ]
    },
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "L2b_MhwdllUxOf0jAAAAAA==",
    "job_title": "Data Engineers (Hybrid | DC Area) Remote / Telecommute Jobs",
    "employer_name": "Rackner",
    "employer_logo": null,
    "employer_website": "https://rackner.com",
    "job_publisher": "Security Clearance Jobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Security Clearance Jobs",
        "apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Title: Data Engineer\n\nLocation: Falls Church, VA (Hybrid)\n\nClearance: Secret Clearance\n\nAbout this role:\n\nRackner is looking for a Data Engineer that will be working within an Agile DevSecOps team environment using latest cloud-native technologies to architect and implement containerized applications, CI/CD pipelines, and Kubernetes platforms using best practices and leading technologies.\n\nWe are seeking professionals with:\n\nB.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms\n\nModern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)\n\nDeveloping data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules\n\nMature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR\n\nNice to have:\n\nM.S in Computer Science or related\n\nAI/ML\n\nKubernetes (Rancher RKE2, AWS EKS) and microservice architectures\n\nData engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR\n\nNIST Risk Management Framework and security accreditation process and tasks\n\nWhat will make you successful:\n\nUsing DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer\n\nDeveloping software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes\n\nLeveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs\n\nApplying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders\n\nEmbracing a shared responsibility for system security\n\nPerforming threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle\n\nContinuously engaging with project teams to deliver quality products\n\nParticipating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions\n\nWho We Are:\n\nRackner is a software consultancy that builds cloud-native solutions for startups, enterprises, and the public sector.\n\nWe are an energetic, growing consultancy with a passion for solving big problems for both startups and enterprises.\n\nEach of us enable digital transformation for large organizations through the newest in distributed technologies as we are laser focused on end-to-end application development, DevSecOps, AI/ML and systems architecture and our methodology focuses on cloud-first and cost-effective innovation.\n\nOur customers hail from a diverse, ever-growing list of industries.\n\nBenefits/Additional Info:\n\nRackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided . Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!\n\n401K with 100% matching up to 6%\n\nHighly competitive PTO\n\nGreat health insurance with large network of providers\n\nMedical/Dental/Vision\n\nLife Insurance, and short & long term disability\n\nIndustry-Leading Weekly Pay Schedule\n\nHome office & equipment plan\n\n#DataEngineer #AWS #Topsecret #FDA #publictrust #DataIngesting #DataPipeline #Python #Terraform #ETL #AI #ML #dataintegration #bigdataanalyticspipeline #awsbigdata #hadoop #apachespark #RDBMS #awsdynamoDB #collaboration #diversity #equity #Inclusion",
    "job_is_remote": true,
    "job_posted_at": "10 hours ago",
    "job_posted_at_timestamp": 1753128000,
    "job_posted_at_datetime_utc": "2025-07-21T20:00:00.000Z",
    "job_location": "Falls Church, VA",
    "job_city": "Falls Church",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.882334,
    "job_longitude": -77.1710914,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DL2b_MhwdllUxOf0jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "B.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms",
        "Modern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)",
        "Developing data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules",
        "Mature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR",
        "M.S in Computer Science or related",
        "Kubernetes (Rancher RKE2, AWS EKS) and microservice architectures",
        "Data engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR",
        "NIST Risk Management Framework and security accreditation process and tasks"
      ],
      "Benefits": [
        "Rackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided ",
        "Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!",
        "401K with 100% matching up to 6%",
        "Highly competitive PTO",
        "Great health insurance with large network of providers",
        "Life Insurance, and short & long term disability",
        "Industry-Leading Weekly Pay Schedule",
        "Home office & equipment plan"
      ],
      "Responsibilities": [
        "Using DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer",
        "Developing software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes",
        "Leveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs",
        "Applying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders",
        "Embracing a shared responsibility for system security",
        "Performing threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle",
        "Continuously engaging with project teams to deliver quality products",
        "Participating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "SjsMVNnmi0YDQr4jAAAAAA==",
    "job_title": "Senior Data Engineer - Full remote",
    "employer_name": "All European Careers",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTfGAmX7q4Hokut7-XE9J8D2NsW-8elqZh_HaX4&s=0",
    "employer_website": "https://www.all-european-careers.com",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JOBITT",
        "apply_link": "https://jobitt.com/job-openings/external/senior-engineer-full-stack-7870835690680623803?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: Europe\n\nThis is a remote position.\n\nFor an International Institution in Geneva, we are urgently looking for an experienced Senior Data Engineer preferably with Azure experience in Geneva or Full remote.As Senior Data Engineer, you work with the Cloud Architect, Data Architect, Solution Engineer and other technical professionals to act as technical focal point for the development of prototypes, providing advice and technical feasibility view, and other technical activities to ensure the Cloud data Platform architecture and technical configuration will address business and IT objectives.\n\nCandidates need to be fluent in English. This positions is long-term. Work permit not required.Candidates need to be based in Europe.\n\nTasks and Responsibilities:\n\u2022 Plan the activites and concrete milestones to deliver the new Cloud Data Platform Architecture;\n\u2022 Lead the development and technical implementation aligned to the Data Platform Architecture;\n\u2022 Provide expertise and technical advice in the development of conceptual, logical and physical data models, in support of interoperability protocols, API design and management, cybersecurity aspects and business intelligence;\n\u2022 Create the data factory pipelines to ingest data, apply data transformations to curate data using databricks, and make data available for downstream applications (API) or reporting from SQL database, Cosmos DB or Synapse Analytics;\n\u2022 Automation and development lifecycle control by development of rbooks, fctions, devops projects, sync to repos, ARM templates and Azure Blueprints;\n\u2022 Working with the Senior Solution Engineer derstand and catalogue the current landscape of Data and systems;\n\u2022 Develop and maintain the to-be Cloud Data Platform with the required capabilities in conformance with security requirements, agreements-based, with user-driven configuration and through documented design and configuration;\n\u2022 Provide technical expertise regarding short terms solution options to leverage in the immediate future to meet urgent data related demands, with consideration of the organization\u2019s wider needs and the potential of deploying enterprise data solution platforms at the enterprise level;\n\u2022 Provide data insights and best practices, ensuring they are reflected in the development;\n\nProfile:\n\u2022 Bachelor or Master degree;\n\u2022 +5 years of relevant experience as Data Engineer;\n\u2022 Experience with modern Data technologies such as Business Intelligence, Analytics, AI, and Big Data;\n\u2022 Experience programming multiple languages, e.g. Phyton, R, Java, Scala, etc; Professional level vendor certifications, such as Microsoft, ITIL, and others;\n\u2022 Extensive experience with Microsoft Azure and other cloud technologies and interoperable solutions,experience as a data analyst, engineer and developer;\n\u2022 Demonstrated experience in the design, development, and implementation of various integrations between diverse infrastructure services, data models and architecture;\n\u2022 Demonstrated experience with the IT systems development life cycle (SDLC), as well as Agile/Scrum methodologies and ITIL processes;\n\u2022 Ability to conduct requirements gathering, interpret needs, and design solutions and manage expectations;\n\u2022 Professional experience in technical design and support of global, distributed corporate information systems;\n\u2022 Understanding state of Azure, Google and AWS components and reference architectures;\n\nExperience in designing for Bigdata/data warehousing/business intelligence/reporting solutions for various physical environments, both on-premises as well as in the Microsoft cloud, while influencing for the most efficient approach based on business requirements;\n\nExcellent written and spoken English;\n\nInterested: Please send your resume to:\nresume@all-european-careers.com\n\nThis offer from \"All European Careers\" has been enriched by Jobgether.com and got a 77% flex score.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DSjsMVNnmi0YDQr4jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "XswWukx0iLqtemRoAAAAAA==",
    "job_title": "Principal Data Engineer *Remote - Most states eligible*",
    "employer_name": "Providence",
    "employer_logo": null,
    "employer_website": "http://www.psjhealth.org/",
    "job_publisher": "Teal",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the position\n\nThe Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making. This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics. The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St. Joseph Health system, focusing on quality and value of care metrics. The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights.\n\nResponsibilities\n\u2022 Collect and analyze clinical data from various sources.\n,\n\u2022 Create visualizations and storytelling using data to communicate findings.\n,\n\u2022 Develop novel reporting platforms to address healthcare challenges.\n,\n\u2022 Collaborate with clinical and administrative leaders to evaluate new measures.\n,\n\u2022 Identify impactful findings in large clinical data stores.\n,\n\u2022 Design strong data visualizations to communicate key information across the organization.\n,\n\u2022 Support team members with experience in computational methods.\n,\n\u2022 Develop innovative methods for outcome measures in the whole person care model.\n\nRequirements\n\u2022 Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience.\n,\n\u2022 9 years of relevant data analysis experience, preferably in a biomedical setting.\n,\n\u2022 Experience with database query and analysis languages (e.g., SQL, R, SAS, Python).\n,\n\u2022 Proficiency in data visualization tools (e.g., Tableau, D3).\n\nNice-to-haves\n\u2022 Ph.D. in a related field or equivalent experience.\n,\n\u2022 Certification in an IT discipline, application, or tool upon hire.\n,\n\u2022 Lean certification or Green Belt, Black Belt upon hire.\n,\n\u2022 Certification in Data Science upon hire.\n,\n\u2022 4+ years of development experience in data exchange using FHIR.\n\nBenefits\n\u2022 401(k) retirement savings plan with employer matching.\n,\n\u2022 Health care benefits (medical, dental, vision).\n,\n\u2022 Life insurance.\n,\n\u2022 Disability insurance.\n,\n\u2022 Paid parental leave.\n,\n\u2022 Vacation and holiday time off.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Renton, WA",
    "job_city": "Renton",
    "job_state": "Washington",
    "job_country": "US",
    "job_latitude": 47.4796927,
    "job_longitude": -122.2079218,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXswWukx0iLqtemRoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience",
        "9 years of relevant data analysis experience, preferably in a biomedical setting",
        "Experience with database query and analysis languages (e.g., SQL, R, SAS, Python)",
        "Proficiency in data visualization tools (e.g., Tableau, D3)",
        "Ph.D. in a related field or equivalent experience",
        "Certification in an IT discipline, application, or tool upon hire",
        "Lean certification or Green Belt, Black Belt upon hire",
        "Certification in Data Science upon hire",
        "4+ years of development experience in data exchange using FHIR"
      ],
      "Benefits": [
        "401(k) retirement savings plan with employer matching",
        "Health care benefits (medical, dental, vision)",
        "Life insurance",
        "Disability insurance",
        "Paid parental leave",
        "Vacation and holiday time off"
      ],
      "Responsibilities": [
        "The Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making",
        "This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics",
        "The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St",
        "The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights",
        "Collect and analyze clinical data from various sources",
        "Create visualizations and storytelling using data to communicate findings",
        "Develop novel reporting platforms to address healthcare challenges",
        "Collaborate with clinical and administrative leaders to evaluate new measures",
        "Identify impactful findings in large clinical data stores",
        "Design strong data visualizations to communicate key information across the organization",
        "Support team members with experience in computational methods",
        "Develop innovative methods for outcome measures in the whole person care model"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "Dt3-pBfk2JtUPqzdAAAAAA==",
    "job_title": "Data Engineer (Remote)",
    "employer_name": "Authority Brands",
    "employer_logo": null,
    "employer_website": "http://www.authoritybrands.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Authority-Brands/Job/Data-Engineer-(Remote)/-in-Columbia,MD?jid=4654298a59de84ef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Authority-Brands/Job/Data-Engineer-(Remote)/-in-Columbia,MD?jid=4654298a59de84ef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-remote-at-authority-brands-4266961170?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Experteer GmbH",
        "apply_link": "https://us.experteer.com/career/view-jobs/data-engineer-remote-columbia-md-usa-52992124?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/QizhCwUPqC5y2cREZFac5D84cSac8fO3fugLwCJXdQCkfBECS4Xrmg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Authority Brands Inc. headquartered in Columbia, MD is a leading provider of home services, building brands that support the success of our franchisees, as well as bettering the lives of the homeowners we serve and the people we employ.\n\nAbout Us: Authority Brands is a leading provider of home service franchises, dedicated to supporting franchise owners with elite marketing, advanced technology, and strong operational support. Founded in 2017, Authority Brands has grown to include 15 industry-leading home service franchisors, providing a wide range of services from plumbing and electrical work to lawn care and pest control.\n\nMission: Our mission is to empower franchise owners to succeed by offering best-in-class training, operational support, and marketing systems. We aim to deliver exceptional home services that customers trust and rely on.\n\nCulture: At Authority Brands, we foster a collaborative and supportive work environment where innovation and excellence are encouraged. We believe in the power of teamwork and are committed to helping our franchise owners achieve their personal and professional goals.\n\nJob Overview: The Data Analytics team at Authority Brands is seeking a skilled Data Engineer with at least 4 years of experience, strong SQL expertise, Python programming skills, and hands-on experience with AWS, including AWS Glue. You will collaborate with our data analytics and development teams to build, maintain, and optimize our data pipelines and infrastructure. We are looking for a candidate who enjoys solving complex data challenges and is eager to contribute to a dynamic team.\n\nKey Responsibilities:\n\u2022 Develop and optimize complex SQL queries to ensure efficient data retrieval, manipulation, and reporting.\n\u2022 Collaborate with data analysts, data scientists, and cross-functional teams to understand their data requirements and deliver solutions.\n\u2022 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.\n\u2022 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\n\u2022 Analyze and solve problems at their root, stepping back to understand the broader context in a timely manner.\n\u2022 Design, build, and maintain efficient and reliable data pipelines that support the company's data platform.\n\u2022 Leverage Python for ETL processes, automation scripts, and data pipeline development.\n\u2022 Utilize AWS services, including S3, RDS, Redshift, Lambda, and AWS Glue, to design and maintain cloud-based data infrastructure.\n\u2022 Work with AWS Glue for ETL transformations, data cataloging, and automation of data processes.\n\u2022 Monitor and improve the performance of data pipelines and data processing systems.\n\nQualifications:\n\u2022 Bachelor's degree in Computer Science, Engineering, or a related field.\n\u2022 Minimum of 4 years of experience in data engineering or a related role.\n\u2022 Strong proficiency in SQL for querying, optimizing, and managing large datasets.\n\u2022 Experience with data modeling and schema design.\n\u2022 Solid programming skills in Python, with experience in building and optimizing ETL pipelines.\n\u2022 Hands-on experience with AWS cloud services, including data-related tools such as S3, Redshift, RDS, Lambda, and AWS Glue.\n\u2022 Familiarity with version control (e.g., Git), CI/CD pipelines, and agile methodologies.\n\u2022 Experience working with large datasets and building scalable solutions.\n\u2022 Strong problem-solving skills and ability to work independently or within a team.\n\u2022 Excellent communication skills with the ability to explain technical concepts to non-technical stakeholders.\n\nWe believe our greatest assets are our employees, we offer competitive salaries and a full benefits package to include, PTO, paid holidays, 401(k) and more.\n\nAuthority Brands Inc. conducts drug screens and background checks on applicants who accept employment offers. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions however we do not sponsor Visa's at this time.\n\nAuthority Brands Inc. is an Equal Opportunity Employer",
    "job_is_remote": true,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Columbia, MD",
    "job_city": "Columbia",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.203714399999996,
    "job_longitude": -76.86104619999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDt3-pBfk2JtUPqzdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Job Overview: The Data Analytics team at Authority Brands is seeking a skilled Data Engineer with at least 4 years of experience, strong SQL expertise, Python programming skills, and hands-on experience with AWS, including AWS Glue",
        "We are looking for a candidate who enjoys solving complex data challenges and is eager to contribute to a dynamic team",
        "Bachelor's degree in Computer Science, Engineering, or a related field",
        "Minimum of 4 years of experience in data engineering or a related role",
        "Strong proficiency in SQL for querying, optimizing, and managing large datasets",
        "Experience with data modeling and schema design",
        "Solid programming skills in Python, with experience in building and optimizing ETL pipelines",
        "Hands-on experience with AWS cloud services, including data-related tools such as S3, Redshift, RDS, Lambda, and AWS Glue",
        "Familiarity with version control (e.g., Git), CI/CD pipelines, and agile methodologies",
        "Experience working with large datasets and building scalable solutions",
        "Strong problem-solving skills and ability to work independently or within a team",
        "Excellent communication skills with the ability to explain technical concepts to non-technical stakeholders",
        "conducts drug screens and background checks on applicants who accept employment offers"
      ],
      "Benefits": [
        "We believe our greatest assets are our employees, we offer competitive salaries and a full benefits package to include, PTO, paid holidays, 401(k) and more"
      ],
      "Responsibilities": [
        "You will collaborate with our data analytics and development teams to build, maintain, and optimize our data pipelines and infrastructure",
        "Develop and optimize complex SQL queries to ensure efficient data retrieval, manipulation, and reporting",
        "Collaborate with data analysts, data scientists, and cross-functional teams to understand their data requirements and deliver solutions",
        "Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc",
        "to drive key business decisions",
        "Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation",
        "Analyze and solve problems at their root, stepping back to understand the broader context in a timely manner",
        "Design, build, and maintain efficient and reliable data pipelines that support the company's data platform",
        "Leverage Python for ETL processes, automation scripts, and data pipeline development",
        "Utilize AWS services, including S3, RDS, Redshift, Lambda, and AWS Glue, to design and maintain cloud-based data infrastructure",
        "Work with AWS Glue for ETL transformations, data cataloging, and automation of data processes",
        "Monitor and improve the performance of data pipelines and data processing systems"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "ebx6kTHusl1EdZhFAAAAAA==",
    "job_title": "Data Engineer II",
    "employer_name": "Astrana Health, Inc.",
    "employer_logo": null,
    "employer_website": "http://www.astranahealth.com/",
    "job_publisher": "Careers At - Astrana Health",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.astranahealth.com/postings/eff68530-94e4-421c-b7c2-6fb55c1c3bae?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At - Astrana Health",
        "apply_link": "https://careers.astranahealth.com/postings/eff68530-94e4-421c-b7c2-6fb55c1c3bae?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://hhmi.wd1.myworkdayjobs.com/en-US/External/job/Data-Engineer-II_R-3508?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Cotiviti | Careers Center - ICIMS",
        "apply_link": "https://careers-cotiviti.icims.com/jobs/15601/data-engineer-ai-ii/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=7991269a18c80541&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Careers Home - Confluent Health",
        "apply_link": "https://careers.goconfluent.com/corporate/jobs/35194?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "JobRxiv",
        "apply_link": "https://jobrxiv.org/job/data-engineer-ii/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In NYC",
        "apply_link": "https://www.builtinnyc.com/job/data-engineer-ii/6600699?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In Colorado",
        "apply_link": "https://www.builtincolorado.com/job/data-engineer-ii/6600699?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer II\n\nDepartment: Data - Analytics\n\nEmployment Type: Full Time\n\nLocation: 1668 S. Garfield Ave. 2nd Floor, Alhambra, CA 91801\n\nCompensation: $105,000 - $115,000 / year\n\nDescription\nWe are currently seeking a highly motivated Data Engineer II. This role will report to the Manager of Data Integration and work closely with data analysts, data engineers, data scientists, and clinical leaders to produce deliverables for internal and external clients. With over a million managed lives across the country and terabytes of data generated, our teams need to be continuously equipped with the tools and insights to drive strategy and innovation to further our core values of improving patient outcomes and empowering our providers.\n\nOur Values:\n\u2022 Put Patients First\n\u2022 Empower Entrepreneurial Provider and Care Teams\n\u2022 Operate with Integrity & Excellence\n\u2022 Be Innovative\n\u2022 Work As One Team\n\nWhat You'll Do\n\u2022 Use data engineering best practices to produce high quality, maximally available data models which are intuitive to data analysts and trusted by stakeholders\n\u2022 Develop deep domain knowledge in healthcare operations, tracking regulatory developments related to analytics products you maintain\n\u2022 Apply quality measures and other metrics to datasets originating from internal and external clients\n\u2022 Build scalable ELT pipelines and business intelligence dashboards as needed, embracing automation wherever possible\n\u2022 Implement data quality checks which proactively identify data issues and distributional shifts to ensure accuracy of downstream analytical products\n\nQualifications\n\u2022 Bachelor's degree required in healthcare, analytics, statistics, finance, business, or related field; Master\u2019s degree (MBA, MPH) preferred.\n\u2022 Experience with relational databases.\n\u2022 Strong understanding of database structures, theories, principles, and practices\n\u2022 Working knowledge with programming or scripting languages such as Python, Spark, and SQL.\n\u2022 Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, build processes, testing, and operations.\n\u2022 Familiarity with normalized, dimensional, star schema and snowflake schematic models\n\u2022 Working experience with Databricks preferred\n\u2022 Familiarity with business intelligence exploratory or visualization tools (e.g., Tableau, PowerBI.) preferred\n\u2022 Strong written and oral communication skills.\n\u2022 Experience with Excel.\n\nYou're a great for this role if:\n\u2022 2+ years of experience working in the data and analytics landscape\n\u2022 2+ years of experience using version control to manage code changes\n\u2022 2+ years of experience in managed care or other healthcare data field preferred\n\u2022 1+ years\u2019 using cloud-based services from AWS, GCP, or Azure\n\nEnvironmental Job Requirements and Working Conditions\n\u2022 This position is remotely based in the U.S.\n\u2022 The total compensation target pay range for this role is: $105,000 - $115,000. The salary range represents our national target range for this role.\n\nAstrana Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. All employment is decided based on qualifications, merit, and business need. If you require assistance in applying for open positions due to a disability, please email us at humanresourcesdept@astranahealth.com to request an accommodation.\n\nAdditional Information:\nThe job description does not constitute an employment agreement between the employer and employee and is subject to change by the employer as the needs of the employer and requirements of the job change.\n\n#LI-remote",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Alhambra, CA",
    "job_city": "Alhambra",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.092754899999996,
    "job_longitude": -118.1268211,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Debx6kTHusl1EdZhFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 105000,
    "job_max_salary": 115000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Experience with relational databases",
        "Strong understanding of database structures, theories, principles, and practices",
        "Working knowledge with programming or scripting languages such as Python, Spark, and SQL",
        "Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, build processes, testing, and operations",
        "Familiarity with normalized, dimensional, star schema and snowflake schematic models",
        "Strong written and oral communication skills",
        "Experience with Excel",
        "2+ years of experience working in the data and analytics landscape",
        "2+ years of experience using version control to manage code changes",
        "1+ years\u2019 using cloud-based services from AWS, GCP, or Azure",
        "This position is remotely based in the U.S"
      ],
      "Benefits": [
        "Compensation: $105,000 - $115,000 / year",
        "The total compensation target pay range for this role is: $105,000 - $115,000"
      ],
      "Responsibilities": [
        "This role will report to the Manager of Data Integration and work closely with data analysts, data engineers, data scientists, and clinical leaders to produce deliverables for internal and external clients",
        "Empower Entrepreneurial Provider and Care Teams",
        "Operate with Integrity & Excellence",
        "Use data engineering best practices to produce high quality, maximally available data models which are intuitive to data analysts and trusted by stakeholders",
        "Develop deep domain knowledge in healthcare operations, tracking regulatory developments related to analytics products you maintain",
        "Apply quality measures and other metrics to datasets originating from internal and external clients",
        "Build scalable ELT pipelines and business intelligence dashboards as needed, embracing automation wherever possible",
        "Implement data quality checks which proactively identify data issues and distributional shifts to ensure accuracy of downstream analytical products"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "3Xcxv18vCFCCxy4SAAAAAA==",
    "job_title": "Senior Data Engineer",
    "employer_name": "Manulife",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4iueApMEAewotOpfPPrFvFNYispIyQUmxDdSW&s=0",
    "employer_website": "https://www.manulife.com",
    "job_publisher": "Manulife Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.manulife.com/global/en/job/JR25011423/Senior-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Manulife Careers",
        "apply_link": "https://careers.manulife.com/global/en/job/JR25011423/Senior-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/senior-data-engineer/4834642?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Philippines",
        "apply_link": "https://ph.linkedin.com/jobs/view/senior-data-engineer-at-manulife-4265089535?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "DEjobs.org",
        "apply_link": "https://dejobs.org/quezon-city-phl/senior-data-engineer/0049DDD1BD044C388C36AB75E6E8D578/job/?vs=28&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://ph.jooble.org/jdp/-3266237644944307244?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/philippines/job/full-time/others/senior-data-engineer-139271256?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs Trabajo.org",
        "apply_link": "https://ph.trabajo.org/job-3165-8815421647ebd7c5701dfb4e0f347a4b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/senior-data-engineer-quezon-city-national-capital-region-ph-manulife-jr25041427?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Senior Data Engineer position at Manulife presents a remarkable opportunity to significantly contribute to our goal of becoming a customer-centric digital leader. This role is pivotal in designing, building, and maintaining innovative data solutions that align with our strategic priorities, ensuring effective data collection, processing, and analysis. By joining our team, you will work with cutting-edge technologies, collaborate with a global network of industry experts, and benefit from comprehensive training resources within a supportive and inclusive environment.\n\nPosition Responsibilities:\n\u2022 Provide hands-on data engineering and technical oversight in developing batch and real-time data pipelines, DevOps pipelines, and end-to-end processes.\n\u2022 Lead the agile software development process, including roadmap planning, cross-team coordination, and task assignments.\n\u2022 Translate business requirements into technical solutions, collaborating closely with development teams, architects, and business analysts.\n\u2022 Design and build modern data pipelines, data streams, and data service APIs using Azure cloud data technologies.\n\u2022 Mentor other staff members and contribute to knowledge transfer across teams and departments.\n\u2022 Work Arrangement: Hybrid Work Set-Up\n\u2022 Schedule: Rotating shift\n\nRequired Qualifications:\n\u2022 At least 5-6 years of professional experience in designing and developing data warehouses and ETL tools and technologies.\n\u2022 Strong experience as an Azure Data Engineer, with expertise in Azure Databricks.\n\u2022 Bachelor's degree in a relevant field.\n\u2022 No licenses required.\n\nPreferred Qualifications:\n\u2022 Expert understanding of Azure Data Lake, Azure SQL databases, Azure Data Factory, and Azure Databricks.\n\u2022 Hands-on experience in designing and developing scripts for ETL processes and automation with Azure Data Factory, Azure Databricks, and PySpark.\n\u2022 Experience with the Hadoop ecosystem and toolset, including HDFS, MapReduce, Spark, Python, Scala, Hive, and Oozie.\n\u2022 Experience with CI/CD tools, such as GitHub, Jenkins, and Azure DevOps.\n\u2022 Knowledge of BI tools, such as PowerBI.\n\u2022 Strong interpersonal, written, and oral communication skills.\n\u2022 Ability to adapt to new service needs and a rapidly changing environment.\n\nWhen you join our team:\n\u2022 We\u2019ll empower you to learn and grow the career you want.\n\u2022 We\u2019ll recognize and support you in a flexible environment where well-being and inclusion are more than just words.\n\u2022 As part of our global team, we\u2019ll support you in shaping the future you want to see.\n\nAbout Manulife and John Hancock\n\nManulife Financial Corporation is a leading international financial services provider, helping people make their decisions easier and lives better. To learn more about us, visit https://www.manulife.com/en/about/our-story.html.\n\nManulife is an Equal Opportunity Employer\n\nAt Manulife/John Hancock, we embrace our diversity. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention, advancement and compensation, and we administer all of our practices and programs without discrimination on the basis of race, ancestry, place of origin, colour, ethnic origin, citizenship, religion or religious beliefs, creed, sex (including pregnancy and pregnancy-related conditions), sexual orientation, genetic characteristics, veteran status, gender identity, gender expression, age, marital status, family status, disability, or any other ground protected by applicable law.\n\nIt is our priority to remove barriers to provide equal access to employment. A Human Resources representative will work with applicants who request a reasonable accommodation during the application process. All information shared during the accommodation request process will be stored and used in a manner that is consistent with applicable laws and Manulife/John Hancock policies. To request a reasonable accommodation in the application process, contact recruitment@manulife.com.\n\nWorking Arrangement\n\nHybrid",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1752019200,
    "job_posted_at_datetime_utc": "2025-07-09T00:00:00.000Z",
    "job_location": "Cebu City, Cebu, Philippines",
    "job_city": "Cebu City",
    "job_state": "Cebu",
    "job_country": "PH",
    "job_latitude": 10.2926115,
    "job_longitude": 123.90219339999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3Xcxv18vCFCCxy4SAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "ev9B9WLM3AqUvc4XAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "ING",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "ING's Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.ing.com/en/job/city-of-taguig/data-engineer/3121/26099673984?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ING's Careers",
        "apply_link": "https://careers.ing.com/en/job/city-of-taguig/data-engineer/3121/26099673984?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs At Atos",
        "apply_link": "https://jobs.atos.net/job/Taguig-City-DATA-ENGINEER/1223031701/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=d86f775779e743db&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Focus Global Inc",
        "apply_link": "https://focusglobalinc.com/jobs/data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kalibrr",
        "apply_link": "https://www.kalibrr.com/c/yondu-inc/jobs/255869/data-engineer-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85317301?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-dapl-it-services-JV_IC2340354_KO0,13_KE14,30.htm?jl=1009695476276&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Jobs By Workable",
        "apply_link": "https://apply.workable.com/octal-philippines-inc-4/j/E2D6A08BFE?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "The Data Engineer we are currently looking for is someone who:\n\n\u00b7 Has more than 4 years\u2019 experience within ETL IBM DataStage tool.\n\n\u00b7 Has technical knowledge in Databases, ETL Processes & BI Reporting.\n\n\u00b7 Is an expert in Unix scripting, PL/SQL and Data Modelling for Informational platforms.\n\n\u00b7 Has knowledge & experience in Azure, and other cloud environments.\n\n\u00b7 Has good analytical skills, with proven domain experience in data warehousing within Bank and Finance Industry.\n\n\u00b7 Has working experience in Agile environments.\n\n\u00b7 Has clear communications skills as well as a high English level (written and oral).\n\n\u00b7 Is able to work without instructions and to take initiative.\n\n\u00b7 Makes things happen thanks to his/her \u2018can do\u2019 attitude.\n\n\u00b7 Has a problem solving mindset.\n\n\u00b7 Is a team player (daily connection with other departments is expected)\n\n\u00b7 Has the ability to work on an international level.\n\n\u00b7 Takes the initiative for improving processes, ensuring creative ideas are implemented.\n\n\u00b7 Is a leader of our Orange Code culture, displaying a positive attitude, commitment towards our goals, accountable, eager to learn and is not afraid of change.\n\nConsidering the software you may use as a Data Software Engineer, specific experience would be desirable for the following:\n\n\u00b7 Oracle (19c).\n\n\u00b7 IBM Infosphere Information Server Suite or similar Data Processing tools\n\n\u00b7 IBM Cognos, Power BI and/or MicroStrategy or similar BI tools\n\n\u00b7 Azure Devops or other devops cloud service\n\n\u00b7 Security, monitoring and logging skills (e.g. ELK, Prometheus, Grafana)\n\n\u00b7 Python\n\n\u00b7 Elastic Cloud Storage (ECS)",
    "job_is_remote": false,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Taguig, Metro Manila, Philippines",
    "job_city": "Taguig",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.530631699999999,
    "job_longitude": 121.0575482,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dev9B9WLM3AqUvc4XAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "2fW_NLNaKzL_7uOnAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Michael Page Philippines",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTZaw-H9AIEnHw8N4L_6YEs9LaETT5ZwSsq4xCa&s=0",
    "employer_website": "https://www.michaelpage.com.ph",
    "job_publisher": "Michael Page Philippines",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.michaelpage.com.ph/job-detail/data-engineer/ref/jn-112024-6593349?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Michael Page Philippines",
        "apply_link": "https://www.michaelpage.com.ph/job-detail/data-engineer/ref/jn-112024-6593349?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://worldvision.wd1.myworkdayjobs.com/en-US/WorldVisionInternational/job/Philippines---Home-Working/Data-Engineer_JR43497?jobFamilyGroup=9953ea81b11801011a0d47c420530000&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=4f4ae0a5337acf31&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Asia Select Inc.",
        "apply_link": "https://careers.asiaselect.ph/job/44264?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-iscale-solutions-inc-JV_KO0,13_KE14,34.htm?jl=1009782390416&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/data-engineer/4427293?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Sourcefit",
        "apply_link": "https://sourcefit.breezy.hr/p/92d5ab1d1a87-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/674dda59b10568bbc7b65291-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "\u2022 Seeking of a stimulating workplace that fosters growth and innovation\n\u2022 Global training opportunities\n\nAbout Our Client\n\nThe client is a Fortune 500 energy company that has a strong presence in the Philippines as one of the country's top employers, recognized for their commitment to diversity, inclusion, and employee development\n\nJob Description\n\u2022 Design and develop ETLprocessesETLprocesses using Azure services (Data Factory, Synapse, DatabricksDatabricks, Fabric)\n\u2022 Manage data storage solutions using Azure Data Lake Storage Gen 2 & Blob storage\n\u2022 OptimizeOptimize data pipelines for performance, scalabilityscalability, and reliability\n\u2022 Implement data validation techniques and quality frameworks\n\u2022 Manage CI/CD processes for data solutions\n\u2022 Collaborate with data scientists, analysts, and architects\n\u2022 Create and maintain technical documentation\n\u2022 Monitor and troubleshoot pipeline issues\n\u2022 Stay current with emerging technologies and industry trends\n\nThe Successful Applicant\n\nMust-have:\n\u2022 Bachelor's degree in Computer Science, Engineering, or related field\n\u2022 3+ years experience as a Data Engineer\n\u2022 Strong expertise in Microsoft Azure services\n\u2022 Proficient in SQL, DML and modern RDBMS\n\u2022 RDBMS Experience with software engineering principles (CI/CD, version control)\n\u2022 Knowledge of big data technologies (e.g., Spark)\n\u2022 Strong problem-solving skills\n\nPreferred:\n\u2022 Python programming skills (or Scala,Scala Java, C#)\n\u2022 Experience with PySpark and file formats (Parquet, Delta, Avro)\n\u2022 Knowledge of Azure DevOps pipelines and Git workflows\n\u2022 workflowsUnderstanding of Ansible\n\u2022 AnsibleAPI integration experience\n\u2022 Learning agility and technical leadership capabilities\n\nWhat's on Offer\n\u2022 Competitive salary package\n\u2022 Hybrid work model with work-from-home flexibility\n\u2022 Comprehensive health care coverage (including dependents)\n\u2022 Life insurance\n\u2022 Career development opportunities through training and mentoring\n\u2022 Opportunity to work with cutting-edge technologies",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D2fW_NLNaKzL_7uOnAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "6tnO5hV6ICVJHf55AAAAAA==",
    "job_title": "Data Engineer (Azure & On Prem)",
    "employer_name": "Lingaro",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTce2f8T15c6tYWtGKmrtnm4wDmdhdIxc48r9nR&s=0",
    "employer_website": "https://lingarogroup.com",
    "job_publisher": "LinkedIn Philippines",
    "job_employment_type": null,
    "job_employment_types": [],
    "job_apply_link": "https://ph.linkedin.com/jobs/view/data-engineer-azure-on-prem-at-lingaro-4270215602?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Philippines",
        "apply_link": "https://ph.linkedin.com/jobs/view/data-engineer-azure-on-prem-at-lingaro-4270215602?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Task\n\u2022 Designing and implementing data processing systems using distributed frameworks like Hadoop, Spark, Snowflake, Airflow, or other similar technologies. This involves writing efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data.\n\u2022 Building data pipelines to ingest data from various sources such as databases, APIs, or streaming platforms. Integrating and transforming data to ensure its compatibility with the target data model or format.\n\u2022 Designing and optimizing data storage architectures, including data lakes, data warehouses, or distributed file systems. Implementing techniques like partitioning, compression, or indexing to optimize data storage and retrieval. Identifying and resolving bottlenecks, tuning queries, and implementing caching strategies to enhance data retrieval speed and overall system efficiency.\n\u2022 Designing and implementing data models that support efficient data storage, retrieval, and analysis. Collaborating with data scientists and analysts to understand their requirements and provide them with well-structured and optimized data for analysis and modeling purposes.\n\u2022 Utilizing frameworks like Hadoop or Spark to perform distributed computing tasks, such as parallel processing, distributed data processing, or machine learning algorithms\n\u2022 Implementing security measures to protect sensitive data and ensuring compliance with data privacy regulations. Establishing data governance practices to maintain data integrity, quality, and consistency.\n\u2022 Identifying and resolving issues related to data processing, storage, or infrastructure. Monitoring system performance, identifying anomalies, and conducting root cause analysis to ensure smooth and uninterrupted data operations.\n\u2022 Collaborating with cross-functional teams including data scientists, analysts, and business stakeholders to understand their requirements and provide technical solutions. Communicating complex technical concepts to non-technical stakeholders in a clear and concise manner.\n\u2022 Independence and responsibility for delivering a solution\n\u2022 Ability to work under Agile and Scrum development methodologies\n\u2022 Staying updated with emerging technologies, tools, and techniques in the field of big data engineering. Exploring and recommending new technologies to enhance data processing, storage, and analysis capabilities.\n\u2022 Train and mentor junior data engineers, providing guidance and knowledge transfer.\n\nRequirements:\n\u2022 A bachelor's or master's degree in Computer Science, Information Systems, or a related field is typically required.\n\u2022 Work commercial experience as a Data Engineer or a similar role.\n\u2022 Proficiency in programming languages such as Python, R or Scala is essential.\n\u2022 In-depth knowledge and experience with distributed systems and technologies, including On-prem Platforms, Apache Hadoop, Spark, Hive or similar frameworks. Familiarity with cloud-based platforms like AWS, Azure, or Google Cloud is highly desirable.\n\u2022 Solid understanding of data processing techniques such as batch processing, real-time streaming, and data integration. Experience with data analytics tools and frameworks like Apache Kafka, Apache Flink, or Apache Storm is a plus.\n\u2022 Proficiency in working with relational and non-relational databases such as MSSQL, MySQL, PostgreSQL or Cassandra. Knowledge of data warehousing concepts and technologies like Redshift, Snowflake, or BigQuery is beneficial.\n\u2022 Good knowledge of data storage architectures, including delta lakes, data warehouses, or distributed file systems\n\u2022 Experience in designing and building data pipelines (ELT/ETL) for large-scale datasets. Familiarity with tools like Databricks, Apache Nifi, Apache Airflow, or Informatica is advantageous. Experience with integration of data from multiple data sources.\n\u2022 Nice to have knowledge of data warehousing concepts and technologies like Synapse, Redshift, Snowflake, or BigQuery. Experience with MS Fabric is a plus.\n\u2022 Strong understanding of distributed computing principles, including parallel processing, data partitioning, and fault-tolerance.\n\u2022 Proficient in data modeling techniques and database optimization. Knowledge of query optimization, indexing, and performance tuning is necessary for efficient data retrieval and processing.\n\u2022 Understanding of data security best practices and experience implementing data governance policies. Familiarity with data privacy regulations and compliance standards is a plus.\n\u2022 Strong problem-solving abilities to identify and resolve issues related to data processing, storage, or infrastructure. Analytical mindset to analyze and interpret complex datasets for meaningful insights.\n\u2022 Knowledge of data orchestration tool will be beneficial\n\u2022 Experience in designing and creating integration and unit test will be beneficial.\n\u2022 Excellent communication skills to effectively collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders. Ability to convey technical concepts to non-technical stakeholders in a clear and concise manner.\n\u2022 A passion for staying updated with emerging technologies and industry trends in the field of big data engineering. Willingness to learn and adapt to new tools and techniques to enhance data processing, storage, and analysis capabilities.\n\u2022 Additional certifications in big data technologies or cloud platforms are advantageous\n\nOffer\n\u2022 Stable employment. On the market since 2008, 1200+ talents currently on board in 7 global sites.\n\u2022 \u201cOffice as an option\u201d model. You can choose to work remotely or in the office.\n\u2022 Flexibility regarding working hours and your preferred form of contract.\n\u2022 Comprehensive online onboarding program with a \u201cBuddy\u201d from day 1.\n\u2022 Cooperation with top-tier engineers and experts.\n\u2022 Certificate training programs. Lingarians earn 500+ technology certificates yearly.\n\u2022 Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.\n\u2022 Grow as we grow as a company. 76% of our managers are internal promotions.\n\u2022 A diverse, inclusive, and values-driven community.\n\u2022 Autonomy to choose the way you work. We trust your ideas.\n\u2022 Create our community together. Refer your friends to receive bonuses.\n\u2022 Activities to support your well-being and health.\n\u2022 Plenty of opportunities to donate to charities and support the environment.\n\u2022 Modern office equipment. Purchased for you or available to borrow, depending on your location.\n\u2022 Great Place to Work Certified Employer in the Philippines",
    "job_is_remote": false,
    "job_posted_at": "23 hours ago",
    "job_posted_at_timestamp": 1753081200,
    "job_posted_at_datetime_utc": "2025-07-21T07:00:00.000Z",
    "job_location": "Philippines",
    "job_city": null,
    "job_state": null,
    "job_country": "PH",
    "job_latitude": 12.879721,
    "job_longitude": 121.774017,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6tnO5hV6ICVJHf55AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "hfv80ygn_76dxUXKAAAAAA==",
    "job_title": "Data Analyst / Data Engineer (Mid - Senior Level)",
    "employer_name": "Nityo Infotech Services Philippines Inc.",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Jobstreet",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.jobstreet.com/job/85890418?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85890418?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "We're Hiring! Data Roles at All Levels | Join a Dynamic and Innovative Tech Team\n\nWe\u2019re on the lookout for passionate and highly skilled Data Professionals to join our team and help drive next-generation data-driven solutions. Whether you're an early-career data enthusiast or a senior cloud expert, we have a spot for you!\nOPEN POSITIONS\n1. Data Engineer\n\nLocation: Bagumbayan, Quezon City\nSchedule: Monday \u2013 Friday | Day Shift\nSetup: Onsite\nExperience: 1\u20132 years\nRate: Open\n\nQualifications:\n\u2022 Solid hands-on experience in Databricks, AWS, and Python\n\u2022 Strong understanding of data pipelines and modern data stack\n\u2022 Experience in implementing ETL workflows and working with cloud-native tools\n\n2. Data Analyst \u2013 Power BI\n\nLocation: Makati - Hybrid\nSchedule: Monday \u2013 Friday | Morning Shift\nExperience: 3-5 years\nRate: Open\n\nQualifications:\n\u2022 Strong background in data analysis and visualization using Power BI\n\u2022 Proven ability to translate data into meaningful business insights\n\u2022 Proficiency in SQL and DAX is a plus\n\n3. Senior / Principal Azure Data Engineer\n\nLocation: Makati - Hybrid\nSchedule: Monday \u2013 Friday | Morning Shift\nExperience: 3-5 years\nRate: Open\n\nQualifications:\n\u2022 Extensive experience with Microsoft Azure Data Services:\n\u2022 Azure App Services, Azure Databricks, Azure Functions, Azure Logic Apps\n\u2022 Azure Synapse Analytics, Azure Cosmos DB\n\u2022 Expertise in architecting and developing scalable, cloud-based data solutions\n\u2022 Proven track record of leading data projects in a senior or principal capacity\n\n4. Data Engineer (Mid to Senior Level)\n\nLocation: Makati or Ortigas\nSetup: Hybrid (2 days onsite, 3 days WFH)\nSchedule: Morning Shift\nSalary: Up to Php 200,000\n\nQualifications:\n\u2022 Proven experience as a Data Engineer or similar role with a focus on data pipelines and ETL processes\n\u2022 Strong knowledge of Microsoft Azure services:\n\u2022 Azure Data Factory, Azure Synapse, Azure Databricks\n\u2022 Azure Blob Storage, Azure Data Lake Gen 2\n\u2022 Proficient in writing efficient SQL queries (SQL Server, PostgreSQL)\n\u2022 Understanding of Software Engineering principles applied to Data Engineering (e.g., CI/CD, version control, testing)\n\u2022 Experience working with big data technologies like Apache Spark\n\nWhy Join Us?\n\u2022 Work with leading technologies and cloud platforms\n\u2022 Hybrid work options available for most roles\n\u2022 Competitive compensation and open rate offers\n\u2022 Opportunity to grow within a tech-forward environment",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Manila, Metro Manila, Philippines",
    "job_city": "Manila",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.5995133,
    "job_longitude": 120.984234,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dhfv80ygn_76dxUXKAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "diLjnls6cNsN2moZAAAAAA==",
    "job_title": "Data Engineer - IoT Cost-Optimisation",
    "employer_name": "ERNI Philippines",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Indeed Job Search",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.indeed.com/viewjob?jk=2084ac0f2cee9768&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=2084ac0f2cee9768&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-iot-cost-optimisation-erni-JV_IC4747401_KO0,35_KE36,40.htm?jl=1009812396511&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6877f38f57fb149ae371f879-data-engineer---iot-cost-optimisation?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Rocketship",
        "apply_link": "https://www.remoterocketship.com/company/betterask-erni/jobs/data-engineer-iot-cost-optimisation-mandaluyong-city-hybrid?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://ph.jooble.org/jdp/-9021195257779167983?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/data-engineer-iot-cost-optimisation-jobs/EA2B195CC497D9C9?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Founded in 1994 and headquartered in Switzerland, ERNI is a leading Software Development company with over 800 employees worldwide. Specializing in IT and software engineering, we drive innovation in process and technology. Our first service center in Asia Pacific, located in Metro Manila (Mandaluyong), supports clients across Europe, APAC, the Philippines, and the USA. As we continue to grow, we're looking for passionate and motivated individuals to join our team.\n\nWhy ERNI is the Perfect Place for You:\n\u2022 International Exposure: Work with global clients on cutting-edge projects.\n\u2022 Inclusive Culture: Thrive in a collaborative and diverse work environment.\n\u2022 Career Development: Enjoy continuous learning and professional growth opportunities.\n\nPerks and Benefits:\n\u2022 Career Stability: Enjoy a stable career path with ample project opportunities.\n\u2022 Skill Enhancement: Access free training and certifications.\n\u2022 Baby Basket: To welcome your newborn to the ERNI family.\n\u2022 Fruit Basket: Boost of vitamins during hospitalization.\n\u2022 Office Perks: Enjoy free snacks and coffee.\n\nGrowth and Opportunities:\n\u2022 Free Training: Advance your skills through technical and non-technical training.\n\u2022 Challenging Projects: Engage in complex software projects across MedTech, Industry, Finance, and Transportation.\n\u2022 Supportive Environment: Benefit from a team dedicated to guiding and supporting your success.\n\u2022 Recognition and Advancement: Receive acknowledgment for your efforts and opportunities for promotion.\n\u2022 Open Communication: Experience transparency and value your input in our culture.\n\nFlexibility:\n\u2022 Hybrid Work Setup: Balance remote and in-person work for better work-life integration.\n\nEvents:\n\u2022 Connect and Celebrate: Participate in a variety of events including leisure, summer, family, social, and year-end gatherings.\n\nWhat are our wishes:\n\u2022 3+ years building data solutions on Microsoft Azure (PaaS & serverless) \u2013 most of the following Azure services: IoT Hub, Event Hub, Stream Analytics, Functions, Cosmos DB, Databricks.\n\u2022 Solid SQL and C# knowledge.\n\u2022 Experience squeezing data size/frequency in high-throughput IoT or event-driven systems.\n\u2022 Fluent English; clear, pragmatic communication.\n\nNice-to-have\n\u2022 MQTT / OPC-UA or other device protocols; edge-computing toolchains.\n\u2022 Power BI, Kusto / ADX, or Grafana Loki for observability.\n\u2022 Experience with IoT solutions.\n\nEngagement details:\n\u2022 Timeline: ASAP 30 Sep 2025 (approx. 50\u201360 person-days).\n\u2022 Location: Remote first; on-site D\u00fcsseldorf workshops ad-hoc.\n\u2022 Working in a small agile team: Principal IoT Consultant, KPI Governance Lead, Azure Architect, Embedded developer.\n\u2022 Outcome: Clear cost breakdown and optimisation roadmap for a big IoT system based on Azure.\n\nThe team is mixed from Germany, Switzerland and Spain. And most of the work can be done remotely.\n\nHow can you contribute to the team?\n\u2022 Design & run Azure-native data pipelines that collect, transform and store telemetry at the lowest safe cost.\n\u2022 Profile data volumes and flows from gateway to cloud, spot \u201cchatty\u201d patterns and propose compression, batching or edge-processing fixes.\n\u2022 Build the active cost-monitoring stack (e.g. Cost Management API + Power BI / Grafana) and KPI dashboards demanded by the project deliverables.\n\u2022 Feed your findings into the Health-Check Report and the cost-reduction backlog; quantify \u20ac-savings vs. engineering effort.\n\nEmployment Type: Project-based (possibility to work onsite in Germany for 1 month)\n\nSwitzerland \u00b7 Germany \u00b7 Spain \u00b7 Slovakia \u00b7 Romania \u00b7 Philippines \u00b7 Singapore \u00b7 USA\n\nERNI Development Center Philippines Inc., 9th Floor, Lica Malls Shaw, 500 Shaw Boulevard, 1555, Mandaluyong City, Philippines\n\n+63 5310 1707 | www.betterask.erni | info@erni.ph",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1752537600,
    "job_posted_at_datetime_utc": "2025-07-15T00:00:00.000Z",
    "job_location": "Mandaluyong City, Metro Manila, Philippines",
    "job_city": "Mandaluyong City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.576267999999999,
    "job_longitude": 121.03924359999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DdiLjnls6cNsN2moZAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "kceiJkBPTHGOW4nYAAAAAA==",
    "job_title": "Data Engineer Informatica",
    "employer_name": "Nezda Technologies, Inc.",
    "employer_logo": null,
    "employer_website": "https://nezdaglobal.com",
    "job_publisher": "Jobstreet",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.jobstreet.com/job/85907725?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85907725?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "\u2022 Design and develop scalable and efficient ETL solutions using Informatica tools.\n\u2022 Extract, transform, and load data from a variety of sources to enterprise data warehouses.\n\u2022 Collaborate with business and technical stakeholders to gather requirements and translate them into ETL processes.\n\u2022 Participate in the full project lifecycle: requirements gathering, design, development, testing, deployment, and maintenance.\n\u2022 Optimize existing ETL workflows for performance and reliability.\n\u2022 Ensure data quality, consistency, and integrity across systems.\n\u2022 Support production deployments and resolve data integration issues in a timely manner.",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Quezon City, Metro Manila, Philippines",
    "job_city": "Quezon City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.6487853,
    "job_longitude": 121.0509385,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DkceiJkBPTHGOW4nYAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "HMkxh5yn4utQb_2yAAAAAA==",
    "job_title": "Senior Data Engineer",
    "employer_name": "ING",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "ING's Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.ing.com/en/job/makati-city/senior-data-engineer/3121/25683897664?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ING's Careers",
        "apply_link": "https://careers.ing.com/en/job/makati-city/senior-data-engineer/3121/25683897664?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=851d3e5f5a203430&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85746384?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/senior-data-engineer-ing-JV_IC4778930_KO0,20_KE21,24.htm?jl=1009760187122&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jora",
        "apply_link": "https://ph.jora.com/job/Data-Engineer-acd369b1fbb11d01bb9ee353504ea8da?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "MyCareersDB",
        "apply_link": "https://mycareersdb.com/jobs/view/10986/Senior-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Key Responsibilities\n\u2022 Implement and maintain data workflows, ETL / ELT processes, and pipelines to move identified data into target platforms for further consumption and integration as required by product owners.\n\u2022 Design and implement identified data processes and storage such as usage of databases, data warehouses, data marts, and data lakes with data security in mind.\n\u2022 Build supporting tooling and procedures needed for data movement and monitoring activities.\n\u2022 Implement available automations for data movement processes.\n\u2022 Ensure delivered workflows and processes meet expected quality outputs through conducting testing.\n\u2022 Document specs and implementations of deliverables mentioned.\n\u2022 Work with operations teams for maintenance and continuous improvement of workflows and pipelines.\n\u2022 Work on distributed Data processing and database with Distribution & partitioning\n\u2022 Able to convert LDM to PDM in Data-vault Data warehouse.\n\u2022 Ability to perform tasks related to end-to-end Data Journey including Reporting.\n\u2022 Ensure integrated data pipeline validation is performed.\n\nKey Capabilities/Experience\n\u2022 Expertise in one/ some of ETL tools like ADF, Airflow.\n\u2022 Expertise in SQL / RDBMS databases, handling of structured data.\n\u2022 Extensive experience in design and implementation of data handling processes (ingestion, transformation, modelling, and storage)\n\u2022 Experience in creating of Big Data warehouses, its design, and its various implementation methods.\n\u2022 Flexible and willing to learn various ETL / data integration tools and products.\n\u2022 Strong problem solving and solutions engineering mindset.\n\u2022 Experience in working and coordinating remotely with teams and stakeholders.\n\u2022 Experience with data integration and ETL technologies.\n\u2022 Experienced with software version control technologies such as Git.\n\u2022 Experience in CI / CD and DevOps is a plus.\n\u2022 Experience in Agile way of working is a plus.\n\nMinimum Qualifications\n\u2022 Expertise in one/ some of ETL tools like ADF, Airflow.\n\u2022 Expertise in SQL / RDBMS databases, handling of structured data.\n\u2022 Extensive experience in design and implementation of data handling processes (ingestion, transformation, modelling, and storage)\n\u2022 Experience in creating of Big Data warehouses, its design, and its various implementation methods.\n\nNice to have\n\u2022 Proficiency and experience in programming languages such as Python, shell scripting\n\u2022 Experience in cloud platforms and technologies (Azure, AWS, GCP) is a plus.",
    "job_is_remote": false,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DHMkxh5yn4utQb_2yAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "MzFJX9JnRS0wxeJhAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "Nimbyx Philippines, Inc.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSeegRMpU9z2DbgFPUOgBY8nOzHivt9OF3I6U-8&s=0",
    "employer_website": null,
    "job_publisher": "Smart Recruiters Jobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.smartrecruiters.com/NimbyxPhilippinesInc/744000062113733-lead-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Smart Recruiters Jobs",
        "apply_link": "https://jobs.smartrecruiters.com/NimbyxPhilippinesInc/744000062113733-lead-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/lead-data-engineer-nimbyx-philippines-inc-JV_IC4778930_KO0,18_KE19,41.htm?jl=1009759137369&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.fr/job-listing/lead-data-engineer-nimbyx-philippines-inc-JV_IC4778930_KO0,18_KE19,41.htm?jl=1009759137369&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Towards AI Jobs",
        "apply_link": "https://jobs.towardsai.net/job/nimbyx-philippines-inc-lead-data-engineer-jcb7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://ph.bebee.com/job/241fdd8aced0de44321b296e27c70e1f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://ph.jooble.org/jdp/6624875891375357054?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido.com",
        "apply_link": "https://ph.jobrapido.com/jobpreview/4799667421189242880?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/lead-data-engineer-jobs/737A132869043248?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Company Description\n\nWe create what others only imagine.\n\nAs an AI-driven venture capital firm based in BGC, Philippines with offices in Vancouver, Canada and Seoul, Korea, we build AI agents that eliminate the mundane, so people can unlock their full creative and strategic potential.\n\nLife at Nimbyx\n\nWe\u2019re a team of curious minds, fearless builders, and lifelong learners, driven by the belief that technology should empower, not replace.\n\nWe move fast, break boundaries, cut through bureaucracy, and operate by simple, powerful rules. What matters here isn\u2019t titles or politics\u2014it\u2019s impact.\n\nWe\u2019re looking for people bold enough to help us build a world that works smarter, faster, and better for everyone.\n\nIf you want comfort, look elsewhere. If you want to shape the future, welcome to Nimbyx!\n\nJob Description\n\nLead Data Engineer\n\nDrive innovation. Shape strategy. Power smarter decisions.\n\nAt Nimbyx, we\u2019re building AI-powered platforms that are reshaping how businesses and industries work - from healthcare to beyond. As Lead Data Engineer, you won\u2019t just be building pipelines - you\u2019ll be building the data backbone of bold ideas, fueling intelligent decisions across our entire ecosystem.\n\nWe\u2019re looking for a visionary technical leader who can architect scalable solutions, inspire a growing team, and turn complex data into business-defining insight. If you\u2019re passionate about solving real-world problems through data, and love rolling up your sleeves while setting the direction\u2014this is your moment.\n\nWhat You\u2019ll Do\n\u2022 Architect, build, and scale production-grade data pipelines and ETL flows using Azure Data Factory and Databricks\n\u2022 Design and implement high-performance, cloud-native data warehouses (Snowflake preferred)\n\u2022 Lead the evolution of our data engineering practice, driving excellence in DevOps, data modeling, security, and automation\n\u2022 Transform diverse data sources (social, web, internal systems, product analytics) into clean, usable datasets for real-time and batch processing\n\u2022 Partner with analysts and stakeholders to develop interactive dashboards and visualizations in Power BI or Tableau that guide business strategy\n\u2022 Mentor and lead a team of engineers, setting technical direction, coaching on best practices, and ensuring alignment with cross-functional goals\n\u2022 Own database performance and optimization efforts (SQL Server, Snowflake), including stored procedures, views, and tuning\n\u2022 Explore, evaluate, and introduce new tools and technologies that make our data infrastructure smarter, faster, and more powerful\n\nQualifications\n\nWhat You Bring\n\u2022 5+ years in data engineering, with 1\u20132 years in a leadership role driving results\n\u2022 Proven expertise in designing scalable pipelines and data architectures on Azure\n\u2022 Mastery of Python, SQL, and solid working knowledge of Snowflake, SQL Server\n\u2022 Deep understanding of data modeling, warehousing concepts, and ETL best practices (bonus if you're Kimball-savvy)\n\u2022 Experience building rich, insightful dashboards using Power BI or Tableau\n\u2022 Familiarity with CI/CD, infrastructure-as-code, and agile environments\n\u2022 A collaborative spirit, bias toward action, and thrive on turning data into impact\n\nWhy Join Nimbyx?\n\u2022 Be part of a mission-driven team building AI-powered tools that change industries\n\u2022 Own your impact and work on meaningful projects that directly shape product and strategy\n\u2022 Join a culture that values curiosity, bold thinking, and continuous learning\n\u2022 Collaborate with brilliant engineers, product leaders, and innovators\n\u2022 Competitive compensation, benefits, and growth opportunities in a fast-scaling company\n\nThis isn\u2019t just another data role. It\u2019s a chance to lead the charge in how data drives the future.\n\nIf you're ready to architect what's next, we want to meet you.\n\nAdditional Information\n\n\ud83c\udf1f Work setup: FULL ONSITE, Monday to Friday or Tuesday to Saturday\n\n\u23f0 Schedule: 8am to 5pm\n\n\ud83d\udccd Location: One World Place, 32nd Street, BGC, Taguig City\n\n\ud83d\udcbc Employment Type: Full-time + Permanent\n\nGet to know us more:\n\nEvident: AI-Powered Growth Partner for Digital Dentists & Labs\n\n\u2022 https://www.instagram.com/evidentdigital/\n\n\u2022 https://evidentdigital.com\n\nNimbyx on Instagram: Global Venture Capital firm in BGC, Vancouver, & Seoul.\n\n\u2022 https://www.instagram.com/nimbyx.official/\n\n\u2022 https://nimbyx.com/careers/",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DMzFJX9JnRS0wxeJhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "818dd7Ajlnxb8TiDAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Schneider Electric",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQUO3e2d-Y4YaXBR8CU2ops9FML0THUqawkFCvd&s=0",
    "employer_website": null,
    "job_publisher": "Schneider Electric",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.se.com/ww/en/about-us/careers/job-details/data-engineer/009GNR/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Schneider Electric",
        "apply_link": "https://www.se.com/ww/en/about-us/careers/job-details/data-engineer/009GNR/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Philippines",
        "apply_link": "https://ph.linkedin.com/jobs/view/data-engineer-at-schneider-electric-4256748633?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85234826?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Schneiderele.taleo.net",
        "apply_link": "https://schneiderele.taleo.net/careersection/2/jobdetail.ftl?job=009GNR&lang=en&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Fairygodboss",
        "apply_link": "https://fairygodboss.com/jobs/schneider-electric/data-engineer-2a7ae718230abb3a6087e963ab5d8523?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://ph.bebee.com/job/3b75a0b1a0953be932e165146b4989e6?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/ph/job/data-engineer--cavite-city--e1f89d61b3168327a13126ce74778ca97?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/data-engineer-cavite-calabarzon-region-iv-a-ph-schneider-electric-north-america-009gnr?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "#LI-Hybrid\n\nSummary\n\nWe are seeking a skilled Data Engineer to join our dynamic team. In this role, you will be responsible for designing, developing, and maintaining robust data pipelines and ETL processes. You will work heavily with SQL and Alteryx to manage dataflows from sources such as AWS Redshift, enabling our business intelligence analysts and data visualization specialists to derive actionable insights.\n\nKey Responsibilities\n\n\u00b7 Design, develop, and maintain scalable ETL pipelines.\n\n\u00b7 Perform data extraction, transformation, and loading (ETL) from various data sources\n\n\u00b7 Optimize and manage dataflows to ensure data integrity and performance.\n\n\u00b7 Collaborate with business intelligence analysts and data visualization specialists to understand data requirements.\n\n\u00b7 Develop and maintain SQL queries and scripts for data manipulation and analysis.\n\n\u00b7 Utilize Alteryx (or similar) for data preparation, blending, and advanced analytics.\n\n\u00b7 Monitor and troubleshoot data pipeline issues to ensure timely data availability.\n\n\u00b7 Implement data quality checks and validation processes.\n\n\u00b7 Document data processes, workflows, and best practices.\n\nQualifications\n\nRequired Skills\n\n\u00b7 Proficiency in SQL and experience with relational databases.\n\n\u00b7 Strong experience with ETL tools\n\n\u00b7 Knowledge of AWS Redshift and other cloud-based data warehousing solutions.\n\n\u00b7 Familiarity with data modeling and database design.\n\n\u00b7 Strong problem-solving and analytical skills.\n\n\u00b7 Ability to work collaboratively in a team environment.\n\n\u00b7 Excellent communication skills.\n\nNice-to-Have Skills\n\n\u00b7 Experience with other ETL tools and platforms.\n\n\u00b7 Knowledge of Python or other programming languages.\n\n\u00b7 Familiarity with data visualization tools such as Tableau or Power BI.\n\n\u00b7 Understanding of data governance and security best practices.\n\n\u00b7 Experience with big data technologies (e.g., Hadoop, Spark).\n\nEducational Background\n\n\u00b7 Bachelor's degree in Computer Science, Information Technology, Data Science, or a related field. A master's degree is a plus.\n\nWork Experience\n\n\u00b7 3+ years of experience in data engineering or a related field.\n\n\u00b7 Proven experience in designing and managing ETL processes.\n\n\u00b7 Hands-on experience with SQL and Alteryx.\n\n\u00b7 Experience working with cloud-based data solutions, particularly AWS Redshift.\n\nSchedule: Full-time\nReq: 009GNR",
    "job_is_remote": false,
    "job_posted_at": "25 days ago",
    "job_posted_at_timestamp": 1750982400,
    "job_posted_at_datetime_utc": "2025-06-27T00:00:00.000Z",
    "job_location": "Cavite, Philippines",
    "job_city": null,
    "job_state": "Cavite",
    "job_country": "PH",
    "job_latitude": 14.245632899999999,
    "job_longitude": 120.87856579999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D818dd7Ajlnxb8TiDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "G4ToQAsZXpo3M6EHAAAAAA==",
    "job_title": "Data Engineering (Data Migration) Consultant - Engineering - PH PDC",
    "employer_name": "Deloitte SEA",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT2Qbc7oKX0Tv-acvCz9bHQqimmAv4Sig763OwT&s=0",
    "employer_website": "https://www2.deloitte.com",
    "job_publisher": "Careers Job Search - Deloitte",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.sea.deloitte.com/job/Manila-Data-Engineering-%28Data-Migration%29-Consultant-Engineering-PH-PDC/1065324866/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers Job Search - Deloitte",
        "apply_link": "https://jobs.sea.deloitte.com/job/Manila-Data-Engineering-%28Data-Migration%29-Consultant-Engineering-PH-PDC/1065324866/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=1273d3c86eb0feee&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/azure-data-engineering-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,52_KE53,61.htm?jl=1009763959918&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.it/job-listing/azure-integration-developer-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,57_KE58,66.htm?jl=1009669949659&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.br/job-listing/cloud-data-engineering-aws-azure-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,62_KE63,71.htm?jl=1009676951205&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.es/job-listing/azure-integration-developer-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,57_KE58,66.htm?jl=1009669949659&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.at/job-listing/data-engineering-data-migration-consultant-engineering-ph-pdc-deloitte-JV_IC2363570_KO0,61_KE62,70.htm?jl=1009741352311&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit Philippines",
        "apply_link": "https://www.foundit.com.ph/job/data-engineering-data-migration-consultant-engineering-ph-pdc-deloitte-philippines-34922227?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Cloud Consultant based in Deloitte Consulting Philippines Delivery Center\n\nAre you ready to unleash your potential?\n\nAt Deloitte, our purpose is to make an impact that matters for our clients, our people, and the communities we serve.\n\nWe believe we have a responsibility to be a force for good, and WorldImpact is our portfolio of initiatives focused on making a tangible impact on society\u2019s biggest challenges and creating a better future. We strive to advise clients on how to deliver purpose-led growth and embed more equitable, inclusive as well as sustainable business practices.\n\nHence, we seek talented individuals driven to excel and innovate, working together to achieve our shared goals.\n\nWe are committed to creating positive work experiences that foster a culture of respect and inclusion, where diverse perspectives are celebrated, and everyone is recognized for their contributions.\n\nReady to unleash your potential with us? Join the winning team now!\n\nWork you will do\n\nDeloitte\u2019s Engineering offers help to enable organization\u2019s end-to-end journey from on-premise legacy systems to the cloud, from design through deployment, and leading to the ultimate destination\u2014a transformed organization primed for growth.\n\nAs a Cloud Engineer, you will help build and connect sustainable cloud-native systems to thrive in the cloud ecosystem. Services include: Strategy & Architecture; Assets & Industry Solutions; Engineering & Integration\n\u2022 Based on selection criteria set up driver tables for key migration objects to track data through migration process\n\u2022 Based on mapping rules, develop data migration transform jobs using SQL with adequate tracing\n\u2022 Establish exception handling through transform process\n\u2022 Set up store procedures in Amazon Aurora PostgreSQL database to automate the migration pipeline\n\u2022 Setup and manage code versioning using CodeCommit in AWS\n\u2022 Follow best practice coding principles ensuring it is well structured and comments are included in code so it can be understood and diagnosed by another developer\n\u2022 Produce documentation outlining pipeline and key code elements to enable handover of code\n\u2022 Use AWS Data Migration Service to load data into target structure\n\u2022 Perform and monitor ETL activities for test loads in Non-Production environments\n\u2022 Receive feedback from peer review of code\n\u2022 Promptly raise questions/issues or escalate blockers regarding the mapping documents so rules can be clarified by the team\n\u2022 Clearly communicate and track progress within delivery tracking tools (ADO)\n\nYour role as a leader\n\nAt Deloitte, we believe in the importance of empowering our people to be leaders at all levels. We connect our purpose and shared values to identify issues as well as to make an impact that matters to our clients, people and the communities. Additionally, Consultants across our Firm are expected to:\n\u2022 Demonstrate a strong commitment to personal learning and development.\n\u2022 Understand how our daily work contributes to the priorities of the team and business.\n\u2022 Understand the set expectations and demonstrate accountability in keeping personal performance on track.\n\u2022 Actively focus on developing effective communications and relationship-building skills with stakeholders, clients and team.\n\u2022 Demonstrate an appreciation for working with others.\n\u2022 Understand what is fundamental to Deloitte\u2019s success as a business.\n\u2022 Demonstrate integrity and an awareness of strengths, differences, and personal impact.\n\u2022 Develop their understanding of Deloitte and offer a fresh perspective.\n\nEnough about us, let\u2019s talk about you\n\u2022 Bachelor\u2019s degree in Software Engineering, Information Technology, or equivalent\n\u2022 Bachelor\u2019s degree in Software Engineering, Information Technology, or equivalent\n\u2022 At least 7 years working experience as a Data Engineer; adept experience in data engineering principles and how to apply advanced data manipulation processes to various types of data.\n\u2022 Knowledge and experience of setting up and managing data engineering (ETL) and store procedures in AWS platform\n\u2022 Built SQL to perform transformation of data for either analytical reporting or migration purposes \u2013 can provide an example of how it was applied and how it was ensured to be performant\n\u2022 Experience in taking mapping rules to convert into code and then generating technical documentation\n\u2022 Experience of generating code that is performant across high volume datasets and tuning to ensure transformation can run within the cutover window\n\u2022 Experience in generating technical documentation\n\u2022 High attention to detail\n\u2022 Motivated and interested to learn\n\u2022 Clear written and spoken communication in English\n\nWhat is in store for you?\n\u2022 Embrace the dynamic nature of our work environment with the opportunity to work on a hybrid set-up and on a shifting schedule.\n\u2022 Rewards platform \u2013 your hard work won't go unnoticed at Deloitte!\n\u2022 Training and development - at Deloitte we believe in investing in our best assets, the people! You will have access to world class training and funding towards industry and other professional certifications.\n\u2022 Receive support andv mentoring to progress your career. You will have access to mentors and coaches who will help you pave a path for career progression.\n\u2022 Benefits effective upon hiring including paid time off and holidays, health, and life insurance!\n\nNext Steps\n\nSound like the sort of role for you? Apply now.\n\nDue to volume of applications, we regret only shortlisted candidates will be notified.\n\nCandidates will only be contacted by authorized Deloitte Recruiters via firm\u2019s business contact number or business email address.\n\n\u00a9 2025 DCPDC Inc.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1752105600,
    "job_posted_at_datetime_utc": "2025-07-10T00:00:00.000Z",
    "job_location": "Manila, Metro Manila, Philippines",
    "job_city": "Manila",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.5995133,
    "job_longitude": 120.984234,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DG4ToQAsZXpo3M6EHAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "_uf3ZFI92dW4jerlAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Robert Walters",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSOnL29esIAAUBZUF66P2NP--9XbgzH7qOlbfE9&s=0",
    "employer_website": "https://www.robertwalters.com/",
    "job_publisher": "Robert Walters Philippines",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.robertwalters.com.ph/techtransformation/jobs/businessintelligenceanalytics/1839295-data-engineer.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Robert Walters Philippines",
        "apply_link": "https://www.robertwalters.com.ph/techtransformation/jobs/businessintelligenceanalytics/1839295-data-engineer.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "ING's Careers",
        "apply_link": "https://careers.ing.com/en/job/makati-city/data-engineer/3121/26173398656?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Chevron Careers",
        "apply_link": "https://careers.chevron.com/job/makati-city/data-engineer/38138/71451952528?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Careers At Mace - Mace Group",
        "apply_link": "https://careers.macegroup.com/gb/en/job/38325/Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=79a46234e7c94a85&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Kalibrr",
        "apply_link": "https://www.kalibrr.com/c/questronix-corporation/jobs/188233/data-engineer-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85298335?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-ksearch-asia-consulting-JV_IC4778930_KO0,13_KE14,37.htm?jl=1009012570692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "An exciting opportunity awaits you to join a pioneering company in the energy and power sector.\n\nWhat you'll do:\n\u2022 Identify, design, and implement automated processes such as billing systems, alert mechanisms, query usage analysis, and resource monitoring to optimise internal operations.\n\u2022 Ideate, iterate on, design, and build new features from scratch or enhance existing commercial projects to deliver additional value for platform users.\n\u2022 Provide constructive feedback or contribute directly to platform feature improvements alongside the Data Infrastructure & Products team to boost efficiency and capabilities.\n\u2022 Lead end-to-end delivery of commercial projects by managing timelines, resources, and technical execution from inception through completion.\n\u2022 Collaborate with cross-functional teams to understand complex data requirements and develop tailored solutions that address specific business needs.\n\u2022 Participate in client-facing meetings to finalise project details, clarify requirements, provide updates, and ensure alignment between stakeholders.\n\u2022 Design and implement configurable data pipelines for ingestion, transformation, reverse ETL processes based on detailed client functional requirements.\n\u2022 Develop comprehensive data quality checks at multiple checkpoints to ensure accuracy, completeness, and consistency throughout all stages of data processing.\n\u2022 Work closely with Machine Learning engineers to create and maintain ML data pipelines for training models, batch inference tasks, deployment workflows, validation routines, and ongoing monitoring.\n\u2022 Assist in troubleshooting and resolving any issues related to data integrity or pipeline performance while maintaining clear documentation.\n\nWhat you bring:\n\u2022 A Bachelor\u2019s degree in Computer Science, Engineering or a related discipline provides you with a solid academic foundation for tackling complex technical challenges.\n\u2022 Proficiency in SQL and Python enables you to efficiently manipulate large datasets and automate repetitive tasks within various environments.\n\u2022 A thorough understanding of conventional database concepts ensures you can design effective storage solutions that meet evolving business needs.\n\u2022 Hands-on experience with modern data warehouse architectures (such as lakehouse or lake) empowers you to build scalable systems capable of handling vast amounts of information.\n\u2022 Practical knowledge of cloud platforms like AWS, Azure or GCP allows you to deploy resilient infrastructure that supports high-availability applications.\n\u2022 Demonstrated analytical skills help you break down intricate problems into manageable components while identifying optimal solutions quickly.\n\u2022 Experience implementing comprehensive Data Quality Checks guarantees the reliability of insights derived from processed information.\n\u2022 Familiarity with data processing frameworks such as dbt streamlines your ability to transform raw inputs into actionable outputs efficiently.\n\u2022 Expertise using integration tools like Airflow enhances your capacity for orchestrating complex ETL workflows across distributed systems.\n\u2022 Exposure to modern warehouse/lakehouse solutions (e.g., Delta Lake, Redshift or Snowflake) equips you with versatile skills applicable across multiple platforms.\n\u2022 Working knowledge of Databricks (on either Azure or AWS) combined with an understanding of Spark positions you well for advanced analytics tasks.\n\u2022 Excellent communication abilities foster smooth collaboration within multidisciplinary teams while ensuring clarity when interacting with clients or stakeholders.\n\u2022 Self-motivation paired with a passion for continuous learning drives your commitment towards personal growth and staying current with industry trends.\n\nWhat sets this company apart:\n\nThis organisation stands out as one of Makati\u2019s most promising start-ups dedicated exclusively to harnessing the power of data science and artificial intelligence for meaningful societal impact. Here you\u2019ll find an inclusive culture that values knowledge sharing just as much as technical excellence\u2014ensuring everyone has access to mentorship regardless of experience level. Flexible working opportunities allow you to balance professional ambitions with personal commitments without compromise. The leadership team is deeply invested in nurturing talent through regular training sessions designed around emerging industry trends so that every member remains ahead of the curve. Collaboration is woven into every aspect of daily operations; whether brainstorming new features or refining existing processes together with colleagues from diverse backgrounds fosters genuine camaraderie among peers. By joining this team not only do you gain exposure to cutting-edge technology stacks but also become part of a supportive network committed towards collective success\u2014making it an ideal place for those who wish their work could truly make a difference beyond business metrics alone.\n\nWhat's next:\n\nIf you're ready to take your career in data engineering further while making an impact alongside passionate professionals\u2014this is your moment!\n\nApply today by clicking on the link provided below; seize this chance to shape tomorrow\u2019s world through transformative data solutions.\n\nDue to the high volume of applications we are experiencing, our team will only be in touch with you if your application is shortlisted.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1752105600,
    "job_posted_at_datetime_utc": "2025-07-10T00:00:00.000Z",
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D_uf3ZFI92dW4jerlAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "dQYKP8za-cksWVy7AAAAAA==",
    "job_title": "Principal Data Engineer (Remote, Day Shift)",
    "employer_name": "SYMPHONY",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Jobstreet",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.jobstreet.com/job/85814863?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/85814863?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "JOB OVERVIEW\n\nWe are looking for a Principal Data Engineer to lead the technical vision, architecture, and delivery of our next-generation GRC data platform. You\u2019ll work at the intersection of AI and cloud-native architecture for building custom data solutions that addresses user critical problems and help us scale intelligent, secure, and resilient data systems across multiple global regions.\n\nYou will play a key role in enabling AI-driven insights, automating compliance workflows, and ensuring our platform remains robust and audit-ready for regulated industries.\n\nKEY RESPONSIBILITIES\n\nData Architecture & Engineering Leadership:\n\u2022 Responsible for identifying, analyzing, and conducting the client's business requirements gap analysis and processes through document analysis, interviews, workshops, and workflow analysis.\n\u2022 Guide and support the Data Engineer in the team for executing on projects involving technology upgrade and implementing optimizations wherever appropriate.\n\u2022 Define data architecture for scalability, performance and cost-efficiency\n\u2022 Evaluate and implement new technologies that facilitates or simplifies the end-to-end data lifecycle targeted for reporting and AI use cases\n\u2022 Documenting the information gathered during the data requirements workshop and preparing the solution architecture documents.\n\u2022 Partnering with internal stakeholders that includes heads of product, engineering and customer success to identify and streamline priorities and execute it with the team for delivering data processes and products.\n\u2022 Act as a technical authority providing guidance on data modelling, schema design, metadata management and governance\n\u2022 Mentor data engineers on the team and help grow the technical capabilities of the data team\n\u2022 Promote best practices in the ELT processes and the accompanying CI/CD and DevOps for data\n\u2022 Developing and maintaining SQL queries using various tools like DBT, Azure Synapse, SQL server, etc. in line with business and customer requirements.\n\u2022 Developing and maintaining reports in various Business Intelligence (BI) platforms, including Yellowfin, in line with business and customer requirements.\n\u2022 Using project management methodologies, principles and techniques, developing project plans and estimate the required effort (cost) and resources\n\u2022 Working with the broader engineering and product team and other stakeholders on troubleshooting, investigating, and remediating reporting bugs and issues.\n\u2022 Analyzing data and actively coordinating with stakeholders to evaluate the inefficiencies of business process and to ensure that they are accurately updated to eliminate any potential issues.\n\u2022 Implementing enterprise data warehouse infrastructure and managing the data pipelines feeding into analytics and advanced AI powered analytics subsystems\n\nAI/ML Enablement:\n\u2022 Design ML-ready data systems that feed into the 6Clicks AI system, including feature stores, training pipelines and inference-serving infrastructure.\n\u2022 Collaborate with the AI engineering, core engineering and product teams to operationalize models that facilitate automation in the GRC processes\n\u2022 Partner with product, Artificial Intelligence/Machine Learning (AI/ML), and platform teams to deliver data solutions that directly power client-facing features\n\u2022 Set technical direction and best practices for the data engineering functions that drive AI/ML functionalities\n\u2022 Evaluate emerging technologies and lead critical platform upgrades or migrations that will accelerate AI/ML feature development\n\nWhat you bring\n\u2022 Preferred candidate with Master\u2019s in Data Analytics or related field of study.\n\u2022 Minimum 12-15 years of relevant work experience.\n\u2022 Prior experience in a principal, staff or lead engineer role within high-growth SaaS or enterprise tech environments\n\u2022 Strong understanding of data security, compliance, and multi-region cloud deployments\n\u2022 Proven experience in designing and developing ETL pipelines using cloud-based technologies.\n\u2022 Experience integrating data from multiple sources, including SQL databases, Excel spreadsheets, and APIs.\n\u2022 Excellent communication and collaboration skills, with the ability to effectively interact with stakeholders at all levels of the organization.\n\u2022 Detail-oriented with strong analytical and problem-solving skills.\n\u2022 Self-motivated and able to work independently as well as part of a team. Ability to relate to people, understand their needs and align it with proposed solutions.\n\u2022 Previous experience in Governance, Risk & Compliance (GRC) or GRC technologies is a plus.",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": "Metro Manila, Philippines",
    "job_city": null,
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.609053699999999,
    "job_longitude": 121.0222565,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DdQYKP8za-cksWVy7AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "VMzXnWB28OOEUSPsAAAAAA==",
    "job_title": "Data Engineer - Python, ETL, SQL (Hybrid)",
    "employer_name": "Tamaray People Solutions Corp,",
    "employer_logo": null,
    "employer_website": "https://www.tamaray.com",
    "job_publisher": "Glassdoor",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.glassdoor.com/job-listing/data-engineer-python-etl-sql-hybrid-tamaray-people-solutions-corp-JV_IC2340354_KO0,35_KE36,65.htm?jl=1009803230662&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-python-etl-sql-hybrid-tamaray-people-solutions-corp-JV_IC2340354_KO0,35_KE36,65.htm?jl=1009803230662&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "DUTIES AND RESPONSIBILITIES:\n\u2022 Design, build, and maintain scalable, secure data pipelines and storage systems; ensure data quality through ETL processes and regular checks.\n\u2022 Implement policies and practices to control, optimize, and secure data assets, ensuring data integrity and accessibility.\n\u2022 Develop and maintain data models, structures, and databases to meet business needs; communicate data architecture effectively.\n\u2022 Develop, test, and maintain scripts and programs to automate data processing and pipelines, adhering to industry standards.\n\u2022 Create and operationalize data visualization solutions to simplify complex data for stakeholders and decision-making.\n\u2022 Work with cross-functional teams to gather data requirements, optimize existing processes, and deliver ad hoc reports and insights\n\nJOB SPECIFICATIONS:\n\u2022 Graduate with a Bachelor\u2019s or Master's Degree in IT, Computer Science, Engineering, or any related course.\n\u2022 At least 2 years of experience in data engineering, data analytics, or related fields.\n\u2022 Proficiency in Python\n\u2022 Hands-on experience with data manipulation tools (e.g., pandas, dplyr, or Spark).\n\u2022 Proficiency in Extract, Transform, and Load (ETL) processes for efficient data pipeline management.\n\u2022 Expertise in both SQL and NoSQL query languages for database interaction and management.\n\u2022 Experience with big data storage and processing solutions, such as MongoDB, Spark, Hive, Snowflake, Redshift, or similar technologies.\n\u2022 Experience with cloud-based or server-based data processing environments (e.g., AWS, Azure, GCP).\n\nJob Types: Full-time, Permanent\n\nPay: Php45,000.00 - Php55,000.00 per month\n\nBenefits:\n\u2022 Company events\n\u2022 Health insurance\n\u2022 Life insurance\n\u2022 Paid training\n\u2022 Promotion to permanent employee\n\u2022 Work from home\n\nSchedule:\n\u2022 8 hour shift\n\u2022 Day shift\n\u2022 Monday to Friday\n\nSupplemental Pay:\n\u2022 13th month salary\n\nAbility to commute/relocate:\n\u2022 Taguig: Reliably commute or planning to relocate before starting work (Preferred)\n\nApplication Question(s):\n\u2022 This is 3x/week onsite in BGC, Taguig. Are you amenable?\n\nEducation:\n\u2022 Bachelor's (Preferred)\n\nExperience:\n\u2022 Data Engineer: 2 years (Preferred)\n\u2022 Python: 2 years (Preferred)\n\u2022 ETL: 2 years (Preferred)\n\u2022 SQL: 2 years (Preferred)\n\nWork Location: Hybrid remote in Taguig",
    "job_is_remote": false,
    "job_posted_at": "15 days ago",
    "job_posted_at_timestamp": 1751846400,
    "job_posted_at_datetime_utc": "2025-07-07T00:00:00.000Z",
    "job_location": "Taguig, Metro Manila, Philippines",
    "job_city": "Taguig",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.530631699999999,
    "job_longitude": 121.0575482,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVMzXnWB28OOEUSPsAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15114100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "B5FpeM6W7FmlpxxTAAAAAA==",
    "job_title": "Data Engineer / Data Architect",
    "employer_name": "Quantrics Enterprises Inc.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRU3FiPriOsmZGVvPbxJfkBli0vmhi00h7nKW7P&s=0",
    "employer_website": null,
    "job_publisher": "Expertini",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.expertini.com/jobs/job/data-engineer-data-architect-rizal-quantrics-enterprises-inc-649-912532/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Expertini",
        "apply_link": "https://ph.expertini.com/jobs/job/data-engineer-data-architect-rizal-quantrics-enterprises-inc-649-912532/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Drive innovation in cybersecurity, software development, and big data technology as part of our award-winning IT team. You can play a critical role as part of a global agile team for Bell Canada, Canada\u2019s largest telco, media, and tech company.\n\nWe know our success is fueled by our people, so we empower our Qmunity and provide a workplace where you can flourish and grow. We offer premium benefits, including:\n\u2022 Home-based setup\n\u2022 Miscellaneous allowances, performance-based bonus, and yearly increase\n\u2022 HMO from day 1 for you + 2 free dependents\n\u2022 6 months paid maternity; 15 days paternity leave\n\u2022 Company-sponsored training and upskilling, and career growth opportunities!\n\nYou will have the opportunity to...\n\u2022 Lead the design and implementation of robust and scalable pipelines, ETL processes and data integration solutions.\n\u2022 Collaborate with Solution Architects, BI Developers, Data analysts and stakeholders to understand their data requirements, translate business needs into technical specifications.\n\u2022 Identify and resolve complex data-related issues, performance bottlenecks, and data pipelines failures.\n\u2022 Evaluate the performance of existing processes and optimize whenever required\n\nCore Competencies (Must-have Competencies)\n\u2022 3+ years of experience in development using Python\n\u2022 3-5+ years of experience in data integration. Must have solid understanding of data modeling, Data warehousing concepts, and data integration techniques along with ETL development.\n\u2022 3+ years of experience in Web APIs development\n\u2022 3-5+ years of experience in SQL and Data Modelling\n\u2022 1+ year of experience in data level security\n\nComplementary Competencies (Good-to-have Competencies)\n\u2022 SAP BW, MicroStrategy\n\nQUALIFICATIONS\n\nEducational Qualifications\n\u2022 Bachelor\u2019s Degree in Computer Science, Information Technology, Engineering, or related fields\n\nWork Conditions\n\u2022 Work from Home Set-up\n\u2022 Mid-Shift: 4:00 PM \u2013 1:00 AM Manila\n\n#WorkFromHome\n#J-18808-Ljbffr",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1752883200,
    "job_posted_at_datetime_utc": "2025-07-19T00:00:00.000Z",
    "job_location": "Pasay City, Metro Manila, Philippines",
    "job_city": "Pasay City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.5376686,
    "job_longitude": 121.0008137,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DB5FpeM6W7FmlpxxTAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15119900",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "Ws8I9bYnL08zTz_-AAAAAA==",
    "job_title": "Data Engineer - Multinational Bank",
    "employer_name": "Michael Page Philippines",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTZaw-H9AIEnHw8N4L_6YEs9LaETT5ZwSsq4xCa&s=0",
    "employer_website": "https://www.michaelpage.com.ph",
    "job_publisher": "Michael Page Philippines",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.michaelpage.com.ph/job-detail/data-engineer-multinational-bank/ref/jn-092024-6543522?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Michael Page Philippines",
        "apply_link": "https://www.michaelpage.com.ph/job-detail/data-engineer-multinational-bank/ref/jn-092024-6543522?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=8a0f42da91051dac&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.br/job-listing/data-engineer-hybrid-techtiera-corp-JV_KO0,20_KE21,35.htm?jl=1009771853053&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-hybrid-techtiera-JV_KO0,20_KE21,30.htm?jl=1009771853053&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "\u2022 Join a growing global team\n\u2022 Enjoy market-aligned salaries & benefits\n\nAbout Our Client\n\nThe company is a multinational bank operating in global markets, offering diverse financial services to their customers and partners.\n\nJob Description\n\u2022 Design, develop, and optimize complex data pipelines to facilitate seamless data integration from multiple sources into our data warehouse.\n\u2022 Collaborate with data analysts, data scientists, and other stakeholders to understand data requirements and deliver actionable insights.\n\u2022 Implement and maintain data warehousing solutions, ensuring high performance, availability, and reliability of data.\n\u2022 Utilize advanced data modeling and schema design techniques to enhance data quality and accessibility.\n\u2022 Troubleshoot and resolve data-related issues, optimizing ETL processes for efficiency and reliability.\n\u2022 Ensure compliance with industry regulations and data governance frameworks.\n\u2022 Mentor and guide junior data engineers, fostering a culture of continuous improvement and knowledge sharing.\n\u2022 Stay updated with the latest technologies and best practices in data engineering and software development.\n\nThe Successful Applicant\n\u2022 Minimum of 5 years of technical experience in data pipelining and data warehousing. Background in banking, financial services, and shared services is preferred.\n\u2022 Strong understanding of software engineering principles and best practices.\n\u2022 Proficiency in programming languages such as Python, Java, or Scala for data processing and ETL development.\n\u2022 Extensive experience with SQL, including PL/SQL, for data manipulation and querying.\n\u2022 Familiarity with IBM DataStage or similar ETL tools is highly desirable.\n\u2022 Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark) is a plus.\n\u2022 Strong analytical and problem-solving skills with an attention to detail.\n\u2022 Excellent communication skills and the ability to work effectively in a collaborative team environment.\n\u2022 Amenable to work in a mid-shift\n\u2022 Amenable to work in Makati\n\u2022 Amenable to a hybrid work setup\n\nWhat's on Offer\n\u2022 Competitive pay & benefits\n\u2022 Healthcare (HMO, Insurance)\n\u2022 Performance bonus/Performance incentives\n\u2022 Leave incentives & Time Off\n\u2022 Training & development",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Makati City, Metro Manila, Philippines",
    "job_city": "Makati City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.554738899999998,
    "job_longitude": 121.0244362,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DWs8I9bYnL08zTz_-AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "uTNSxvjgp0m6Q28FAAAAAA==",
    "job_title": "Sr. Data Engineer",
    "employer_name": "Dev Partners Philippines",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSYduuTql18PjuyHmhR0agTvUTwtKgG-gGV3WOR&s=0",
    "employer_website": null,
    "job_publisher": "Wellfound",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://wellfound.com/jobs/3318771-sr-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Wellfound",
        "apply_link": "https://wellfound.com/jobs/3318771-sr-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Workday",
        "apply_link": "https://pwc.wd3.myworkdayjobs.com/pt-BR/Global_Experienced_Careers/job/Sr-Data-Engineer_566505WD?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "\ud83d\ude80 Now Hiring: Senior Data Engineer (100% Remote)\n\nWe are looking for a highly skilled Senior Data Engineer with strong expertise in Microsoft Business Central to join our team and help drive data integration, modeling, and analytics capabilities across the organization.\n\nAs a Senior Data Engineer, you will be responsible for:\n\n\ud83d\uddf8 Design, build, and maintain scalable data pipelines and ETL processes integrating Microsoft Business Central with other data platforms.\n\n\ud83d\uddf8 Work closely with business stakeholders to understand data needs and translate them into technical solutions.\n\n\ud83d\uddf8 Develop and optimize data models and database architectures to support analytics and reporting.\n\n\ud83d\uddf8 Ensure data quality, security, and governance standards are met across data workflows.\n\n\ud83d\uddf8 Collaborate with cross-functional teams including data analysts, engineers, and product managers to deliver timely and actionable data insights.\n\n\ud83d\uddf8Troubleshoot and resolve data-related issues and optimize system performance.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Philippines",
    "job_city": null,
    "job_state": null,
    "job_country": "PH",
    "job_latitude": 12.879721,
    "job_longitude": 121.774017,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuTNSxvjgp0m6Q28FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 70000,
    "job_max_salary": 80000,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "mK0hlY_kQCAgQ12LAAAAAA==",
    "job_title": "Data Engineering Lead",
    "employer_name": "IWG",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQYlOzYM9l2l52bEC2qR25pg-VeSZOhNPsvm_kS&s=0",
    "employer_website": "http://www.iwgplc.com/",
    "job_publisher": "IWG",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.iwgplc.com/jobs/data-engineering-lead-bonifacio-global-city-philippines?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "IWG",
        "apply_link": "https://jobs.iwgplc.com/jobs/data-engineering-lead-bonifacio-global-city-philippines?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=ab4f0d7de391798a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit Philippines",
        "apply_link": "https://www.foundit.com.ph/job/data-engineering-lead-international-workplace-group-plc-philippines-34817953?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the company\n\nIWG has been at the forefront of the flexible workspace revolution for more than 30 years. We have made it possible for organisations and individuals everywhere to take a new approach to the traditional working day. We have over 3,400 locations across over 110 countries allowing millions of people every day to have a great day at work.\n\nOur customers are start-ups, small and medium-sized enterprises, and large multinationals. With unique business goals, people and aspirations. They want workspaces and communities to match their needs. We provide them with choice through our portfolio of brands, covering serviced offices (Regus, Spaces, HQ, Signature and No18), commercial real estate brokerage and managed office solutions.\n\nJoin us at www.iwgplc.com\n\nOverview:\n\nThe Data Engineering Lead is a key leadership role responsible for guiding a team of data engineers, designing and optimizing data architecture, and ensuring the efficient operation of data pipelines. This role involves managing data quality, collaborating with stakeholders to support data-driven decision-making, and setting the strategic direction for the data engineering function. The Data Engineering Lead also ensures compliance with data governance standards, provides technical leadership, and stays informed about emerging data technologies to drive innovation and efficiency.\n\nKey Responsibilities:\n\u2022 Leadership:\n\u2022 Recruit, manage, and mentor a team of data engineers.\n\u2022 Set performance expectations, provide feedback, and foster career development.\n\u2022 Create a positive team culture and promote collaboration.\n\u2022 Data Architecture Design:\n\u2022 Develop and maintain a scalable data architecture, including data ingestion, data pipelines, data warehousing, and data modeling.\n\u2022 Define data governance policies and standards to ensure data quality and compliance.\n\u2022 Data Pipeline Development:\n\u2022 Design and implement robust, efficient data pipelines for structured, semi-structured, and unstructured data.\n\u2022 Optimize data processing and transformation to ensure timely data delivery.\n\u2022 Stakeholder Management:\n\u2022 Collaborate with business stakeholders to understand data needs and translate them into actionable data solutions.\n\u2022 Communicate data insights and findings to key decision-makers.\n\u2022 Technology Evaluation and Innovation:\n\u2022 Stay abreast of emerging data technologies and tools.\n\u2022 Evaluate and implement new technologies to improve data processing efficiency and scalability.\n\u2022 Performance Monitoring and Optimization:\n\u2022 Monitor data pipeline performance and identify areas for improvement.\n\u2022 Implement performance optimization strategies to address data bottlenecks.\n\nRequired Skills and Experience:\n\u2022 Graduate of any IT or Computer related course preferably Engineering, Business Intelligence, Computer Science or Statistics related\n\u2022 3 to 5 years' experience\n\u2022 Strong understanding of data engineering principles, including data warehousing, data modeling, ETL/ELT processes.\n\u2022 Expertise in various data technologies (e.g. Spark, Kafka, SQL, NoSQL databases)\n\u2022 Experience with cloud computing platforms (Azure, AWS, GCP)\n\u2022 Proven leadership skills in managing and mentoring technical teams\n\u2022 Excellent communication and collaboration skills to work effectively with cross-functional teams\n\nTechnical Skills Candidate must have:\n\u2022 MS SSIS or any other Microsoft tool for data integration\n\u2022 SQL (Advanced skills) and Spark\n\u2022 Power BI and Excel (using Power Query)\n\u2022 Python",
    "job_is_remote": false,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Philippines",
    "job_city": null,
    "job_state": null,
    "job_country": "PH",
    "job_latitude": 12.879721,
    "job_longitude": 121.774017,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DmK0hlY_kQCAgQ12LAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "V3I9UcijELmuRko3AAAAAA==",
    "job_title": "DATA ENGINEER",
    "employer_name": "Unilab, Inc.",
    "employer_logo": null,
    "employer_website": "https://www.unilab.com.ph/",
    "job_publisher": "Careers | Unilab",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.unilab.com.ph/job/MANDALUYONG-CITY-DATA-ENGINEER-METR/1210770401/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers | Unilab",
        "apply_link": "https://careers.unilab.com.ph/job/MANDALUYONG-CITY-DATA-ENGINEER-METR/1210770401/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=70ca3e155ee81d06&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-unilab-JV_IC4747401_KO0,13_KE14,20.htm?jl=1009781261604&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Philippines",
        "apply_link": "https://ph.linkedin.com/jobs/view/data-engineer-at-unilab-inc-4250883335?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs At Stratpoint - Trakstar",
        "apply_link": "https://stratpoint.hire.trakstar.com/jobs/fk0px4x?apply=true&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.br/job-listing/data-engineer-unilab-JV_IC4747401_KO0,13_KE14,20.htm?jl=1009781261604&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.mx/job-listing/data-engineer-unilab-JV_IC4747401_KO0,13_KE14,20.htm?jl=1009781261604&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://ph.jobstreet.com/job/84987443?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "It is the spirit of Bayanihan that drives us to continue our legacy of excellence and commitment to care. As an organization, we achieve our successes through good, honest, and persevering hard work - TOGETHER. It is in this way in which our company was built; we progressed as the country's leading Pharmaceutical company, not by sheer luck, but by pure perseverance, integrity, and brotherhood. Grow with us, and be a part of the Bayanihan spirit.\n\nRole Overview:\n\u2022 The Data Engineer is responsible for translating the analytics requirements of the Grupong Tamang Alaga IT & Analytics Team into technical plans and execution. Responsibilities include, but may not be limited to:\n- Acquire and process datasets required by the business\n- Build, test and maintain optimal data pipeline architectures\n- Identify, design and implement internal process improvements\n- Develop algorithms to transform data into useful and actionable information\n- Monitors data update and resolves data issues\n- Collaborate with other internal IT groups, management and external parties\n- Ensure compliance with data governance and security policies\n\u2022 The role will also be responsible for data engineering-related initiatives worked together with Unilab Corporate IT's Data Team, as well as projects that are outsourced to third-party providers.\n\nRequired Qualifications:\n\u2022 SOFT SKILLS:\n- Ability to effectively partner with internal CIT/Unilab/Business Unit functional and technical teams\n- Excellent verbal and written communication skills\n- Analytical thinker with creative problem-solving skills and attention to detail.\n\u2022 TECH SKILLS:\n- Bachelor's Degree holder of Computer Science, Engineering, Statistics, Management Information Systems, or other Science and Technology related fields\n- Experience with managing structured data in RDBMS with practitioner level experience with MS SQL (Database and SSIS)\n- Experience with Tableau development is a plus\n- Experience with managing non-structured data, including use of cloud native services from AWS and GCP\n- Certifications for database administration, agile development, and/or cloud services are a plus\n- Awareness level with machine learning and AI\n\u2022 TARGET INDUSTRIES:\n- Consulting companies, Telco, Local conglomerates, Research Firms\n\u2022 ACADEMIC BACKGROUND:\n- Computer Science, Engineering, Statistics, Mangement Information Systems, or other Science and Technology related fields\n\nWe are committed to providing our employees with the best possible experience. As a LEARNING ORGANIZATION, we are eager to support your development and create the most fitting career path for you. As DESIGNERS AND DRIVERS OF INNOVATION, we are keen to provide you with opportunities to positively transform processes that will intensify business growth. As a NURTURING FAMILY, we are passionate about conducting programs that can promote your wellness, and help you be the best that you can be. As BELIEVERS OF OUR PURPOSE, we are and we will always remain earnest in giving meaningful tasks that will keep you delighted and fulfilled - at work and beyond.",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": "Mandaluyong City, Metro Manila, Philippines",
    "job_city": "Mandaluyong City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.576267999999999,
    "job_longitude": 121.03924359999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DV3I9UcijELmuRko3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "ip_KnQGFo1bEqILzAAAAAA==",
    "job_title": "Data Engineer - IoT Cost-Optimisation",
    "employer_name": "ERNI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmW8v3ZkMmIrdreDxMXpjIiAb8PsOJMGrWwwF5&s=0",
    "employer_website": null,
    "job_publisher": "Indeed Job Search",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ph.indeed.com/viewjob?jk=cb26d5bce7ad9aaf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed Job Search",
        "apply_link": "https://ph.indeed.com/viewjob?jk=cb26d5bce7ad9aaf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/philippines/job/full-time/others/remote-data-engineer-sao-paolo-137391453?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the position\n\nFounded in 1994 and headquartered in Switzerland, ERNI is a leading Software Development company with over 800 employees worldwide. Specializing in IT and software engineering, we drive innovation in process and technology. Our first service center in Asia Pacific, located in Metro Manila (Mandaluyong), supports clients across Europe, APAC, the Philippines, and the USA. As we continue to grow, we're looking for passionate and motivated individuals to join our team.\n\nWhy ERNI is the Perfect Place for You:\n\u2022 International Exposure: Work with global clients on cutting-edge projects.\n\u2022 Inclusive Culture: Thrive in a collaborative and diverse work environment.\n\u2022 Career Development: Enjoy continuous learning and professional growth opportunities.\n\nPerks and Benefits:\n\u2022 Career Stability: Enjoy a stable career path with ample project opportunities.\n\u2022 Skill Enhancement: Access free training and certifications.\n\u2022 Baby Basket: To welcome your newborn to the ERNI family.\n\u2022 Fruit Basket: Boost of vitamins during hospitalization.\n\u2022 Office Perks: Enjoy free snacks and coffee.\n\nGrowth and Opportunities:\n\u2022 Free Training: Advance your skills through technical and non-technical training.\n\u2022 Challenging Projects: Engage in complex software projects across MedTech, Industry, Finance, and Transportation.\n\u2022 Supportive Environment: Benefit from a team dedicated to guiding and supporting your success.\n\u2022 Recognition and Advancement: Receive acknowledgment for your efforts and opportunities for promotion.\n\u2022 Open Communication: Experience transparency and value your input in our culture.\n\u2022 Flexibility:\n\u2022 Hybrid Work Setup: Balance remote and in-person work for better work-life integration.\n\nEvents:\n\u2022 Connect and Celebrate: Participate in a variety of events including leisure, summer, family, social, and year-end gatherings.\n\nWhat are our wishes:\n\u2022 3+ years building data solutions on Microsoft Azure (PaaS & serverless) \u2013 most of the following Azure services: IoT Hub, Event Hub, Stream Analytics, Functions, Cosmos DB, Databricks.\n\u2022 Solid SQL and C# knowledge.\n\u2022 Experience squeezing data size/frequency in high-throughput IoT or event-driven systems.\n\u2022 Fluent English; clear, pragmatic communication.\n\nNice-to-have\n\u2022 MQTT / OPC-UA or other device protocols; edge-computing toolchains.\n\u2022 Power BI, Kusto / ADX, or Grafana Loki for observability.\n\u2022 Experience with IoT solutions.\n\nEngagement details:\n\u2022 Timeline: ASAP 30 Sep 2025 (approx. 50\u201360 person-days).\n\u2022 Location: Remote first; on-site D\u00fcsseldorf workshops ad-hoc.\n\u2022 Working in a small agile team: Principal IoT Consultant, KPI Governance Lead, Azure Architect, Embedded developer.\n\u2022 Outcome: Clear cost breakdown and optimisation roadmap for a big IoT system based on Azure.\n\nThe team is mixed from Germany, Switzerland and Spain. And most of the work can be done remotely.\n\nHow can you contribute to the team?\n\u2022 Design & run Azure-native data pipelines that collect, transform and store telemetry at the lowest safe cost.\n\u2022 Profile data volumes and flows from gateway to cloud, spot \u201cchatty\u201d patterns and propose compression, batching or edge-processing fixes.\n\u2022 Build the active cost-monitoring stack (e.g. Cost Management API + Power BI / Grafana) and KPI dashboards demanded by the project deliverables.\n\u2022 Feed your findings into the Health-Check Report and the cost-reduction backlog; quantify \u20ac-savings vs. engineering effort.\n\nEmployment Type: Project-based (possibility to work onsite in Germany for 1 month)\n\nSwitzerland \u00b7 Germany \u00b7 Spain \u00b7 Slovakia \u00b7 Romania \u00b7 Philippines \u00b7 Singapore \u00b7 USA\n\nERNI Development Center Philippines Inc., 9th Floor, Lica Malls Shaw, 500 Shaw Boulevard, 1555, Mandaluyong City, Philippines\n\n+63 5310 1707 | www.betterask.erni | [email protected]",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1752537600,
    "job_posted_at_datetime_utc": "2025-07-15T00:00:00.000Z",
    "job_location": "Mandaluyong City, Metro Manila, Philippines",
    "job_city": "Mandaluyong City",
    "job_state": "Metro Manila",
    "job_country": "PH",
    "job_latitude": 14.576267999999999,
    "job_longitude": 121.03924359999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dip_KnQGFo1bEqILzAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "FWriH--HRVnwoB8JAAAAAA==",
    "job_title": "Data Engineer | Remote | Strong on AWS Glue/AWS Services |   GC-EAD & -EAD(Only Genuine) | W2 Position",
    "employer_name": "URSI Technologies Inc.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS2a9CliPK7OMiRF0tz_mpWj4sKVrMcEj0UMyWG&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/fa29282b-81e1-49d4-9386-188f2360713f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/fa29282b-81e1-49d4-9386-188f2360713f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Title: Data EngineerLocation: Remote RoleDuration: 6+ Months ContractVisa Status: GC-EAD & -EAD ONLY!! (Need Passport Copy f & Passport Number f-EAD & EAD) No H1's/OPT/CPT!! Note: This is a Remote role, but potential office for the quarterly planning meets it s in Pittsburgh or Philadelphia, PA. Roles and Responsibilities:Collaborate closely with cross-functional teams including Data Scientists, Analysts, and Software Engineers to understand data requirements and translate them into efficient solutions using AWS Glue and AWS services.Develop and maintain ETL processes using AWS Glue to facilitate seamless and reliable data extraction, transformation, and loading.Implement robust data security measures and access controls in alignment with company policies and industry best practices.Monitor, troubleshoot, and enhance data pipelines, identifying and resolving performance bottlenecks, data quality issues, and other challenges.Stay up-to-date with the latest advancements in AWS Glue and AWS services, and advocate for their effective utilization within the organization Experience/Minimum RequirementsProven experience 8+ years as a Data Engineer, with a strong emphasis on AWS Glue and AWS services.In-depth understanding of architecture, performance optimization techniques, and best practices.Proficiency in SQL and experience with database design principles.Hands-on expertise in designing, building, and maintaining complex ETL pipelines using and AWS Glue.Familiarity with data warehousing concepts and methodologies.Competence in cloud computing and AWS services, with a focus on data-related services such as S3, Redshift, and Lambda.Proficiency in scripting and programming languages such as Python, Java, or similar.",
    "job_is_remote": true,
    "job_posted_at": "13 hours ago",
    "job_posted_at_timestamp": 1753117200,
    "job_posted_at_datetime_utc": "2025-07-21T17:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DFWriH--HRVnwoB8JAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "VuP-kAKuKrvGJDw8AAAAAA==",
    "job_title": "Data Engineer III (Associate) (Remote)",
    "employer_name": "Jobright.ai",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRc11c46XEF7nUag98qauGWM3tBuQH8ELcz8Cbg&s=0",
    "employer_website": "https://jobright.ai",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-associate-remote-at-jobright-ai-4270055641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-associate-remote-at-jobright-ai-4270055641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs - Alooba",
        "apply_link": "https://jobs.alooba.com/us/job/everi-holdings-inc-data-engineer-iii-remote-842416/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Truemote",
        "apply_link": "https://truemote.com/remote-job/data-engineer-iii-associate-remote-b2435c5f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "USAJobCareer.com",
        "apply_link": "https://usajobcareer.com/jobs/view/data-engineer-iii-associate-remote-1672550.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed Jobs",
        "apply_link": "https://store.joblagii.com/blogs/news/data-engineer-iii-remote-id-15864?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job_Summary:\n\nAstrana Health is seeking a highly motivated Data Engineer III to join their Data - Analytics department. The role involves collaborating with product teams and clinical leaders to manage and enhance data products, ensuring high-quality data models and addressing data quality issues.\n\nResponsibilities:\n\n\u2022 Use data engineering best practices to produce high quality, maximally available data models which are intuitive and trusted by stakeholders.\n\n\u2022 Scope and implement new entities for Astrana\u2019s unified data model.\n\n\u2022 Interfacing with business customers, gathering requirements and developing new datasets in data platform\n\n\u2022 Identifying the data quality issues to address them immediately to provide great user experience\n\n\u2022 Extracting and combining data from various heterogeneous data sources\n\n\u2022 Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets\n\n\u2022 Modelling data and metadata to support machine learning and AI\n\n\u2022 Support integration efforts from acquisitions as necessary.\n\nQualifications:\n\n-Required:\n\n\u2022 Bachelor's degree required in computer science, information technology, or related field\n\n\u2022 Strong understanding of database structures, theories, principles, and practices\n\n\u2022 Working knowledge with programming or scripting languages such as Python, Spark, and SQL\n\n\u2022 Demonstrated track record executing best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, deployment processes, testing, and operations\n\n\u2022 Familiarity with normalized, dimensional, star schema and snowflake schematic models\n\n\u2022 Healthcare domain and data experience\n\n\u2022 Strong written and oral communication skills\n\n\u2022 4+ years of experience working with diverse healthcare data sources, for example, claims, encounters, ADT data, FHIR, EHR data etc.\n\n\u2022 3+ years\u2019 using cloud-based services from AWS, GCP, or Azure\n\n\u2022 2+ years serving data sets to both BI tools and SE applications\n\n-Preferred:\n\n\u2022 Master\u2019s degree in healthcare-related field\n\n\u2022 Working experience with Databricks\n\n\u2022 Databricks/Microsoft Azure Certification is a plus\n\nCompany:\n\nLeading physician-centric, technology-powered, risk-bearing healthcare mgmt. company delivering high quality care in a cost-effective manner Astrana Health has a track record of offering H1B sponsorships.",
    "job_is_remote": true,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1753056000,
    "job_posted_at_datetime_utc": "2025-07-21T00:00:00.000Z",
    "job_location": "Alhambra, CA",
    "job_city": "Alhambra",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.092754899999996,
    "job_longitude": -118.1268211,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVuP-kAKuKrvGJDw8AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree required in computer science, information technology, or related field",
        "Strong understanding of database structures, theories, principles, and practices",
        "Working knowledge with programming or scripting languages such as Python, Spark, and SQL",
        "Demonstrated track record executing best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, deployment processes, testing, and operations",
        "Familiarity with normalized, dimensional, star schema and snowflake schematic models",
        "Healthcare domain and data experience",
        "Strong written and oral communication skills",
        "4+ years of experience working with diverse healthcare data sources, for example, claims, encounters, ADT data, FHIR, EHR data etc",
        "3+ years\u2019 using cloud-based services from AWS, GCP, or Azure",
        "2+ years serving data sets to both BI tools and SE applications",
        "Master\u2019s degree in healthcare-related field",
        "Working experience with Databricks"
      ],
      "Responsibilities": [
        "The role involves collaborating with product teams and clinical leaders to manage and enhance data products, ensuring high-quality data models and addressing data quality issues",
        "Use data engineering best practices to produce high quality, maximally available data models which are intuitive and trusted by stakeholders",
        "Scope and implement new entities for Astrana\u2019s unified data model",
        "Interfacing with business customers, gathering requirements and developing new datasets in data platform",
        "Identifying the data quality issues to address them immediately to provide great user experience",
        "Extracting and combining data from various heterogeneous data sources",
        "Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets",
        "Modelling data and metadata to support machine learning and AI",
        "Support integration efforts from acquisitions as necessary"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "00eCRolQcTBw_16xAAAAAA==",
    "job_title": "Data Engineer - Consultant (Remote)",
    "employer_name": "Releady",
    "employer_logo": null,
    "employer_website": "https://www.releady.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Releady/Job/Data-Engineer-Consultant-(Remote)/-in-Sacramento,CA?jid=c193f93875eb5ad4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/consultant-data-engineer_93b50b06-e8ad-4459-ac66-68a2c9229a93?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Pangian",
        "apply_link": "https://pangian.com/remote/job/data-engineer-consultant-remote-1o?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EarnBetter",
        "apply_link": "https://earnbetter.com/app/job/01JM148F98JPMEKGYAMF1HY8HZ/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "OVERVIEW\n\nThis Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team. You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies. Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure.\n\u2022 Duration: 6+ months contract\n\u2022 Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada. Working hours will be PST. Preference for California.\n\u2022 Rate: $70/hr - $85/hr DOE\n\u2022 **Must be able to work in the United States without sponsorship***\n\nRESPONSIBILITIES\n\u2022 Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)\n\u2022 Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability\n\u2022 Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling\n\u2022 Work with Collibra for data governance, quality assurance, and metadata management\n\u2022 Leverage Refuel.ai for data mastering and Striim for data validation processes\n\u2022 Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows\n\u2022 Collaborate with client stakeholders to translate requirements into effective data solutions\n\u2022 Support data visualization and reporting needs through Tableau\n\u2022 Implement CI/CD practices using Git repositories and modern DevOps tools\n\u2022 Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners\n\nQUALIFICATIONS\n\u2022 Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)\n\u2022 10+ years of experience in data engineering or related roles\n\u2022 Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server\n\u2022 Hands-on experience with dbt Cloud for data transformation and pipeline development\n\u2022 Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse\n\u2022 Knowledge of Data Vault modeling principles and implementation techniques\n\u2022 Experience with data governance and data quality tools, particularly Collibra\n\u2022 Familiarity with data visualization platforms, especially Tableau\n\u2022 Understanding of version control systems (Git, Bitbucket) and CI/CD practices\n\u2022 Experience with scheduling systems like Tidal or Control-M\n\u2022 Working knowledge of Agile methodologies and DevOps principles applied to data pipelines\n\u2022 Preferred Skills:\n\u2022 Experience with data observability platforms and data quality monitoring\n\u2022 Knowledge of Python, R, KNIME, or Alteryx for data science applications\n\u2022 Experience with Refuel.ai and Striim technologies\n\u2022 Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms\n\u2022 Experience with enterprise scheduling tools like Tidal\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or other non-merit factor. We are committed to creating a diverse and inclusive environment for all employees.",
    "job_is_remote": true,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Sacramento, CA",
    "job_city": "Sacramento",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 38.5781342,
    "job_longitude": -121.4944209,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D00eCRolQcTBw_16xAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 70,
    "job_max_salary": 85,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "**Must be able to work in the United States without sponsorship***",
        "Bachelor's degree in Computer Science, Information Systems, or related field (or equivalent experience)",
        "10+ years of experience in data engineering or related roles",
        "Strong proficiency in SQL and database technologies including Snowflake, Oracle, and SQL Server",
        "Hands-on experience with dbt Cloud for data transformation and pipeline development",
        "Demonstrated experience with Azure cloud technologies, particularly ADLS and Synapse",
        "Knowledge of Data Vault modeling principles and implementation techniques",
        "Experience with data governance and data quality tools, particularly Collibra",
        "Familiarity with data visualization platforms, especially Tableau",
        "Understanding of version control systems (Git, Bitbucket) and CI/CD practices",
        "Experience with scheduling systems like Tidal or Control-M",
        "Working knowledge of Agile methodologies and DevOps principles applied to data pipelines",
        "Experience with data observability platforms and data quality monitoring",
        "Knowledge of Python, R, KNIME, or Alteryx for data science applications",
        "Experience with Refuel.ai and Striim technologies",
        "Background in data migration from traditional databases (Oracle, SQL Server) to cloud platforms",
        "Experience with enterprise scheduling tools like Tidal"
      ],
      "Benefits": [
        "Rate: $70/hr - $85/hr DOE"
      ],
      "Responsibilities": [
        "This Data Engineer Consultant position is with a healthcare insurance industry client where you'll join their Data Engineering development team",
        "You'll be responsible for the design, development, testing, and deployment of enterprise data solutions using both on-premises and cloud technologies",
        "Reporting to the client's Manager of Data Engineering Development, you'll build and maintain data pipelines and warehousing solutions utilizing their modern cloud-based tech stack centered around Snowflake, dbt Cloud, and Azure",
        "Duration: 6+ months contract",
        "Location: Remote, but must reside in California, Arizona, Washington, Oregon, Nevada",
        "Working hours will be PST. Preference for California",
        "Design, develop, and implement data integration pipelines and data warehouse solutions using Snowflake, dbt Cloud, and Azure technologies (ADLS, Synapse)",
        "Build and optimize production-ready data workflows, ensuring high performance, reliability, and scalability",
        "Create and maintain data pipelines following Data Vault architectural principles for warehouse modeling",
        "Work with Collibra for data governance, quality assurance, and metadata management",
        "Leverage Refuel.ai for data mastering and Striim for data validation processes",
        "Assist in troubleshooting and resolving data pipeline issues by analyzing end-to-end workflows",
        "Collaborate with client stakeholders to translate requirements into effective data solutions",
        "Support data visualization and reporting needs through Tableau",
        "Implement CI/CD practices using Git repositories and modern DevOps tools",
        "Participate in an Agile/DevSecOps pod model alongside solution architects, data modelers, analysts, and business partners"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "uINRzsWvsKKLdrOtAAAAAA==",
    "job_title": "URGENT by 6/25:REMOTE:Certified Data Engineer-Databricks, PySpark/Scala, ADFactory, W2 Only",
    "employer_name": "Solitsys",
    "employer_logo": null,
    "employer_website": "http://www.solitsys.com",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time and Contractor",
    "job_employment_types": [
      "FULLTIME",
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=fb41b59e0542a620&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "URGENTLY NEEDED: REMOTE DATA ENGINEER- DATA BRICKS , PySpark/Scala, AZURE DATA FACTORY [W2 ONLY, ALL Corp-to-Corp WILL BE REJECTED]\n\nWe cannot offer Corp-to-Corp arrangement. This position is being offered on W2 basis only (no Corp-to-Corp or 1099), we are NOT a head-hunting agency. Please respond ASAP with your detailed resume in Word format. Resume must address the minimum qualifications listed below.\n\nDATA ENGINEER - DATABRICKS SPECIALIST\n\nAre you a skilled Data Engineer with a passion for modernizing data solutions? We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity. This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity. Time-sensitive, project-based with defined deliverables. Apply now!\n\nYOUR RESUME MUST SHOW THESE MINIMUM QUALIFICATIONS:\n\u2022 3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling.\n\u2022 5+ years background: PySpark or Scala for data engineering tasks.\n\u2022 5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning.\n\u2022 5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF).\n\u2022 3+ years of: Delta Lake architecture and implementation for data warehousing.\n\u2022 3+ years of experience with: BI tools such as Power BI or Tableau for reporting.\n\u2022 Familiarity with: Azure Cloud environment (Blob Storage, ADLS).\n\u2022 Proficiency in: Python scripting for data manipulation and automation.\n\nEducation & Certifications:\n\u2022 Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or related field.\n\u2022 Databricks Certified Data Engineer Associate or relevant industry certifications.\n\u2022 Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields.\n\nJob Types: Full-time, Contract\n\nPay: $60,000.00 per year\n\nCompensation Package:\n\u2022 Hourly pay\n\nSchedule:\n\u2022 8 hour shift\n\nEducation:\n\u2022 Bachelor's (Required)\n\nExperience:\n\u2022 Databricks: 3 years (Required)\n\u2022 Python: 5 years (Required)\n\u2022 PySpark: 5 years (Required)\n\u2022 Scala: 5 years (Required)\n\u2022 Azure Data Lake: 3 years (Required)\n\u2022 Azure Data Factory: 3 years (Required)\n\u2022 Power BI: 5 years (Preferred)\n\u2022 Tableau: 5 years (Preferred)\n\nWork Location: Remote",
    "job_is_remote": true,
    "job_posted_at": "27 days ago",
    "job_posted_at_timestamp": 1750809600,
    "job_posted_at_datetime_utc": "2025-06-25T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuINRzsWvsKKLdrOtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Resume must address the minimum qualifications listed below",
        "3+ years hands-on: Databricks workspace management, cluster configuration & optimization, job scheduling",
        "5+ years background: PySpark or Scala for data engineering tasks",
        "5+ years demonstrated success: SQL optimization, advanced querying, data modeling, performance tuning",
        "5+ years experience: Building efficient ETL pipelines, using Azure Data Factory (ADF)",
        "3+ years of: Delta Lake architecture and implementation for data warehousing",
        "3+ years of experience with: BI tools such as Power BI or Tableau for reporting",
        "Familiarity with: Azure Cloud environment (Blob Storage, ADLS)",
        "Proficiency in: Python scripting for data manipulation and automation",
        "Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or related field",
        "Databricks Certified Data Engineer Associate or relevant industry certifications",
        "Certifications in Databricks, Azure Cloud, Data Engineering, or similar fields",
        "Bachelor's (Required)",
        "Databricks: 3 years (Required)",
        "Python: 5 years (Required)",
        "PySpark: 5 years (Required)",
        "Scala: 5 years (Required)",
        "Azure Data Lake: 3 years (Required)",
        "Azure Data Factory: 3 years (Required)"
      ],
      "Benefits": [
        "Pay: $60,000.00 per year",
        "Compensation Package:",
        "Hourly pay",
        "8 hour shift"
      ],
      "Responsibilities": [
        "We're looking for an expert to accelerate our data initiatives, optimize pipelines, and ensure data integrity",
        "This is a critical role to accelerate data modernization, convert legacy reports, improve pipeline efficiency, ensure data integrity",
        "Time-sensitive, project-based with defined deliverables"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "PrsQLoxA4Q6wEGlDAAAAAA==",
    "job_title": "Senior/Lead Data Engineer , Remote, $63/HR",
    "employer_name": "FASTRA LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4AyPlflVLMXHqcj8rlWeGkNh8rDl0qeg-V8eO&s=0",
    "employer_website": null,
    "job_publisher": "Dice",
    "job_employment_type": "Contractor",
    "job_employment_types": [
      "CONTRACTOR",
      "CONTRACTOR"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/ad401eb1-1ad4-43b0-a278-e84c25ea2ed7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Quality Contracts",
        "apply_link": "https://qualitycontracts.co.uk/contract-jobs/remote-senior-lead-data-engineer-remote-63-hr-in-usa-1752741389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior/Lead Data Engineer\n\nLocation: Remote\n\nDuration: 12 Months\n\nResponsibilities:\n\u2022 Collaborate with cross-functional stakeholders (including data scientists, analysts, and business teams) to define data requirements, processing needs, and analytics solutions.\n\u2022 Create and sustain scalable data models and efficient extraction processes.\n\u2022 Establish and monitor data quality checks and validation systems.\n\u2022 Construct and enhance business intelligence dashboards.\n\u2022 Produce comprehensive documentation for data models and processes.\n\u2022 Adapt to shifting priorities and ad-hoc requests.\n\u2022 Drive continuous improvement in data practices and visualization techniques.\n\nRequirements:\n\u2022 5+ years of proven experience in database engineering and software development.\n\u2022 Advanced skills in SQL for complex query development and database management.\n\u2022 Strong Python programming skills for automation and data processing workflows.\n\u2022 Ability to handle large-scale data processing tasks using Spark.\n\u2022 Knowledge of data visualization tools, Qlik is preferred.\n\u2022 Familiarity with cloud platforms, particularly AWS (Glue and Athena desired).\n\u2022 Growth mindset with enthusiasm to learn new technologies.",
    "job_is_remote": true,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPrsQLoxA4Q6wEGlDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "GsDGmxGZgv_H39E5AAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Nike",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTBuieX_9ud1sNvwsldw07_VlE1vE0QqXp1Insm&s=0",
    "employer_website": "https://about.nike.com",
    "job_publisher": "Nike Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.nike.com/data-engineer/job/R-61137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Nike Careers",
        "apply_link": "https://careers.nike.com/data-engineer/job/R-61137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Cardinal Health",
        "apply_link": "https://jobs.cardinalhealth.com/search/jobdetails/data-engineer/610d0c2e-65a6-4123-82b5-db5a51021d33?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Workday",
        "apply_link": "https://pnc.wd5.myworkdayjobs.com/External/job/OH---Strongsville/Technology-Engineer---Data-and-Automation--Python--R--Unix-_R193407-1/apply?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Wd3.Myworkdaysite.com",
        "apply_link": "https://wd3.myworkdaysite.com/recruiting/magna/Magna/job/Lowell-Massachusetts-US/Data-Engineer_R00164181?source=BuiltInNationwide&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "StarSevenSix",
        "apply_link": "https://starsevensix.com/careers/data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "TigerData",
        "apply_link": "https://www.tigerdata.com/careers/29788682-de5c-45d8-a0b8-9603c833a8d8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Viasat Careers",
        "apply_link": "https://careers.viasat.com/jobs/4560?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Jobvite",
        "apply_link": "https://jobs.jobvite.com/uplight/job/o5ilwfwY?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Nike Inc.- Beaverton, OR. Design and build simple reusable components of larger process or framework to support analytics products with mentorship from experienced peers; design and implement product features in collaboration with Business and Technology partners;; clean, prepare and optimize data at scale for ingestion and consumption; support the implementation of new data management projects and re-structure of the current data architecture; implement automated workflows and routines using workflow scheduling tools; understand and use continuous integration, test driven development and production deployment frameworks; Anticipate, identify and solve issues concerning data management to improve data quality. participate in design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards; analyze and profile data for the purpose of designing scalable solutions; and solve straightforward data issues and perform root cause analysis to proactively resolve product issues. Telecommuting is available from anywhere in the U.S., except from AK, AL, AR, DE, HI, IA, ID, IN, KS, KY, LA, MT, ND, NE, NH, NM, NV, OH, OK, RI, SD, VT, WV, and WY.\n\nMust have a Master's degree in Computer Science, Engineering, Electronics Engineering and 2 years of experience in the job offered or in a data-related occupation. Experience must include;\n\n\u2022 System development life cycle\n\n\u2022 Cloud platforms, such as AWS and Databricks\n\n\u2022 SQL\n\n\u2022 Version control and CI/CD pipelines\n\n\u2022 Extracting, transforming, and loading and data pipelines\n\n\u2022 Scripting and automation\n\n\u2022 Big data technologies, such as Hadoop, and Spark\n\n\u2022 Data Warehouse concepts and methodologies\n\n\u2022 Relational and nonrelational database design\n\n\u2022 Programming languages, such as Python, Java, and Scala\n\nApply at www.Nike.com/Careers (Job #R-61137)\n\n#LI-DNI\n\nWe offer a number of accommodations to complete our interview process including screen readers, sign language interpreters, accessible and single location for in-person interviews, closed captioning, and other reasonable modifications as needed. If you discover, as you navigate our application process, that you need assistance or an accommodation due to a disability, please complete the Candidate Accommodation Request Form.",
    "job_is_remote": true,
    "job_posted_at": "18 days ago",
    "job_posted_at_timestamp": 1751587200,
    "job_posted_at_datetime_utc": "2025-07-04T00:00:00.000Z",
    "job_location": "Beaverton, OR",
    "job_city": "Beaverton",
    "job_state": "Oregon",
    "job_country": "US",
    "job_latitude": 45.486928299999995,
    "job_longitude": -122.80403199999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DGsDGmxGZgv_H39E5AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Must have a Master's degree in Computer Science, Engineering, Electronics Engineering and 2 years of experience in the job offered or in a data-related occupation",
        "System development life cycle",
        "Cloud platforms, such as AWS and Databricks",
        "SQL",
        "Version control and CI/CD pipelines",
        "Extracting, transforming, and loading and data pipelines",
        "Scripting and automation",
        "Big data technologies, such as Hadoop, and Spark",
        "Data Warehouse concepts and methodologies",
        "Relational and nonrelational database design",
        "Programming languages, such as Python, Java, and Scala"
      ],
      "Responsibilities": [
        "Design and build simple reusable components of larger process or framework to support analytics products with mentorship from experienced peers; design and implement product features in collaboration with Business and Technology partners;; clean, prepare and optimize data at scale for ingestion and consumption; support the implementation of new data management projects and re-structure of the current data architecture; implement automated workflows and routines using workflow scheduling tools; understand and use continuous integration, test driven development and production deployment frameworks; Anticipate, identify and solve issues concerning data management to improve data quality",
        "participate in design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards; analyze and profile data for the purpose of designing scalable solutions; and solve straightforward data issues and perform root cause analysis to proactively resolve product issues"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "6UkieosGdj3DK2hqAAAAAA==",
    "job_title": "Data Engineer - Growth Insights and Foundations",
    "employer_name": "Netflix",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQljIThV_aNQmMjwmu4kEPJEPGF3VYelRwpmpum&s=0",
    "employer_website": "https://about.netflix.com",
    "job_publisher": "Remote Rocketship",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Remote Rocketship",
        "apply_link": "https://www.remoterocketship.com/company/netflix-2/jobs/data-engineer-growth-insights-and-foundations-united-states?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description:\n\u2022 Partner closely with data scientists and other engineers to build low-latency data products\n\u2022 Ensure the availability of critical data to enhance \"in-the-moment\" experiences\n\u2022 Develop highly available and reliable distributed data systems and services\n\u2022 Optimize for the best customer experience with data insights\n\u2022 Ensure timely delivery of high-quality data for Netflix product\n\nRequirements:\n\u2022 Proficient in at least one major language preferably on the JVM stack (e.g., Java, Scala) and SQL (any variant)\n\u2022 Strive to write elegant and maintainable code\n\u2022 Comfortable with picking up new technologies\n\u2022 Have a product mindset and are curious to understand the business's needs\n\u2022 Naturally collaborative style to work with product management, data science, engineering, etc.\n\u2022 Strong data intuition and know how to apply analytical skills to support building high quality data products\n\u2022 Experience building applications that use large-scale distributed systems and data processing frameworks (batch and real-time)\n\u2022 Passionate about making data available for self-service and wider integration\n\u2022 Knowledge about transport protocols and building APIs/services and frameworks (e.g. Spring, gRPC)\n\u2022 Experience in supporting and maintaining products that run 24x7\n\u2022 Can craft scalable systems and solutions to realize a range of product and engineering goals\n\u2022 Strong operational awareness and design multi-tenant systems handling high-scale demands\n\u2022 Prioritize observability in designs with comprehensive monitoring, logging, and alerting\n\u2022 Own what you build and have a passion for quality\n\u2022 Comfortable working in agile environments with vague requirements\n\u2022 Nimble and can pivot easily when needed\n\u2022 Unafraid to take smart risks\n\nBenefits:\n\u2022 Health Plans\n\u2022 Mental Health support\n\u2022 401(k) Retirement Plan with employer match\n\u2022 Stock Option Program\n\u2022 Disability Programs\n\u2022 Health Savings and Flexible Spending Accounts\n\u2022 Family-forming benefits\n\u2022 Life and Serious Injury Benefits\n\u2022 Paid leave of absence programs\n\u2022 Paid time off",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6UkieosGdj3DK2hqAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 170000,
    "job_max_salary": 720000,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "j0uGbWgrvId1fahdAAAAAA==",
    "job_title": "Data Engineer (Full-Time, Remote, North Carolina Based)",
    "employer_name": "Alliance Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRhPb_eu7pJl1gzkeLNRLOVuqLa0EIv62YtXd0S&s=0",
    "employer_website": "https://www.alliancehealthplan.org",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=72cf05daf772f45c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Alliance/Job/Data-Engineer-(Full-Time,-Remote,-North-Carolina-Based)/-in-Morrisville,NC?jid=63c088f9feb31195&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-full-time-remote-north-carolina-based-alliance-health-JV_KO0,51_KE52,67.htm?jl=1009695680692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-full-time-remote-north-carolina-based-at-alliance-health-4256300763?refId=ZZ8HCMbeJKzEURTwY97AHQ%3D%3D&trackingId=Ty0o5ZD5tUD13LeFLBMwKA%3D%3D&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/685c8729de1ebd49c8be30eb-data-engineer-full-time-remote-north-carolina-based?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5274727037?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://vividhireio.com/job/606346?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Kinetichires",
        "apply_link": "https://kinetichires.net/job/data-analyst-diversity-equity-inclusion-and-health-equity-full-time-remote-north-carolina-based-264200?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages. The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities.\n\nThis position is fulltime remote. Selected candidate must reside in North Carolina. Some travel for onsite meetings to the Home office at Morrisville may be required.\n\nResponsibilities & Duties\n\u2022 Analyze business and technical requirements for the design of data integration solutions\n\u2022 Define the overall data integration and dataflow architectures to support data integration projects\n\u2022 Design and develop SQL and SSIS processes to support data integration projects\n\u2022 Design and develop APIs to consume and distribute healthcare data\n\u2022 Design, develop and execute unit testing plans\n\u2022 Ensure data quality and integrity in all data integration projects\n\u2022 Develop technical and business process documentation for data integration projects\n\u2022 Maintain and continually improve data integration projects\n\u2022 Assist in establishing standards for the design, development, implementation and support of data integration projects\n\u2022 Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders\n\u2022 Any other tasks as reasonably required\n\nMinimum Requirements\n\nEducation & Experience:\n\u2022 Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position.\n\u2022 Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis.\n\nPreferred\n\u2022 Bachelor\u2019s degree plus five (5) years of experience in a computer science related field including experience in a data integration or ETL development position including developing complex data integration software applications.\n\u2022 Microsoft Certified Solutions Expert, MuleSoft Certified Developer and/or HL7 Certifications.\n\nKnowledge, Skills, & Abilities\n\u2022 Expert programming in SQL\n\u2022 Proficient designing and developing ETL processes, preferably using SSIS\n\u2022 Proficient designing and developing APIs, preferably using .NET Framework\n\u2022 Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI\n\u2022 Experience working with API management and data integration platforms such as Apigee or MuleSoft\n\u2022 Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP\n\u2022 Experience working with HIEs and/or HISPs\n\u2022 Strong communication and organizational skills\n\u2022 Ability to access and analyze large data sets for completeness and quality\n\u2022 Ability to work independently and in a team setting\n\nEmployment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date.\n\nSalary Range\n\n$102,424-$130,591/Annually\n\nExact compensation will be determined based on the candidate's education, experience, external market data and consideration of internal equity.\n\nAn excellent fringe benefit package accompanies the salary, which includes:\n\u2022 Medical, Dental, Vision, Life, Long Term Disability\n\u2022 Generous retirement savings plan\n\u2022 Flexible work schedules including hybrid/remote options\n\u2022 Paid time off including vacation, sick leave, holiday, management leave\n\u2022 Dress flexibility\n\nEqual Opportunity Employer\nThis employer is required to notify all applicants of their rights pursuant to federal employment laws. For further information, please review the Know Your Rights notice from the Department of Labor.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "North Carolina",
    "job_city": null,
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.7595731,
    "job_longitude": -79.01929969999999,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dj0uGbWgrvId1fahdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 102424,
    "job_max_salary": 130591,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Selected candidate must reside in North Carolina",
        "Graduation from a Community College or Technical School with a major in Information Technology or related field and six (6) years of experience in a computer science related field including experience in a data integration or ETL development position",
        "Military experience and education in the field of work related to the position's role may be substituted on a year-for-year basis",
        "Knowledge, Skills, & Abilities",
        "Expert programming in SQL",
        "Proficient designing and developing ETL processes, preferably using SSIS",
        "Proficient designing and developing APIs, preferably using .NET Framework",
        "Experience with healthcare interoperability tools and protocols, including FHIR, HL7, CDA and EDI",
        "Experience working with API management and data integration platforms such as Apigee or MuleSoft",
        "Experience with healthcare data, including CMS-1500, UB-04, EDI 837 and NCPDP",
        "Experience working with HIEs and/or HISPs",
        "Strong communication and organizational skills",
        "Ability to access and analyze large data sets for completeness and quality",
        "Ability to work independently and in a team setting",
        "Employment for this position is contingent upon a satisfactory background check and credit check, which will be performed after acceptance of an offer of employment and prior to the employee's start date"
      ],
      "Benefits": [
        "$102,424-$130,591/Annually",
        "An excellent fringe benefit package accompanies the salary, which includes:",
        "Medical, Dental, Vision, Life, Long Term Disability",
        "Generous retirement savings plan",
        "Flexible work schedules including hybrid/remote options",
        "Paid time off including vacation, sick leave, holiday, management leave",
        "Dress flexibility"
      ],
      "Responsibilities": [
        "The Data Engineer is responsible for working collaboratively with different IT roles to design and develop advanced healthcare data interoperability solutions using multiple tools and programming languages",
        "The Data Engineer uses industry standards and best practices to develop data integration solutions that support key strategic organizational priorities",
        "This position is fulltime remote",
        "Analyze business and technical requirements for the design of data integration solutions",
        "Define the overall data integration and dataflow architectures to support data integration projects",
        "Design and develop SQL and SSIS processes to support data integration projects",
        "Design and develop APIs to consume and distribute healthcare data",
        "Design, develop and execute unit testing plans",
        "Ensure data quality and integrity in all data integration projects",
        "Develop technical and business process documentation for data integration projects",
        "Maintain and continually improve data integration projects",
        "Assist in establishing standards for the design, development, implementation and support of data integration projects",
        "Provide data integration support and collaborate on data requirements and needs with internal and external stakeholders",
        "Any other tasks as reasonably required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "7jGiM9T-GjtGDCWIAAAAAA==",
    "job_title": "Principal Data Engineer - Remote US",
    "employer_name": "Seamless.AI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTVw7NbcCd2ZQwIRbtYPEgUPiVfGPYawTZcHVKg&s=0",
    "employer_website": "https://seamless.ai",
    "job_publisher": "Wellfound",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://wellfound.com/jobs/3227170-principal-data-engineer-remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Wellfound",
        "apply_link": "https://wellfound.com/jobs/3227170-principal-data-engineer-remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Seamless.AI/Job/Principal-Data-Engineer-Remote-US/-in-Columbus,OH?jid=d298d1770fe754ee&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=a0a3d5a80823c1a4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/principal-data-engineer-remote-us/4275987?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67c03123b965ace1d468e64f-principal-data-engineer---remote-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Startup Jobs",
        "apply_link": "https://startup.jobs/principal-data-engineer-remote-us-seamlessai-2-6223376?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-us_04bbb179-c9c8-4960-851f-91caa0a4f220?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://jooble.org/jdp/4971728553806184117?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Opportunity:\n\nAt Seamless.AI, we\u2019re seeking a highly skilled and experienced Principal Data Engineer with expertise in Python, Spark, AWS Glue, and other ETL (Extract, Transform, Load) technologies. The ideal candidate will have a proven track record in data acquisition and transformation, as well as experience working with large data sets and applying methodologies for data matching and aggregation. Strong organizational skills and the ability to work independently as a self-starter are essential for this role.\n\nResponsibilities:\n\u2022 Design, develop, and maintain robust and scalable ETL pipelines to acquire, transform, and load data from various sources into our data ecosystem.\n\u2022 Collaborate with cross-functional teams to understand data requirements and develop efficient data acquisition and integration strategies.\n\u2022 Implement data transformation logic using Python and other relevant programming languages and frameworks.\n\u2022 Utilize AWS Glue or similar tools to create and manage ETL jobs, workflows, and data catalogs.\n\u2022 Optimize and tune ETL processes for improved performance and scalability, particularly with large data sets.\n\u2022 Apply methodologies and techniques for data matching, deduplication, and aggregation to ensure data accuracy and quality.\n\u2022 Implement and maintain data governance practices to ensure compliance, data security, and privacy.\n\u2022 Collaborate with the data engineering team to explore and adopt new technologies and tools that enhance the efficiency and effectiveness of data processing.\n\nSkillset:\n\u2022 Strong proficiency in Python and experience with related libraries and frameworks (e.g., pandas, NumPy, PySpark).\n\u2022 Hands-on experience with AWS Glue or similar ETL tools and technologies.\n\u2022 Solid understanding of data modeling, data warehousing, and data architecture principles.\n\u2022 Expertise in working with large data sets, data lakes, and distributed computing frameworks.\n\u2022 Experience developing and training machine learning models.\n\u2022 Strong proficiency in SQL.\n\u2022 Familiarity with data matching, deduplication, and aggregation methodologies.\n\u2022 Experience with data governance, data security, and privacy practices.\n\u2022 Strong problem-solving and analytical skills, with the ability to identify and resolve data-related issues.\n\u2022 Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.\n\u2022 Highly organized and self-motivated, with the ability to manage multiple projects and priorities simultaneously.\n\nEducation and Requirements:\n\u2022 Bachelor's degree in Computer Science, Information Systems, related fields or equivalent years of work experience.\n\u2022 7+ years of experience as a Data Engineer, with a focus on ETL processes and data integration.\n\u2022 Professional experience with Spark and AWS pipeline development required.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Columbus, OH",
    "job_city": "Columbus",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.9625112,
    "job_longitude": -83.00322179999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D7jGiM9T-GjtGDCWIAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate will have a proven track record in data acquisition and transformation, as well as experience working with large data sets and applying methodologies for data matching and aggregation",
        "Strong organizational skills and the ability to work independently as a self-starter are essential for this role",
        "Strong proficiency in Python and experience with related libraries and frameworks (e.g., pandas, NumPy, PySpark)",
        "Hands-on experience with AWS Glue or similar ETL tools and technologies",
        "Solid understanding of data modeling, data warehousing, and data architecture principles",
        "Expertise in working with large data sets, data lakes, and distributed computing frameworks",
        "Experience developing and training machine learning models",
        "Strong proficiency in SQL",
        "Familiarity with data matching, deduplication, and aggregation methodologies",
        "Experience with data governance, data security, and privacy practices",
        "Strong problem-solving and analytical skills, with the ability to identify and resolve data-related issues",
        "Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams",
        "Highly organized and self-motivated, with the ability to manage multiple projects and priorities simultaneously",
        "Bachelor's degree in Computer Science, Information Systems, related fields or equivalent years of work experience",
        "7+ years of experience as a Data Engineer, with a focus on ETL processes and data integration",
        "Professional experience with Spark and AWS pipeline development required"
      ],
      "Responsibilities": [
        "Design, develop, and maintain robust and scalable ETL pipelines to acquire, transform, and load data from various sources into our data ecosystem",
        "Collaborate with cross-functional teams to understand data requirements and develop efficient data acquisition and integration strategies",
        "Implement data transformation logic using Python and other relevant programming languages and frameworks",
        "Utilize AWS Glue or similar tools to create and manage ETL jobs, workflows, and data catalogs",
        "Optimize and tune ETL processes for improved performance and scalability, particularly with large data sets",
        "Apply methodologies and techniques for data matching, deduplication, and aggregation to ensure data accuracy and quality",
        "Implement and maintain data governance practices to ensure compliance, data security, and privacy",
        "Collaborate with the data engineering team to explore and adopt new technologies and tools that enhance the efficiency and effectiveness of data processing"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "VSDv3_uV9S8qF8byAAAAAA==",
    "job_title": "Google Cloud Platform Data Engineer(Fulltime) for Remote",
    "employer_name": "Amaze Systems Inc",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7sj9aM48iM-XElhKT8EMmjwhDrfGkFy-kqO6e&s=0",
    "employer_website": null,
    "job_publisher": "Dice.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Dice.com",
        "apply_link": "https://www.dice.com/job-detail/6469e9fe-b09c-4967-9896-5b3cd9c4cb56?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Remote Jobs\u2122 - DentaQuest",
        "apply_link": "https://dev-cm.dentaquest.com/job/work-from-home-system-data-analyst-google-cloud-platform-data-fn8ov.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://jobxpedia.com/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Jobxpedia",
        "apply_link": "https://buzzcloud.in/job-google-cloud-platform-data-analyst-hyderabad-2-to-5-355845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Fulltime opportunity\n\nRole: Google Cloud Platform Data Engineer\n\nLocation: Remote - USA\n\nDuration: FTE only\n\nJob Description:\n\u2022 10+ years' proven experience as a Data Engineer with a focus on Google Cloud Platform services.\n\u2022 Strong proficiency in Google Cloud Platform services such as GCS, Dataflow with Apache Beam (Batch & Stream data processing), BigQuery, cloud Composer and Pub/Sub.\n\u2022 Proficiency in SQL and Python for data manipulation and analysis is mandatory.\n\u2022 Solid understanding of data warehousing concepts and ETL processes.\n\nThanks &Regards\n\nRahul Sharma | Lead Technical Recruiter\nAmaze Systems Inc\n\nE: |",
    "job_is_remote": true,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1752883200,
    "job_posted_at_datetime_utc": "2025-07-19T00:00:00.000Z",
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVSDv3_uV9S8qF8byAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "RxeJao3yHoNlD18FAAAAAA==",
    "job_title": "Data Engineer - MLOps | Remote in the USA - 1725",
    "employer_name": "PlacingIT",
    "employer_logo": null,
    "employer_website": "https://www.placingit.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PlacingIT/Job/Data-Engineer-MLOps-%7C-Remote-in-the-USA-1725/-in-Dallas,TX?jid=2436d5410b3fe571&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - MLOps | Remote in the USA - 1725\n\nLocation: Hybrid - Onsite 4 days a week in Dallas, TX\nEmployment Type 6-12+ months contract w/possible extensions\nInterview: In-Person interview is required for this role.\nResidency Requirements: Those authorized to work in the US are encouraged to apply.\n\nRequired Qualifications\n\u2022 5+ years of software development experience building and delivering customer facing cloud-based analytic solutions.\n\u2022 5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives.\n\u2022 5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope.\n\u2022 5+ years coding in Python/PySpark, SQL, and LLM modeling.\n\u2022 5+ years in cloud-based platforms such as Azure, GCP and AWS.\n\nPreferred Qualifications\n\u2022 5+ years of experience leading the design, development, and deployment of applications on Palantir Foundry Platform\n\u2022 Retail merchandising domain expertise including category management, assortment optimization, product clustering, product price sensitivity and promotion affinity evaluation, store clustering and localization.\n\u2022 Experience with Prompt Engineering developing solutions leveraging LLM models.\n\nResponsibilities include:\n\u2022 You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions.\n\u2022 You will solve complex problems and deliver decision support tools to improve customer experience.\n\u2022 Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses.\n\u2022 Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces.\n\u2022 Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses.\n\u2022 Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry.\n\u2022 Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle.\n\u2022 Provide technical guidance, mentorship, and support to junior developers and team members.\n\u2022 Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem.",
    "job_is_remote": true,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1752624000,
    "job_posted_at_datetime_utc": "2025-07-16T00:00:00.000Z",
    "job_location": "Dallas, TX",
    "job_city": "Dallas",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7766642,
    "job_longitude": -96.79698789999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRxeJao3yHoNlD18FAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Residency Requirements: Those authorized to work in the US are encouraged to apply",
        "5+ years of software development experience building and delivering customer facing cloud-based analytic solutions",
        "5+ years of end-to-end application development experience on Palantir foundry or similar systems, leveraging tools such as Workshop, Code Repositories, Pipeline Builder, Ontology objects/actions and Typescript functions Foundry applications or similar systems to meet business requirements and objectives",
        "5+ years of experience collaborating with cross-functional teams including data analysts, engineers, and business stakeholders to gather requirements and define project scope",
        "5+ years coding in Python/PySpark, SQL, and LLM modeling",
        "5+ years in cloud-based platforms such as Azure, GCP and AWS"
      ],
      "Responsibilities": [
        "You will drive revenue and customer satisfaction in the company's business using advanced analytic solutions to optimizing merchandising and pricing decisions",
        "You will solve complex problems and deliver decision support tools to improve customer experience",
        "Collaborate with business stakeholders to understand business needs and convert them into requirements for solution enhancements or ad hoc analyses",
        "Help guide the overall roadmap for enterprise optimal pricing solutions that deploy complex data science models through interactive user interfaces",
        "Work with teams of data scientists, analysts, data engineers, and developers to execute both solution upgrades and analyses",
        "Architect scalable and efficient data pipelines to ingest, transform, and analyze large volumes of structured and unstructured data within Palantir Foundry",
        "Ensure compliance with best practices, security standards, and data governance policies throughout the development lifecycle",
        "Provide technical guidance, mentorship, and support to junior developers and team members",
        "Proactively identify opportunities for process improvements and optimization within Palantir Foundry ecosystem"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": "SjsMVNnmi0YDQr4jAAAAAA==",
    "job_title": "Senior Data Engineer - Full remote",
    "employer_name": "All European Careers",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTfGAmX7q4Hokut7-XE9J8D2NsW-8elqZh_HaX4&s=0",
    "employer_website": "https://www.all-european-careers.com",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/67ecb0263b1cb2ad171c92e8-senior-data-engineer---full-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JOBITT",
        "apply_link": "https://jobitt.com/job-openings/external/senior-engineer-full-stack-7870835690680623803?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: Europe\n\nThis is a remote position.\n\nFor an International Institution in Geneva, we are urgently looking for an experienced Senior Data Engineer preferably with Azure experience in Geneva or Full remote.As Senior Data Engineer, you work with the Cloud Architect, Data Architect, Solution Engineer and other technical professionals to act as technical focal point for the development of prototypes, providing advice and technical feasibility view, and other technical activities to ensure the Cloud data Platform architecture and technical configuration will address business and IT objectives.\n\nCandidates need to be fluent in English. This positions is long-term. Work permit not required.Candidates need to be based in Europe.\n\nTasks and Responsibilities:\n\u2022 Plan the activites and concrete milestones to deliver the new Cloud Data Platform Architecture;\n\u2022 Lead the development and technical implementation aligned to the Data Platform Architecture;\n\u2022 Provide expertise and technical advice in the development of conceptual, logical and physical data models, in support of interoperability protocols, API design and management, cybersecurity aspects and business intelligence;\n\u2022 Create the data factory pipelines to ingest data, apply data transformations to curate data using databricks, and make data available for downstream applications (API) or reporting from SQL database, Cosmos DB or Synapse Analytics;\n\u2022 Automation and development lifecycle control by development of rbooks, fctions, devops projects, sync to repos, ARM templates and Azure Blueprints;\n\u2022 Working with the Senior Solution Engineer derstand and catalogue the current landscape of Data and systems;\n\u2022 Develop and maintain the to-be Cloud Data Platform with the required capabilities in conformance with security requirements, agreements-based, with user-driven configuration and through documented design and configuration;\n\u2022 Provide technical expertise regarding short terms solution options to leverage in the immediate future to meet urgent data related demands, with consideration of the organization\u2019s wider needs and the potential of deploying enterprise data solution platforms at the enterprise level;\n\u2022 Provide data insights and best practices, ensuring they are reflected in the development;\n\nProfile:\n\u2022 Bachelor or Master degree;\n\u2022 +5 years of relevant experience as Data Engineer;\n\u2022 Experience with modern Data technologies such as Business Intelligence, Analytics, AI, and Big Data;\n\u2022 Experience programming multiple languages, e.g. Phyton, R, Java, Scala, etc; Professional level vendor certifications, such as Microsoft, ITIL, and others;\n\u2022 Extensive experience with Microsoft Azure and other cloud technologies and interoperable solutions,experience as a data analyst, engineer and developer;\n\u2022 Demonstrated experience in the design, development, and implementation of various integrations between diverse infrastructure services, data models and architecture;\n\u2022 Demonstrated experience with the IT systems development life cycle (SDLC), as well as Agile/Scrum methodologies and ITIL processes;\n\u2022 Ability to conduct requirements gathering, interpret needs, and design solutions and manage expectations;\n\u2022 Professional experience in technical design and support of global, distributed corporate information systems;\n\u2022 Understanding state of Azure, Google and AWS components and reference architectures;\n\nExperience in designing for Bigdata/data warehousing/business intelligence/reporting solutions for various physical environments, both on-premises as well as in the Microsoft cloud, while influencing for the most efficient approach based on business requirements;\n\nExcellent written and spoken English;\n\nInterested: Please send your resume to:\nresume@all-european-careers.com\n\nThis offer from \"All European Careers\" has been enriched by Jobgether.com and got a 77% flex score.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DSjsMVNnmi0YDQr4jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "L2b_MhwdllUxOf0jAAAAAA==",
    "job_title": "Data Engineers (Hybrid | DC Area) Remote / Telecommute Jobs",
    "employer_name": "Rackner",
    "employer_logo": null,
    "employer_website": "https://rackner.com",
    "job_publisher": "Security Clearance Jobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Security Clearance Jobs",
        "apply_link": "https://www.clearancejobs.com/jobs/8439118/data-engineers-hybrid-dc-area?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Title: Data Engineer\n\nLocation: Falls Church, VA (Hybrid)\n\nClearance: Secret Clearance\n\nAbout this role:\n\nRackner is looking for a Data Engineer that will be working within an Agile DevSecOps team environment using latest cloud-native technologies to architect and implement containerized applications, CI/CD pipelines, and Kubernetes platforms using best practices and leading technologies.\n\nWe are seeking professionals with:\n\nB.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms\n\nModern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)\n\nDeveloping data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules\n\nMature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR\n\nNice to have:\n\nM.S in Computer Science or related\n\nAI/ML\n\nKubernetes (Rancher RKE2, AWS EKS) and microservice architectures\n\nData engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR\n\nNIST Risk Management Framework and security accreditation process and tasks\n\nWhat will make you successful:\n\nUsing DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer\n\nDeveloping software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes\n\nLeveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs\n\nApplying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders\n\nEmbracing a shared responsibility for system security\n\nPerforming threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle\n\nContinuously engaging with project teams to deliver quality products\n\nParticipating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions\n\nWho We Are:\n\nRackner is a software consultancy that builds cloud-native solutions for startups, enterprises, and the public sector.\n\nWe are an energetic, growing consultancy with a passion for solving big problems for both startups and enterprises.\n\nEach of us enable digital transformation for large organizations through the newest in distributed technologies as we are laser focused on end-to-end application development, DevSecOps, AI/ML and systems architecture and our methodology focuses on cloud-first and cost-effective innovation.\n\nOur customers hail from a diverse, ever-growing list of industries.\n\nBenefits/Additional Info:\n\nRackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided . Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!\n\n401K with 100% matching up to 6%\n\nHighly competitive PTO\n\nGreat health insurance with large network of providers\n\nMedical/Dental/Vision\n\nLife Insurance, and short & long term disability\n\nIndustry-Leading Weekly Pay Schedule\n\nHome office & equipment plan\n\n#DataEngineer #AWS #Topsecret #FDA #publictrust #DataIngesting #DataPipeline #Python #Terraform #ETL #AI #ML #dataintegration #bigdataanalyticspipeline #awsbigdata #hadoop #apachespark #RDBMS #awsdynamoDB #collaboration #diversity #equity #Inclusion",
    "job_is_remote": true,
    "job_posted_at": "10 hours ago",
    "job_posted_at_timestamp": 1753128000,
    "job_posted_at_datetime_utc": "2025-07-21T20:00:00.000Z",
    "job_location": "Falls Church, VA",
    "job_city": "Falls Church",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.882334,
    "job_longitude": -77.1710914,
    "job_benefits": [
      "health_insurance",
      "paid_time_off",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DL2b_MhwdllUxOf0jAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "B.S. in Computer Science (or equivalent experience) and at least 3 years professional experience working as part of an Agile team using DevOps practices and modern programming languages and frameworks to develop and engineer data-related services and applications to cloud platforms",
        "Modern programming languages and frameworks including Python (FastAPI, Django, Flask), JavaScript (Node.js, Express, Vue, Svelte, React), and Java (Spring Boot)",
        "Developing data focused solutions to include experience designing and implementing OpenAPI compliant data schemas and REST APIs, implementing data persistence and query logic using Object Relational Mapping (ORM) libraries and SQL, and implementing data validation business rules",
        "Mature AWS cloud solutions with a specific focus on data services such as S3, RDS, and EMR",
        "M.S in Computer Science or related",
        "Kubernetes (Rancher RKE2, AWS EKS) and microservice architectures",
        "Data engineering and big data technologies such as Apache Airflow, Trino, and AWS EMR",
        "NIST Risk Management Framework and security accreditation process and tasks"
      ],
      "Benefits": [
        "Rackner embraces and promotes employee development and training and covers the cost of certifications relevant to a position and the technologies/services provided ",
        "Fitness/Gym membership eligibility, weekly pay schedule and employee swag, snacks & events are offered as well!",
        "401K with 100% matching up to 6%",
        "Highly competitive PTO",
        "Great health insurance with large network of providers",
        "Life Insurance, and short & long term disability",
        "Industry-Leading Weekly Pay Schedule",
        "Home office & equipment plan"
      ],
      "Responsibilities": [
        "Using DevSecOps best practices to rapidly develop and deliver first class solutions for a DoD customer",
        "Developing software using leading languages and frameworks including Python (FastAPI), AWS, and Rancher Kubernetes",
        "Leveraging Test Driven Development and User Centered design best practices automate testing and ensure delivered software meets and exceeds customer needs",
        "Applying software design best practices to identify and analyze potential solutions, generate design diagrams to convey and capture system design, and participate in technical exchange meetings to discuss with the team and customer stakeholders",
        "Embracing a shared responsibility for system security",
        "Performing threat modeling to identify and mitigate system security threats, implement protective and preventive security measures continuously throughout the software development lifecycle",
        "Continuously engaging with project teams to deliver quality products",
        "Participating in daily standups, sprint planning/review meetings, discover and framing workshops, and customer demo sessions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "XswWukx0iLqtemRoAAAAAA==",
    "job_title": "Principal Data Engineer *Remote - Most states eligible*",
    "employer_name": "Providence",
    "employer_logo": null,
    "employer_website": "https://www.providence.org",
    "job_publisher": "Teal",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/principal-data-engineer-remote-most-states-eligible_5e428b59-327a-4612-a9ad-19be161218d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About the position\n\nThe Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making. This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics. The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St. Joseph Health system, focusing on quality and value of care metrics. The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights.\n\nResponsibilities\n\u2022 Collect and analyze clinical data from various sources.\n,\n\u2022 Create visualizations and storytelling using data to communicate findings.\n,\n\u2022 Develop novel reporting platforms to address healthcare challenges.\n,\n\u2022 Collaborate with clinical and administrative leaders to evaluate new measures.\n,\n\u2022 Identify impactful findings in large clinical data stores.\n,\n\u2022 Design strong data visualizations to communicate key information across the organization.\n,\n\u2022 Support team members with experience in computational methods.\n,\n\u2022 Develop innovative methods for outcome measures in the whole person care model.\n\nRequirements\n\u2022 Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience.\n,\n\u2022 9 years of relevant data analysis experience, preferably in a biomedical setting.\n,\n\u2022 Experience with database query and analysis languages (e.g., SQL, R, SAS, Python).\n,\n\u2022 Proficiency in data visualization tools (e.g., Tableau, D3).\n\nNice-to-haves\n\u2022 Ph.D. in a related field or equivalent experience.\n,\n\u2022 Certification in an IT discipline, application, or tool upon hire.\n,\n\u2022 Lean certification or Green Belt, Black Belt upon hire.\n,\n\u2022 Certification in Data Science upon hire.\n,\n\u2022 4+ years of development experience in data exchange using FHIR.\n\nBenefits\n\u2022 401(k) retirement savings plan with employer matching.\n,\n\u2022 Health care benefits (medical, dental, vision).\n,\n\u2022 Life insurance.\n,\n\u2022 Disability insurance.\n,\n\u2022 Paid parental leave.\n,\n\u2022 Vacation and holiday time off.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Renton, WA",
    "job_city": "Renton",
    "job_state": "Washington",
    "job_country": "US",
    "job_latitude": 47.4796927,
    "job_longitude": -122.2079218,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXswWukx0iLqtemRoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Master's Degree in Health Information Management, Computer Science, Mathematics, Business Administration, Healthcare, or a related field, or equivalent experience",
        "9 years of relevant data analysis experience, preferably in a biomedical setting",
        "Experience with database query and analysis languages (e.g., SQL, R, SAS, Python)",
        "Proficiency in data visualization tools (e.g., Tableau, D3)",
        "Ph.D. in a related field or equivalent experience",
        "Certification in an IT discipline, application, or tool upon hire",
        "Lean certification or Green Belt, Black Belt upon hire",
        "Certification in Data Science upon hire",
        "4+ years of development experience in data exchange using FHIR"
      ],
      "Benefits": [
        "401(k) retirement savings plan with employer matching",
        "Health care benefits (medical, dental, vision)",
        "Life insurance",
        "Disability insurance",
        "Paid parental leave",
        "Vacation and holiday time off"
      ],
      "Responsibilities": [
        "The Principal Data Engineer is a subject matter expert (SME) level position responsible for collecting, analyzing, and visualizing clinical data to support healthcare decision-making",
        "This role involves working with diverse data types from millions of clinical encounters, utilizing analytical methods such as visualization science, predictive modeling, and biostatistics",
        "The Engineer will develop innovative reporting platforms and communicate key findings to various stakeholders within the Providence St",
        "The position requires strong collaboration with clinical and administrative leaders to ensure analytical rigor and effective communication of insights",
        "Collect and analyze clinical data from various sources",
        "Create visualizations and storytelling using data to communicate findings",
        "Develop novel reporting platforms to address healthcare challenges",
        "Collaborate with clinical and administrative leaders to evaluate new measures",
        "Identify impactful findings in large clinical data stores",
        "Design strong data visualizations to communicate key information across the organization",
        "Support team members with experience in computational methods",
        "Develop innovative methods for outcome measures in the whole person care model"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "bMI3tTg8CkVNRcoxAAAAAA==",
    "job_title": "AWS Data Engineer - Fully Remote - US Only",
    "employer_name": "Scalepex",
    "employer_logo": null,
    "employer_website": "https://scalepex.com",
    "job_publisher": "Jobs By Workable",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://apply.workable.com/scalepex/j/F5ED805D80?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Jobs By Workable",
        "apply_link": "https://apply.workable.com/scalepex/j/F5ED805D80?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Insight Global Jobs",
        "apply_link": "https://jobs.insightglobal.com/find_a_job/connecticut/job-313330/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ApplicantSite",
        "apply_link": "https://applicants.bairesdev.com/job/190/252655/apply?utm_source=trampedecasa&utm_medium=jobposting&utm_campaign=Remote-20240530?utm_source%3Dtrampedecasautm_medium%3Dreferral&utm_campaign=vaga-publicada-na-trampedecasa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "DailyRemote",
        "apply_link": "https://dailyremote.com/remote-job/aws-data-engineer-fully-remote-us-only-3512285?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Ladders",
        "apply_link": "https://www.theladders.com/job/aws-data-engineer-remote-nava-software-solutions-virtual-travel_82025315?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "\u274b Why Scalepex?\n\nScalepex is a dynamic services firm specializing in providing solutions for premium brands like Nike, Pepsi, Toyota, Virgin and Walgreens. Our mission is to connect prominent market leaders with top-tier professionals from around the world, fostering collaboration, efficiency, and growth.\n\n\u274b Take your portfolio to the next level by working with one of our fastest growing clients.\n\nJoin the Innovation Frontier at Scalepex!\n\nAbout the Role\n\nWe are seeking an experienced AWS Data Engineer with a strong background in building scalable data solutions and expertise in utilities-related datasets. The ideal candidate will have at least 5 years of experience in data engineering, a deep understanding of distributed systems, and proficiency with AWS services and tools like Step Functions, Lambda, Glue, and Redshift. This role will focus on designing, developing, and optimizing data pipelines to support analytics and decision-making in the utilities industry.\n\nKey Responsibilities\n\u2022 Design and Build Data Pipelines: Develop scalable, reliable data pipelines using AWS services (e.g., Glue, S3, Redshift) to process and transform large datasets from utility systems like smart meters or energy grids.\n\u2022 Workflow Orchestration: Use AWS Step Functions to orchestrate workflows across data pipelines; experience with Airflow is acceptable but Step Functions is preferred.\n\u2022 Data Integration and Transformation: Implement ETL/ELT processes using PySpark, Python, and Pandas to clean, transform, and integrate data from multiple sources into unified datasets.\n\u2022 Distributed Systems Expertise: Leverage experience with complex distributed systems to ensure reliability, scalability, and performance in handling large-scale utility data.\n\u2022 Serverless Application Development: Use AWS Lambda functions to build serverless solutions for automating data processing tasks.\n\u2022 Data Modeling for Analytics: Design data models tailored for utilities use cases (e.g., energy consumption forecasting) to enable advanced analytics\n\u2022 Optimize Data Pipelines: Continuously monitor and improve the performance of data pipelines to reduce latency, enhance throughput, and ensure high availability.\n\u2022 Ensure Data Security and Compliance: Implement robust security measures to protect sensitive utility data and ensure compliance with industry regulations.\n\nRequired Qualifications\n\u2022 Minimum of 5 years of experience in data engineering\n\u2022 Proficiency in AWS services such as Step Functions, Lambda, Glue, S3, DynamoDB, and Redshift.\n\u2022 Strong programming skills in Python with experience using PySpark and Pandas for large-scale data processing.\n\u2022 Hands-on experience with distributed systems and scalable architectures.\n\u2022 Knowledge of ETL/ELT processes for integrating diverse datasets into centralized systems.\n\u2022 Familiarity with utilities-specific datasets (e.g., smart meters, energy grids) is highly desirable.\n\u2022 Strong analytical skills with the ability to work on unstructured datasets.\n\u2022 Knowledge of data governance practices to ensure accuracy, consistency, and security of data.\n\u2022 Strong experience in AWS data engineering\n\u2022 Ability to work independently\n\u2022 Ability to work with a cross-functional teams, including interfacing and communicating with business stakeholders\n\u2022 Professional oral and written communication skills\n\u2022 Strong problem solving and troubleshooting skills with experience exercising mature judgement\n\u2022 Excellent teamwork and interpersonal skills\n\u2022 Ability to obtain and maintain the required clearance for this role",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": null,
    "job_city": null,
    "job_state": null,
    "job_country": null,
    "job_latitude": null,
    "job_longitude": null,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DbMI3tTg8CkVNRcoxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "Gg8QqwU2vYyezjEoAAAAAA==",
    "job_title": "Senior Data Engineer, Customer Facing  (US Remote)",
    "employer_name": "LeanTaaS",
    "employer_logo": null,
    "employer_website": "https://leantaas.com",
    "job_publisher": "Built In",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://builtin.com/job/senior-data-engineer-customer-facing-us-remote/3060713?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/senior-data-engineer-customer-facing-us-remote/3060713?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs In USA",
        "apply_link": "https://todayremotejobs.com/job/582664?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remote Jobs USA",
        "apply_link": "https://distantgigsio.com/job/556721?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talenttundra",
        "apply_link": "https://talenttundra.com/job/sr-sql-engineer-healthcare-us-remote-236249?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We are a growth stage company that creates software solutions combining lean principles, predictive and prescriptive analytics, and machine learning to transform hospital and infusion center operations. More than 180 health systems and over 1,000 hospitals and centers across 49 states rely on our award-winning products to increase patient access, decrease wait times, and reduce healthcare delivery costs. We have raised more than $300 million from top-tier investors such as Bain Capital, Insight Partners, and Goldman Sachs. We have been named among the top 100 AI companies in the world.\n\nPlease note that while this role is listed as available for remote, we are currently employing in the following states: AK, AZ, CA, CO, CT, DC, FL, GA, IL, IN, KS, LA, MA, MD, ME, MI, MO, MT, NC, NH, NJ, NY, OH, OR, PA, SC, TN, TX, UT, VA, WA, WI, WV. If your state is not listed, we may not be able to proceed with your application. We have offices in Santa Clara, CA and Charlotte, NC for employees who prefer to work regularly or occasionally from an office.\n\nWe are seeking a Senior Data Engineer to join the team responsible for managing the data integrations of our hospital inpatient SaaS product. Data is what powers our platform and empowers our users. This is a hands-on role that requires skills in engineering, data wrangling, and good old-fashioned customer charm to engage both technical and clinical staff at hospitals.\n\nWHAT YOU\u2019LL DO\n\u2022 Manage, write, and maintain hospital data ingestion SQL queries and configuration in our custom system\n\u2022 Work with hospital IT teams to obtain data extracts and ensure timely integrations\n\u2022 Manage data warehouse fidelity, ensuring stakeholders trust the data and understand both its opportunities and limitations\n\u2022 Ensure client data feeds are operational and error-free\n\u2022 Oversee data specifications, technical designs, and customer-specific glossaries\n\u2022 Work with backend engineering and product to set technical direction for the data ingestion pipeline for performance, scale, and reliability\n\u2022 Assist sales team with pre-sales technical requirements and phased onboarding plans\n\nWHAT YOU\u2019LL BRING\n\u2022 3+ years of SQL experience\n\u2022 Expertise in building relationships and trust with clients\n\u2022 Excellent written and verbal communication skills\n\u2022 Knowledge of hospital data protocols such as HL7 and FHIR\n\u2022 Experience with one or more EHR vendors\n\u2022 Experience working in or with hospital IT teams\n\u2022 Empathy for our users\n\nBONUS POINTS IF YOU HAVE\n\u2022 Clinical or direct patient care experience\n\u2022 Python or similar programming languages\n\nWHAT YOU'LL GET\n\u2022 Intellectual and emotional satisfaction of solving tough operational problems in healthcare while improving patient access and saving lives!\n\u2022 Competitive compensation package that includes base salary, target bonus, and stock options\n\u2022 401(k) Match\n\u2022 Comprehensive healthcare benefits\n\u2022 Generous Paid Time Off and Parental Leave\n\u2022 Monthly reimbursement for Skill Building\n\u2022 Monthly reimbursement for Wellness, Transportation, and/or Home Office\n\u2022 Education Reimbursement for select courses/programs\n\nVaccination policy\n\nWe have an obligation to protect our employees, our customers, and the patients of our customers. Therefore, COVID-19 vaccination is required to work from the office, attend in-person company events, or to travel on behalf of the company.\n\nCandidates must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position.\n\nLeanTaaS is an equal opportunity employer committed to promoting an inclusive work environment free of discrimination and harassment. We value diversity, inclusion, and aim to provide a sense of belonging for everyone. All qualified applicants for employment will be considered without regard to race, color, sex, gender identity, gender expression, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other basis protected by federal, state or local laws. If you require assistance during the application process, please reach out to accommodations@leantaas.com. LeanTaaS will reasonably accommodate qualified individuals with disabilities to the extent required by applicable law.\n\nPlease note: LeanTaaS is not accepting agency resumes at this time, and we are not responsible for any fees related to unsolicited resumes. Thank you.",
    "job_is_remote": true,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DGg8QqwU2vYyezjEoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 85000,
    "job_max_salary": 145000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "3+ years of SQL experience",
        "Expertise in building relationships and trust with clients",
        "Excellent written and verbal communication skills",
        "Knowledge of hospital data protocols such as HL7 and FHIR",
        "Experience with one or more EHR vendors",
        "Experience working in or with hospital IT teams",
        "Empathy for our users",
        "Clinical or direct patient care experience",
        "Python or similar programming languages",
        "Candidates must be legally authorized to work in the United States",
        "Verification of employment eligibility will be required at the time of hire"
      ],
      "Benefits": [
        "Intellectual and emotional satisfaction of solving tough operational problems in healthcare while improving patient access and saving lives!",
        "Competitive compensation package that includes base salary, target bonus, and stock options",
        "401(k) Match",
        "Comprehensive healthcare benefits",
        "Generous Paid Time Off and Parental Leave",
        "Monthly reimbursement for Skill Building",
        "Monthly reimbursement for Wellness, Transportation, and/or Home Office",
        "Education Reimbursement for select courses/programs"
      ],
      "Responsibilities": [
        "Data is what powers our platform and empowers our users",
        "This is a hands-on role that requires skills in engineering, data wrangling, and good old-fashioned customer charm to engage both technical and clinical staff at hospitals",
        "Manage, write, and maintain hospital data ingestion SQL queries and configuration in our custom system",
        "Work with hospital IT teams to obtain data extracts and ensure timely integrations",
        "Manage data warehouse fidelity, ensuring stakeholders trust the data and understand both its opportunities and limitations",
        "Ensure client data feeds are operational and error-free",
        "Oversee data specifications, technical designs, and customer-specific glossaries",
        "Work with backend engineering and product to set technical direction for the data ingestion pipeline for performance, scale, and reliability",
        "Assist sales team with pre-sales technical requirements and phased onboarding plans"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "B4AEepGQz_M1JeavAAAAAA==",
    "job_title": "Lead Data Engineer (Data Management & Governance)",
    "employer_name": "Temus",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSWmUxen1MzTSEKoVuhFFBQ735VNM9TOJwZEDKD&s=0",
    "employer_website": "https://temus.com",
    "job_publisher": "LinkedIn Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.linkedin.com/jobs/view/lead-data-engineer-data-management-governance-at-temus-4268385166?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/lead-data-engineer-data-management-governance-at-temus-4268385166?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Temus was established by Temasek in partnership with UST, to provide digital transformation solutions for the private and public sectors as we aspire to be a strategic partner in realising the Singapore Government\u2019s Smart Nation vision. We are headquartered in Singapore and have more than 400 employees across a wide range of disciplines in strategy, design, architecture, technology, data & AI.\n\nYour objectives\n\u2022 Solve complex data management challenges and deliver measurable business value through governance transformation\n\u2022 Lead data maturity assessments and develop transformation roadmaps for enterprise clients across various industries\n\u2022 Guide clients in establishing and scaling data analytics teams, from organizational design to capability building\n\u2022 Design and implement comprehensive data governance frameworks, policies, and operating models aligned with business objectives\n\u2022 Build and deploy metadata management solutions, data catalogues, and lineage tracking systems\n\u2022 Facilitate executive workshops and present data strategy recommendations to C-suite stakeholders\n\u2022 Drive adoption of DAMA-DMBOK principles and industry best practices across client organizations\n\u2022 Lead pre-sales engagements, solution architecture, and technical due diligence for data governance initiatives\n\u2022 Mentor clients' data teams on governance practices, data quality management, and stewardship models\n\nYour background\n\u2022 Bachelor's or master's degree in science, technology, engineering, mathematics, or information management\n\u2022 Deep expertise in data governance frameworks, with hands-on implementation experience using tools like Collibra, Alation, Informatica, Purview, or DataHub\n\u2022 Strong practical knowledge of DAMA-DMBOK framework and ability to adapt it to different organizational contexts\n\u2022 Proven track record conducting data environment assessments and maturity evaluations for large enterprises\n\u2022 Hands-on experience implementing metadata management, data lineage, and data cataloguing solutions at scale\n\u2022 Technical proficiency with data profiling, data quality tools, and master data management platforms\n\u2022 Experience with at least one major data platforms (Databricks, Snowflake, AWS, Azure, DataFabric) and understanding of their governance capabilities\n\u2022 Working knowledge of SQL, Python, or similar for data analysis and quality validation\n\u2022 Strong experience designing data operating models and organizational structures for analytics teams\n\u2022 8+ years in data management consulting with direct client engagement and delivery accountability\n\u2022 Demonstrated ability to influence and advise senior executives on data strategy and governance investments\n\u2022 Excellence in creating and delivering executive presentations, turning technical concepts into business value stories\n\u2022 Experience managing complex stakeholder environments and driving consensus across business and IT teams\n\u2022 Track record of hands-on governance tool implementation, not just strategy documentation\n\u2022 Active engagement in data governance communities, certifications (CDMP, DCAM), and industry forums\n\nTemus is an equal opportunities employer. We welcome applications from all. We do not discriminate by race, religion, belief, ethnicity, origin, disability, age, partnership status, sexual orientation, or gender identity.\n\nWe see the diversity of our team as a strategic advantage, and we work actively to maintain it.\n\nBy applying for this role, you have read and acknowledge the data privacy statement via this link - temus.com/job-applicant-data-protection/",
    "job_is_remote": false,
    "job_posted_at": "5 hours ago",
    "job_posted_at_timestamp": 1753146000,
    "job_posted_at_datetime_utc": "2025-07-22T01:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DB4AEepGQz_M1JeavAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "QI7t3m021DbcFcjSAAAAAA==",
    "job_title": "Data Engineer - QuantumBlack",
    "employer_name": "McKinsey & Company",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSUIfYLPh-6LOWj-Pqf3Q-XIK5A01sXJfIjQjHK&s=0",
    "employer_website": "http://www.mckinsey.com/",
    "job_publisher": "McKinsey",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.mckinsey.com/careers/search-jobs/jobs/dataengineer-quantumblack-94882?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "McKinsey",
        "apply_link": "https://www.mckinsey.com/careers/search-jobs/jobs/dataengineer-quantumblack-94882?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://id.indeed.com/viewjob?jk=4bdbf65a3c0c050c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://id.linkedin.com/jobs/view/data-engineer-contract-based-at-pt-astra-international-tbk-4118012549?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.mx/job-listing/data-engineer-quantumblack-mckinsey-company-JV_IC2709872_KO0,26_KE27,43.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com.br/job-listing/data-engineer-quantumblack-mckinsey-company-JV_IC2709872_KO0,26_KE27,43.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-quantumblack-mckinsey-and-company-JV_IC2709872_KO0,26_KE27,47.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.es/job-listing/data-engineer-quantumblack-mckinsey-company-JV_IC2709872_KO0,26_KE27,43.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.co.nz/job-listing/data-engineer-quantumblack-mckinsey-company-JV_IC2709872_KO0,26_KE27,43.htm?jl=1009643089494&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Your Growth Driving lasting impact and building long-term capabilities with our clients is not easy work. You are the kind of person who thrives in a high performance/high reward culture - doing hard things, picking yourself up when you stumble, and having the resilience to try another way forward.\n\nIn return for your drive, determination, and curiosity, we'll provide the resources, mentorship, and opportunities you need to become a stronger leader faster than you ever thought possible. Your colleagues\u2014at all levels\u2014will invest deeply in your development, just as much as they invest in delivering exceptional results for clients. Every day, you'll receive apprenticeship, coaching, and exposure that will accelerate your growth in ways you won\u2019t find anywhere else.\n\nWhen you join us, you will have:\n\u2022 Continuous learning: Our learning and apprenticeship culture, backed by structured programs, is all about helping you grow while creating an environment where feedback is clear, actionable, and focused on your development. The real magic happens when you take the input from others to heart and embrace the fast-paced learning experience, owning your journey.\n\u2022 A voice that matters: From day one, we value your ideas and contributions. You\u2019ll make a tangible impact by offering innovative ideas and practical solutions. We not only encourage diverse perspectives, but they are critical in driving us toward the best possible outcomes.\n\u2022 Global community: With colleagues across 65+ countries and over 100 different nationalities, our firm\u2019s diversity fuels creativity and helps us come up with the best solutions for our clients. Plus, you\u2019ll have the opportunity to learn from exceptional colleagues with diverse backgrounds and experiences.\n\u2022 World-class benefits: On top of a competitive salary (based on your location, experience, and skills), we provide a comprehensive benefits package to enable holistic well-being for you and your family.\n\nYour ImpactYou will work on real-world, high-impact projects across a variety of industries. You will have the opportunity to collaborate with QB/Labs teams and build complex and innovative ML systems to accelerate our work in AI and help solve business problems at speed and scale.\n\nYou will experience the best environment to grow as a technologist and a leader. You will develop a sought-after perspective connecting technology and business value by working on real-life problems across a variety of industries and technical challenges to serve our clients on their changing needs.\n\nYou will be surrounded by inspiring individuals as part of diverse and multidisciplinary teams. You will develop a holistic perspective of AI by partnering with the best design, technical, and business talent in the world as your team members.\n\nWhile we advocate for using the right tech for the right task, we often leverage the following technologies: Python, PySpark, the PyData stack, SQL, Airflow, Databricks, our own open-source data pipelining framework called Kedro, Dask/RAPIDS, container technologies such as Docker and Kubernetes, cloud solutions such as AWS, GCP, and Azure, and more.\n\nAs a Data Engineer, you will:\n\u2022 Contribute to cross-functional problem-solving sessions with your team and our clients, from data owners and users to C-level executives, to address their needs and build impactful analytics solutions\n\u2022 Have the opportunity to contribute to R&D projects and internal asset development\n\u2022 Design and build GenAI applications (RAG, Agentic AI, etc) collaboratively with data scientists\n\u2022 Map data fields to hypotheses and curate, wrangle, and prepare data for use in advanced analytics models\n\u2022 Apply knowledge about clients data landscape and assess data quality\n\u2022 Create and manage data environments and ensure information security standards are maintained at all times\n\u2022 Design and build data pipelines for machine learning that are robust, modular, scalable, deployable, reproducible, and versioned\n\u2022 Help to build and maintain the technical platform for advanced analytics engagements, spanning data science and data engineering work\n\nYou will be part of our global Data Engineering community and you will work in cross-functional Agile project teams alongside Data Scientists, Machine Learning Engineers, other Data Engineers, Project Managers, and industry experts.\n\nYou will work hand-in-hand with our clients, from data owners, users, and fellow engineers to C-level executives.\n\nWho you are: You are a highly collaborative individual who wants to solve problems that drive business value. You have a strong sense of ownership and enjoy hands-on technical work. Our values resonate with yours.\n\nYour qualifications and skills\n\u2022 Degree in computer science, engineering, mathematics, or equivalent experience\u202f\u202f\n\u2022 2+ years of relevant professional experience\n\u2022 Ability to write clean, maintainable, scalable and robust code in an object-oriented language, e.g., Python, Scala, Java, in a professional setting\n\u2022 Proven experience building data pipelines in production for advanced analytics use cases\n\u2022 Experience working across structured, semi-structured and unstructured data\n\u2022 Exposure to software engineering concepts and best practices, inc. DevOps, DataOps and MLOps would be considered a plus\n\u2022 Familiarity with distributed computing frameworks, cloud platforms, containerization, and analytics libraries (e.g. pandas, numpy, matplotlib)\n\u2022 Experienced on Big Data platforms and tools like AWS, Azure, GCP and tools like Spark, Kafka, Snowflake, GCS Data Proc, Azure DataFactory, AWS Glue, Apache Beam/Flink, Python/PySpark, etc.\n\u2022 Commercial client-facing or senior stakeholder management experience would be beneficial",
    "job_is_remote": false,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DQI7t3m021DbcFcjSAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "TwOHvv0eFL6EkyyFAAAAAA==",
    "job_title": "Associate, Full Stack Data Engineer (Singapore) | Singapore, SG",
    "employer_name": "Nomura Asia",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8qc3mifOLjmnwm2DhBL84ZGuD5CBg82M9gYn4&s=0",
    "employer_website": "https://www.nomuraholdings.com",
    "job_publisher": "EFinancialCareers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.efinancialcareers.sg/jobs-Singapore-Singapore-Associate_Full_Stack_Data_Engineer_Singapore.id22996447?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "EFinancialCareers",
        "apply_link": "https://www.efinancialcareers.sg/jobs-Singapore-Singapore-Associate_Full_Stack_Data_Engineer_Singapore.id22996447?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Job title: Full Stack Data Engineer\nCorporate Title: Associate\nDepartment: Chief Data Office\nLocation: Singapore\nCompany overview\nNomura is an Asia-based financial services group with an integrated global network spanning over 30 countries. By connecting markets East & West, Nomura services the needs of individuals, institutions, corporates and governments through its three business divisions: Retail, Asset Management, and Wholesale (Global Markets and Investment Banking). Founded in 1925, the firm is built on a tradition of disciplined entrepreneurship, serving clients with creative solutions and considered thought leadership. For further information about Nomura, visit www.nomura.com\nDepartment overview:\nThe Chief Data Office plays a key role in defining and implementing the firm's data, cloud and AI strategy, driving change through these capabilities, enforcing data, cloud and AI governance for the firm, and elevating Nomura's data culture. Governance remains a critical focus area, and the Chief Data Office, in partnership with Business and Corporate functions, is responsible for ensuring that the firm's data assets are managed in line with the firm's data management framework, policy and standards.\nRole Description:\nJob Responsibilities:\n\u2022 Information delivery & analytics. State-of-the-art expertise across, data/information preparation, data insight & visualization using BI (or similar tools), and advanced data prediction using AI, ML, DL, etc.\n\u2022 AI/ML Ops. Responsible for integration, deployment and monitoring of AI/ML products and solutions,\n\u2022 Data management. Demonstrate expertise in data management to ensure the analytics products are appropriate/ethical and well-controlled. Enabling data architecture and delivery of data-analytics platforms and Solutions- on-premises, cloud, and hybrid ensuring adherence and conformance to Nomura standards and policies\n\u2022 Be a trusted partner. Shape the information & analytics agenda at Nomura, and work with all of Nomura's businesses in laying out their information & analytics adoption roadmaps.\n\u2022 Risk Mindset: Familiar with risk and controls frameworks and ability to operate with a control mindset\nSkills, experience, qualifications and knowledge required:\nCore Skills requirement:\n\u2022 Designing and developing scalable data pipelines to collect and process large volumes of data from multiple sources.\n\u2022 Building physical data models and ETL processes to ensure data quality, integrity, and accessibility.\n\u2022 Microservices Development: Building and maintaining highly scalable and fault tolerant microservice, including efficient server-side APIs.\n\u2022 Deployment: Hands on with CI/CD, Jenkins, Ansible, DevOps process, Enterprise integration patterns.\n\u2022 Hands-on with programming languages (Python, SQL, Java, Unix scripting etc.) and with orchestration tools like Airflow or Autosys\n\u2022 Experience with cloud technologies such as EC2, EMR, Snowflake or similar tools with ability to drive design and data model discussions, hybrid data architecture.\n\u2022 Proficiency in React with hands-on experience in UI development a plus.\n\u2022 Ability to understand and integrate cultural differences and work effectively with virtual cross-cultural, cross-border teams.\n\u2022 Flexibility to adjust to multiple demands, shifting priorities, ambiguity, and rapid change.\n\u2022 Experience with senior stakeholder management will be an added advantage.\n\u2022 Excellent communication (verbal, written, listening), presentation, and interpersonal skills.\n\u2022 Able to analyze complex situations and derive workable actions.\n\u2022 Able to constructively challenge requirements and current state to increase overall value to the firm.\nEducation and experience\nWide variety of degrees will be considered, however work experience will be of equal, if not greater importance\n\u2022 At least 4-year Bachelor's degree in quantitative fields with minimum of 5 years of relevant data experience in data engineering / MLOps, full stack engineering, preferably in financial organizations or Masters in quantitative fields (Computer Science, Statistics or similar)\n\u2022 Experience of working with a multi-cultural, multi-disciplined, globally dispersed teams\n\u2022 Certifications in relevant technologies or frameworks are a plus.\nDiversity Statement\nNomura is committed to an employment policy of equal opportunities, and is fundamentally opposed to any less favourable treatment accorded to existing or potential members of staff on the grounds of race, creed, colour, nationality, disability, marital status, pregnancy, gender or sexual orientation.\n\nDISCLAIMER: This Job Description is for reference only, and whilst this is intended to be an accurate reflection of the current job, it is not necessarily an exhaustive list of all responsibilities, duties, skills, efforts, requirements or working conditions associated with the job. The management reserves the right to revise the job and may, at his or her discretion, assign or reassign duties and responsibilities to this job at any time.\nNomura is an Equal Opportunity Employer",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DTwOHvv0eFL6EkyyFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113300",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "vgTNNr_ZkBb3Of6BAAAAAA==",
    "job_title": "Senior Software Engineer (Data Engineer)",
    "employer_name": "EPAM Systems",
    "employer_logo": null,
    "employer_website": "http://www.epam.com/",
    "job_publisher": "EPAM",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.epam.com/careers/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "EPAM",
        "apply_link": "https://www.epam.com/careers/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EPAM",
        "apply_link": "https://careers.epam.ua/vacancies/job-listings/job-detail.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EPAM",
        "apply_link": "https://careers.epam-poland.pl/careers/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "MyCareersFuture",
        "apply_link": "https://www.mycareersfuture.gov.sg/job/information-technology/senior-software-engineer-epam-systems-4fcdb9d5bd3be6f260b5f750d4eb993b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "EPAM",
        "apply_link": "https://careers.epam.hu/careers/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Careers.epam.cn",
        "apply_link": "https://careers.epam.cn/job-listings/job.epamgdo_blt642bc7aaf10d988b_en-us_Singapore_Singapore.senior-software-engineer-data-engineer_singapore_singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit",
        "apply_link": "https://www.foundit.sg/job/senior-software-engineer-data-engineer-epam-systems-pte-ltd-singapore-35437150?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/senior-software-engineer-data-engineer-at-epam-systems-4258451865?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We are seeking a skilled Data Engineer/Reporting Analyst to analyze the organization\u2019s data needs, design efficient data pipelines, and develop reporting solutions. You will migrate data collection to optimized channels, implement ETL processes (Python, PySpark, Informatica), and build dashboards (Tableau/SAP BO). The role involves collaborating with technical teams to deploy solutions, ensuring system reliability, and supporting production issues. Experience in Agile/Waterfall methodologies, SQL, big data (Cloudera/Hive), and DevOps (Denodo) is essential.\n\nRESPONSIBILITIES\n\u2022 Analyse the Authority\u2019s data needs and document the requirements\n\u2022 Refine data collection/consumption by migrating data collection to more efficient channels\n\u2022 Plan, design and implement data engineering jobs and reporting solutions to meet the analytical needs\n\u2022 Develop a test plan and scripts for system testing, and support user acceptance testing\n\u2022 Build reports and dashboards according to user requirements\n\u2022 Work with the Authority\u2019s technical teams to ensure smooth deployment and adoption of the new solution\n\u2022 Ensure the smooth operations and service level of IT solutions\n\u2022 Support production issues\n\nREQUIREMENTS\n\u2022 Minimum 5 years of experience\n\u2022 Good understanding and completion of projects using Waterfall/Agile methodologies\n\u2022 Strong SQL, data modeling, and data analysis skills\n\u2022 Hands-on experience in big data engineering jobs using Python, PySpark, Linux, and ETL tools like Informatica\n\u2022 Hands-on experience in a reporting or visualization tool like SAP BO and Tableau\n\u2022 Good understanding of analytics and data warehouse implementations\n\u2022 Ability to troubleshoot complex issues ranging from system resources to application stack traces\n\u2022 Passion for automation, standardization, and best practices\n\u2022 No visa sponsorship is available\n\nNICE TO HAVE\n\u2022 Hands-on experience in DevOps deployment and data virtualization tools like Denodo\n\u2022 Track record in implementing systems using Hive, Impala, and Cloudera Data Platform\n\u2022 Track record in implementing systems with high availability, high performance, and high security hosted at various data centres or hybrid cloud environments\n\nWE OFFER\n\u2022 By choosing EPAM, you're getting a job at one of the most loved workplaces according to Newsweek 2021 & 2022&2023\n\u2022 Employee ideas are the main driver of our business. We have a very supportive environment where your voice matters\n\u2022 You will be challenged while working side-by-side with the best talent globally. We work with top-notch technologies, constantly seeking new industry trends and best practices\n\u2022 We offer a transparent career path and an individual roadmap to engineer your future & accelerate your journey\n\u2022 At EPAM, you can find vast opportunities for self-development: online courses and libraries, mentoring programs, partial grants of certification, and experience exchange with colleagues around the world. You will learn, contribute, and grow with us",
    "job_is_remote": false,
    "job_posted_at": "25 days ago",
    "job_posted_at_timestamp": 1750982400,
    "job_posted_at_datetime_utc": "2025-06-27T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DvgTNNr_ZkBb3Of6BAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "57oJKe0o8V6CkydyAAAAAA==",
    "job_title": "Data Engineer - Growth",
    "employer_name": "TikTok",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRy6v5l1pXh-OB9BurAw49fhjT5XQCS6iZ_lQSp&s=0",
    "employer_website": "https://www.tiktok.com",
    "job_publisher": "LinkedIn Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-growth-at-tiktok-4268168199?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-growth-at-tiktok-4268168199?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/data-engineer-growth-jobs/985E7896AA93B761?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Responsibilities\nThe Growth team plays a core role in acquisition, activation, and retention of billions of users, through our globally popular products such as TikTok, Lemon8, etc. We are building platform foundations, leveraging data and ML models, and providing end-to-end solutions to power global growth of products.\n\nYou will:\n- Build data pipelines to portray business status, based on a deep understanding of our fast changing business and data-driven approach;\n- Extract information and signals from a broad range of data and build hierarchies to accomplish analytical and mining goals for \u201cPackaged Business Capability\u201d such as user-growth, gaming and searching;\n- Keep improving the integrity of data pipelines to provide a comprehensive data service.\n\nQualifications\nMinimum Qualifications\uff1a\n- Bachelor's degree in Computer Science, Statistic, Data Science or a related field;\n- Skilled in SQL and additional object-oriented programming language (e.g. Scala, Java, or Python);\n- Experience in issue tracking and problem solving on data pipelines;\n- Fast business understanding and collaborative in teamwork.\n\nPreferred Qualification\uff1a\n- Industry experience working with user growth.\n\nAbout TikTok\nTikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.\n\nWhy Join Us\nInspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.\nWe strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.\n\nDiversity & Inclusion\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D57oJKe0o8V6CkydyAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "t7K78VJEBsgBxWmFAAAAAA==",
    "job_title": "Data Engineer (Oracle) - SPVL",
    "employer_name": "SCIENTEC CONSULTING PTE. LTD.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLhvpEIhKaA5svWSjfxinCVsbo5G8NVHjrer3Y&s=0",
    "employer_website": null,
    "job_publisher": "MyCareersFuture",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-spvl-scientec-consulting-329e54c1ffef9899bd51a5491cf921ab?source=MCF&event=Search&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "MyCareersFuture",
        "apply_link": "https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-spvl-scientec-consulting-329e54c1ffef9899bd51a5491cf921ab?source=MCF&event=Search&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/singapore/job/full-time/technology/data-engineer-oracle-spvl-145489038?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs In Singapore. New Jobs, New Recruitment And Fastest 2025",
        "apply_link": "https://jobsingapore247.com/fr/data-engineer-seekbetter-urgent-job282470?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs In Singapore. Find Jobs Now, Find New Jobs, Recruit New Jobs 2025",
        "apply_link": "https://singaporejob24h.com/ja/data-engineer-integrated-e-services-job17460?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We are looking for Data Engineer to join our team of analytics experts, you'll be responsible for expanding and optimising data and data pipeline architecture, as well as optimising data flow and collect for cross-function teams\n\u2022 Exciting opportunity to join one of the fast growing IT company\n\u2022 Permanent position, positive working environment\n\u2022 Salary up to $7,000 + AWS + Bonuses\n\nResponsibilities\n\u2022 Develop, construct, test and maintain data architectures such as databases, data warehouses and large-scale data processing systems\n\u2022 Design and develop data pipelines/systems for data modelling, mining and production\n\u2022 Ensure the data architecture is in place to support routine and ad-hoc requirements of data analytics team, stakeholders and the business\n\u2022 Leverage on variety of programming languages and data crawling/processing tools to make raw data clean and highly available for use in descriptive and predictive modelling\n\u2022 Recommend and implement ways to improve data quality, reliability, flexibility and efficiency\n\u2022 Ensure data assets and data catalogs are organized and stored in an efficient way so that information is easy to access and retrieve\n\u2022 PL/SQL and SQL Tuning and optimization of newly develop and existing applications\n\nRequirements\n\u2022 At least 3 years' working experience in data architecture, data warehousing, data processing, data modelling and ETL/ELT, familiarity with real-time streaming solutions.\n\u2022 Working experience in Kubernetes-based DevOps practices, with experience in container orchestration, CI/CD pipelines, and microservices deployment.\n\u2022 Working experience in database development (Oracle SQL/PLSQL)\n\u2022 Working experience in AWS cloud environment, familiar with solutions such as EC2, S3, EMR, Redshift, Athena, Kinesis\n\u2022 Programming knowledge in Python, R, SQL for data cleaning, processing and aggregation\n\nBy submitting your resume, you consent to the collection, use, and disclosure of your personal information per ScienTec\u2019s Privacy Policy (scientecconsulting.com/privacy-policy).\n\nThis authorizes us to:\n\nContact you about potential opportunities.\n\nDelete personal data not required at this application stage.\n\nTo withdraw consent, email dpo@scientecconsulting.com.\n\nAll applications will be processed with strict confidence. Only shortlisted candidates will be contacted.\n\nLiew Chien Hui - R2090138\n\nScienTec Consulting Pte Ltd \u2013 11C5781",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dt7K78VJEBsgBxWmFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "MONTH",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "pItNW5ktrvI-6J5vAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "Thoughtworks",
    "employer_logo": null,
    "employer_website": "http://www.thoughtworks.com/",
    "job_publisher": "Thoughtworks",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.thoughtworks.com/careers/jobs/7007052?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Thoughtworks",
        "apply_link": "https://www.thoughtworks.com/careers/jobs/7007052?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://sg.indeed.com/viewjob?jk=64f93427a708fccc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/lead-data-engineer/6183183?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.sg/job-listing/lead-data-engineer-kyndryl-JV_IC3235921_KO0,18_KE19,26.htm?jl=1009716968816&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs By Workable",
        "apply_link": "https://jobs.workable.com/view/dzHg4JhGANf315FGXJNAcB/data-engineer-lead-in-singapore-at-unison-consulting-pte-ltd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Built In Singapore",
        "apply_link": "https://builtinsingapore.com/job/data-engineer-lead/6542350?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Smart Recruiters Jobs",
        "apply_link": "https://jobs.smartrecruiters.com/Grab/744000030786095?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/lead-data-engineer-at-temus-4268071512?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Lead data engineers at Thoughtworks develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. They might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On projects, they will be leading the design of technical solutions, or perhaps overseeing a program inception to build a new product. Alongside hands-on coding, they are leading the team to implement the solution.",
    "job_is_remote": false,
    "job_posted_at": "15 days ago",
    "job_posted_at_timestamp": 1751846400,
    "job_posted_at_datetime_utc": "2025-07-07T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DpItNW5ktrvI-6J5vAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "HGgV0-Xe7pagLjspAAAAAA==",
    "job_title": "Data Engineer, GovTech Anti Scam Products (GASP)",
    "employer_name": "Government Technology Agency",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRRGxo7lK-qWdmiMqiHEA6h9VBkBdV75ldgjk1i&s=0",
    "employer_website": "http://www.tech.gov.sg/",
    "job_publisher": "Careers - Careers@Gov",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.careers.gov.sg/jobs/hrp/16207454/992bab65-4819-1fe0-98d0-4bc31378c24e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers - Careers@Gov",
        "apply_link": "https://jobs.careers.gov.sg/jobs/hrp/16207454/992bab65-4819-1fe0-98d0-4bc31378c24e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-govtech-anti-scam-products-gasp-at-govtech-singapore-4255673317?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit",
        "apply_link": "https://www.foundit.sg/job/data-engineer-govtech-anti-scam-products-gasp-gvt-government-technology-agency-singapore-35393196?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs Trabajo.org",
        "apply_link": "https://sg.trabajo.org/job-3820-e0125065d3bfad0721cf63192b9f3acc?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "The BIG Jobsite | Singapore",
        "apply_link": "https://sg.thebigjobsite.com/details/C6BDC3C05A6A1BA7408F3A1F8A19D1F8/data-engineer--govtech-anti-scam-products--gasp?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "What the role is:\n\nThe Government Technology Agency (GovTech) is the lead agency driving Singapore\u2019s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government\u2019s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.\n\nAt GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.\n\nGovTech aims to transform the delivery of Government digital services by taking an \"outside-in\" view, putting citizens and businesses at the heart of everything we do.\n\nPlay a part in Singapore\u2019s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!\n\nLearn more about GovTech at tech.gov.sg.\n\nWhat you will be working on:\n\nThe GovTech Anti Scam Products Team\u2019s mission is to build tech solutions to detect, disrupt and deter scammers at scale. The work spans data infrastructure, developing and deploying AI & ML to scale proactive detection, API connections to partners, as well as front-end systems for enforcement agencies.\n\nThe Data Engineer will be solving abstract problems to tackling scams using data:\u202f\n\u2022 Design and implement ETL processes\n\u2022 Manage data warehousing solutions\n\u2022 Expose and deploy machine learning models to production\n\u2022 Ensure data quality and consistency across various sources\n\u2022 Collaborate with data scientists and software engineers to develop anti-scam tech products\n\n\u202f\n\nMore senior engineers will also be expected to:\n\u2022 Establish best practices for developer operations\n\u2022 Provide technical leadership across multiple product teams\n\u2022 Share your expertise and mentor other engineers\n\u2022 Help with recruiting\n\nWhat we are looking for:\n\u2022 Proficient in Python\u202f\n\u2022 Experience with SQL/NoSQL/Graph databases\n\u2022 Glue, Presto, Apache Spark and/or Hive\n\u2022 Experience working with CI/CD setups\n\u2022 Strong knowledge of algorithms and database structures\n\u2022 Strong knowledge of database integration and migration strategy\n\u2022 Strong knowledge in designing and implementing scalable data infrastructure\n\nPreferred\n\u2022 Experience in AWS Cloud\n\u2022 Experience building high scale, low latency APIs & deploying scaled AI / ML solutions\n\u2022 Experience with data intensive frameworks & data warehousing solutions, bonus if you\u2019re familiar with graph data structures\n\u2022 Understanding of event driven architectures involving SQS / kafka, bonus if you\u2019re familiar with Flink!\n\nWe're here to improve how we live as a society through what we can offer as a government. Because our team focuses on pushing new initiatives, you will also have to:\n\u2022 Identify potential projects that improve the public good\n\u2022 Design novel systems that work around bureaucratic constraints\n\u2022 Advocate and explain these technical ideas to other government agencies\n\n\u202f\n\nYou're not just here to write code, but also to figure out what we should be building and how we should build it. You will work on meaningful projects to reduce scams and make a big impact on people\u2019s lives.\u202f\n\nOur employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. These include leave benefits to meet your work-life needs and employee wellness programmes.\n\nWe champion flexible work arrangements (subject to your job role) and trust that you will manage your own time to deliver your best, wherever you are, and whatever works best for you.\n\nLearn more about life inside GovTech at go.gov.sg/GovTechCareers.\n\nStay connected with us on social media at go.gov.sg/ConnectWithGovTech.\n\nAbout Government Technology Agency\nThe Government Technology Agency (GovTech) is the lead agency driving Singapore\u2019s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government\u2019s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.\n\nAt GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.\n\nGovTech aims to transform the delivery of Government digital services by taking an \"outside-in\" view, putting citizens and businesses at the heart of everything we do.\n\nPlay a part in Singapore\u2019s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!\n\nLearn more about GovTech at tech.gov.sg.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DHGgV0-Xe7pagLjspAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "-Zlw5zo4rsFh8aCFAAAAAA==",
    "job_title": "Data Engineer (must have Oracle)",
    "employer_name": "Randstad Singapore",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQnGi2f4uoxgnabfk515UOSB5L5BbLT5Hq0qaii&s=0",
    "employer_website": "https://www.randstad.com",
    "job_publisher": "Randstad Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.randstad.com.sg/jobs/data-engineer-must-have-oracle_singapore_45548391/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Randstad Singapore",
        "apply_link": "https://www.randstad.com.sg/jobs/data-engineer-must-have-oracle_singapore_45548391/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Randstad",
        "apply_link": "https://www.randstad.com/jobs/data-engineer-must-have-oracle_singapore_45548391/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-must-have-oracle-at-randstad-singapore-4266486378?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "MyCareersFuture",
        "apply_link": "https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-randstad-78c115d0db514360f3809728030ca440?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobsDB",
        "apply_link": "https://sg.jobsdb.com/job/Data-Engineer-fdb205247cd4241273fd6349777c4431?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://en-sg.whatjobs.com/jobs/database-management?id=56832899&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "about company\n\nI am currently working with a software consultancy firm that provides premium software development and strategy expertise to a wide spectrum of clients. Projects are about Gaming/Gambling/Blockchain/Government Project.\n\nSalary structure: Base + AWS + variable bonus. 2 rounds of interview process. Hybrid working arrangement. Office at CBD.\n\nabout job\n\n\u2022 Develop, construct, test and maintain data architectures such as databases, data warehouses and large-scale data processing systems\n\n\u2022 Design and develop data pipelines/systems for data modelling, mining and production\n\n\u2022 Make raw data clean and highly available for use in descriptive and predictive modelling\n\n\u2022 Recommend and implement ways to improve data quality, reliability, flexibility and efficiency\n\n\u2022 Ensure data assets and data catalogs are organized and stored in an efficient way\n\n\u2022 PL/SQL and SQL Tuning and optimization\n\nskills and requirements\n\n\u2022 Min 3 years in data architecture, data warehousing, data processing, data modelling and ETL/ELT, familiarity with real-time streaming solutions\n\n\u2022 Experience in Kubernetes-based DevOps practices, with experience in container orchestration, CI/CD pipelines, and microservices deployment.\n\n\u2022 Experience in database development (Oracle SQL/PLSQL)\n\n\u2022 Working experience in AWS cloud environment, familiar with solutions such as EC2, S3, EMR, Redshift, Athena, Kinesis \u2022 Programming knowledge in Python, R, SQL for data cleaning, processing and aggregation\n\n\u2022 Mandarin speaking required as you need to liaise with Chinese counterparts who can only speak and write in Mandarin\n\nTo apply online please use the 'apply' function, alternatively you may contact Stella at 96554170 (EA: 94C3609 /R1875382)\n\nskills\n\nno additional skills required\n\nqualifications\n\nno additional qualifications required\n\neducation\n\nBachelor Degree",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D-Zlw5zo4rsFh8aCFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "MONTH",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "kvYQr9pAcrfhoccgAAAAAA==",
    "job_title": "Data Engineer - ETL (MOH ITDG)",
    "employer_name": "Synapxe",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS6wrGguMBLMKMhimRHQSuMlVSQ_Tnt-UxIEYMK&s=0",
    "employer_website": "http://www.synapxe.sg/",
    "job_publisher": "HealthTech Career | Tech Jobs At Synapxe And Public Healthcare - Synapxe",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers-public-healthtech-jobs.synapxe.sg/job/Data-Engineer-ETL-%28MOH-ITDG%29/43267444/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "HealthTech Career | Tech Jobs At Synapxe And Public Healthcare - Synapxe",
        "apply_link": "https://careers-public-healthtech-jobs.synapxe.sg/job/Data-Engineer-ETL-%28MOH-ITDG%29/43267444/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-etl-moh-itdg-at-synapxe-4267780519?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/singapore/job/part-time/technology/etl-data-engineer-part-time-140913043?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Overview\n\nRole & Responsibilities\n\nDevelop TRUST data strategy:\n\u2022 Work with stakeholders to understand data analytics needs, data structure requirements (both in terms of scalability and accessibility), and translate this into a coherent near to long term data strategy for TRUST\n\u2022 Support translation of data business needs into technical system requirements for MCDR, in terms of collection, storage, batch -time processing, as well as analysis of information from structured and unstructured sources in a scalable, repeatable, and secure manner\n\u2022 Identify opportunities for improvements and optimisation e.g., Implement best practices and performance optimization on Big Data and Cloud to achieve the best data engineering outcomes\n\nOversee data preparation and data provisioning for TRUST:\n\u2022 Collaborate with data engineers to organise and prepare anonymised datasets in MCDR according to TRUST standards, and then providing the data in accordance with the approved TRUST Data Request. This involves working with the data engineers closely to ensure that the datasets meet the required standards and are made available as per the specific data request guidelines set by TRUST\n\u2022 Oversee implementation of common data model and data quality programme in TRUST and MCDR\n\u2022 Work with data analysts, data scientists, clinicians and other stakeholders to implement common data models to support analytics use cases\n\u2022 Design and implement tools to enhance the data strategy and enable seamless integration with the data, potentially leveraging API calls for efficient integration\n\u2022 Implement data management standards and practices\n\nRequirements\n\u2022 Degree/master\u2019s in computer science, Information Technology, Computer Engineering or equivalent\n\u2022 At least ten (10) years of relevant working experience in Data management / Integration / Modelling the data warehouse or advanced analytics solutions\n\u2022 Demonstrate good, in-depth knowledge in relevant Extract-Transform-Load (ETL) hardware/software products, frameworks, and methodologies\n\u2022 Experience in designing and implementing cloud-based data solutions using cloud platforms (e.g., AWS cloud native tools)\n\u2022 Databases (e.g., Oracle, MS SQL, MySQL, Teradata)\n\u2022 Big data (e.g., Hadoop ecosystem)\n\u2022 ETL development using ETL tools (e.g., Informatica, IBM DataStage, Talend)\n\u2022 Data repository design (e.g., operational data stores, dimensional data stores, data marts)\n\u2022 Experience in interacting with analytics stakeholders (economists, statisticians, clinicians, policy makers) on a business or domain level\n\u2022 Comfortable working independently to carry out data analysis, estimate data quality and sufficiency\n\u2022 Good interpersonal skills, a detail-oriented & flexible person who can work across different areas within the team\n\u2022 The following will be preferred: Some understanding of Singapore Healthcare System and healthcare data governance, management; and/or familiarity with health informatics\n\nApply Now\n\nNOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!\n\n#LI-SYNX40",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DkvYQr9pAcrfhoccgAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113300",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "XYhI4KEG-YaTLDBoAAAAAA==",
    "job_title": "AWS Data Engineer",
    "employer_name": "Unison Consulting",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRtm-sKot-t1syBrDfxcPuMiIeomIbiENUIrvGd&s=0",
    "employer_website": "http://www.unison-ucg.com/",
    "job_publisher": "LinkedIn Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.linkedin.com/jobs/view/aws-data-engineer-at-unison-consulting-4269382253?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/aws-data-engineer-at-unison-consulting-4269382253?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jooble",
        "apply_link": "https://sg.jooble.org/jdp/7638434842567790994?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs Trabajo.org",
        "apply_link": "https://sg.trabajo.org/job-3088-4c3dfbe626aba6cbd1a938ec263b05ad?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Hirewand",
        "apply_link": "https://www.hirewand.com/apply/job/detail?jid=1709102sid%3D5af414cf6ed33ca5454ecbdb&src=jobpost&cpid=1709&uid=88869&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://en-sg.whatjobs.com/jobs/data-management?id=56864579&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/singapore/singapore/info_technology/4383965951/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "\u2022 Design and build scalable ETL pipelines and data integration workflows using AWS services and Python\n\u2022 Develop and optimize data lake and data warehouse solutions for structured and unstructured data\n\u2022 Leverage Apache Spark for large-scale data processing and transformation tasks\n\u2022 Collaborate with cross-functional teams to gather requirements and deliver clean, usable datasets for analytics and reporting\n\u2022 Ensure high data quality, security, and compliance in all stages of the data lifecycle\n\nRequirements\n\u2022 5+ years of hands-on experience in data engineering roles\n\u2022 Proficiency in Python for building and automating data pipelines\n\u2022 Strong experience with AWS services (S3, Glue, Redshift, Lambda, etc.)\n\u2022 Solid understanding of ETL processes and modern data warehousing concepts\n\u2022 Experience with big data tools, especially Apache Spark (PySpark preferred)\n\u2022 Familiarity with DevOps and CI/CD practices for data pipeline deployment\n\u2022 Knowledge of data governance and cataloging tools\n\u2022 Strong problem-solving, communication, and collaboration skills",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXYhI4KEG-YaTLDBoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "zlIHUsDVBSxg5HpJAAAAAA==",
    "job_title": "Sr. Data Engineer",
    "employer_name": "VISA WORLDWIDE PTE. LIMITED",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRJtTS8GbNWfNwL_Gn6oVsFvHRNmzA8-HnlhIC-&s=0",
    "employer_website": "http://www.visa.com.sg/",
    "job_publisher": "Jobstreet",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.jobstreet.com/job/85840488?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobstreet",
        "apply_link": "https://sg.jobstreet.com/job/85840488?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido.com",
        "apply_link": "https://sg.jobrapido.com/jobpreview/182079774099767296?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Expertini",
        "apply_link": "https://sg.expertini.com/jobs/job/sr-data-engineer-singapore-r-systems-singapore-pte-limited-2502-13920920/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Company Description\n\nVisa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose \u2013 to uplift everyone, everywhere by being the best way to pay and be paid.\n\nMake an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n\nJob Description\n\nVisa\u2019s Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world\u2019s most sophisticated processing networks, capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. You\u2019ll work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cybersecurity, and B2C platforms.\n\nIn addition, Value Added Services (VAS) - VAS Digital Marketing is a key growth strategy for Visa globally, aimed at diversifying Visa\u2019s revenue with products and solutions that differentiate its network and deliver valuable solutions across other networks.\n\nThe Opportunity:\n\nWe are developing and executing a shared strategic vision for Digital Marketing platforms and products that enable Visa to be the world-leading data-driven payments company. As a Senior Data Engineer, you will be part of a world-class team of Engineers to define, drive and execute on this vision. We are looking for a self-motivated, versatile and energetic individual with software engineering skills and expertise with Java, Big data & Web technologies, who embraces solving complex challenges on a global scale. The candidate will be extensively involved in hands-on activities including POCs, design, development, testing, and managing applications globally used by Visa cardholders. Candidate must be flexible and willing to switch tasks based on team's needs.\n\nYou will use your Java skills and experience with various technologies to design, develop, test, and deploy high-quality code that meets stringent business, security, and resiliency requirements. You will collaborate with other teams, vendors, and stakeholders to ensure the smooth delivery and operation of the application. You will have the opportunity to learn and apply new technologies and frameworks, such as AI and generative AI, to enhance the functionality and performance of the application.\n\nPrimary responsibilities will include:\n\u2022 Design, develop, test, document, and implement new applications and enhance existing systems to ensure high performance and reliability.\n\u2022 Write secure, maintainable, and efficient code that adheres to Java/J2EE best practices, organizational and security standards.\n\u2022 Create and maintain comprehensive technical documentation, including design changes and architectural decisions, using Wiki or similar tools.\n\u2022 Participate in code and design review sessions to ensure high-quality deliverables and adherence to development standards.\n\u2022 Collaborate with architects, product owners, and technical stakeholders to deliver products that meet business requirements and leverage modern technologies.\n\u2022 Identify and recommend opportunities for process improvements, enhancements, and adoption of best practices within the development team.\n\u2022 Mentor and support junior developers, fostering knowledge sharing and contributing to the development of departmental procedures and standards.\n\u2022 Coordinate and contribute to Continuous Integration (CI) activities and the implementation of automated testing frameworks.\n\u2022 Develop proof-of-concepts (POCs) and prototypes to validate ideas and quickly iterate new features or enhancements.\n\u2022 Communicate technical solutions, project status, issues, and risks effectively to both technical and non-technical stakeholders.\n\u2022 Ensure the delivery of high-quality, defect-free code and take accountability for meeting project timelines and quality standards.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\nQualifications\n\nPreferred Qualifications\n\n\u20223 or more years of work experience with a Bachelor\u2019s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\n\n\u20224\u20137 years of relevant experience in Java/J2EE enterprise applications.\n\n\u2022Strong skills in Core Java, J2EE, Spring Framework, Spring Boot, Hibernate, and Web services.\n\n\u2022Proficiency in object-oriented design and software design principles.\n\n\u2022Experience with secure coding practices.\n\n\u2022Strong SQL skills with experience in relational (MySQL, PostgreSQL) and NoSQL (MongoDB) databases.\n\n\u2022Understanding of data warehousing concepts and tools.\n\n\u2022Exposure to data engineering frameworks such as Apache Spark, Hadoop, or Kafka is an advantage.\n\n\u2022Basic understanding of ETL processes and data pipeline development.\n\n\u2022Hands-on experience with containerization and orchestration tools (Docker, Kubernetes).\n\n\u2022Proficiency in version control systems (Git/Stash), build tools (Maven), and CI/CD tools (Jenkins).\n\n\u2022Familiarity with Unix/Linux operating systems and shell scripting.\n\n\u2022Experience with UI frameworks and frontend development using Angular or React, Next.js, JavaScript, HTML, and CSS.\n\n\u2022AI and generative AI skills are highly desirable.\n\n\u2022Experience working in all phases of the software development life cycle.\n\n\u2022Experience with Agile methodologies (Scrum, sprints) and tools (Jira).\n\n\u2022Understanding of DevOps practices.\n\n\u2022Solid foundation in computer science, including data structures and algorithms.\n\n\u2022Willingness to learn and improve coding skills, especially in Java or Scala.\n\nAdditional Information:\n\nSkills/Abilities\n\n\u2022Strong analytical and problem-solving abilities.\n\n\u2022Quick to learn and adapt to new technologies and challenges.\n\n\u2022Excellent organizational skills with the ability to manage multiple tasks and deadlines in a fast-paced environment.\n\n\u2022Outstanding written and verbal communication skills for conveying ideas and implementation plans to team members and stakeholders.\n\n\u2022Highly detail-oriented, resourceful, and results-driven.\n\n\u2022Self-motivated with a demonstrated ability to work independently and meet commitments.\n\n\u2022Comfortable collaborating in dynamic, fast-paced, and highly interactive team settings.\n\n\u2022Eager to learn new skills, embrace new initiatives, and contribute to team success.\n\n\u2022Proven ability to maintain a positive attitude and have fun while working as part of a team.\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DzlIHUsDVBSxg5HpJAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "KnUoVvpG0KDQSXvFAAAAAA==",
    "job_title": "Data Analytics Engineer",
    "employer_name": "Singapore University of Social Sciences",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQYbAGGB4EnwoSsDTGPHKFHfcPAlMMRCMBIhiKy&s=0",
    "employer_website": "https://www.suss.edu.sg/",
    "job_publisher": "SUSS Careers - Singapore University Of Social Sciences",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.suss.edu.sg/job/Data-Analytics-Engineer/1210933501/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "SUSS Careers - Singapore University Of Social Sciences",
        "apply_link": "https://careers.suss.edu.sg/job/Data-Analytics-Engineer/1210933501/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-analytics-engineer-at-beyondsoft-singapore-4266123569?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://sg.indeed.com/viewjob?jk=b85491c606f34afb&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.sg/job-listing/analytics-data-engineer-formfactor-inc-JV_IC3235921_KO0,23_KE24,38.htm?jl=1009732423046&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "MyCareersFuture",
        "apply_link": "https://www.mycareersfuture.gov.sg/job/others/data-analytics-engineer-singapore-university-social-sciences-e4c2cec82d761500b885fbf054d9092f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobsDB",
        "apply_link": "https://sg.jobsdb.com/job/Data-Analytics-Engineer-adfda264e934ee08a963cce9350f57e5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs Trabajo.org",
        "apply_link": "https://sg.trabajo.org/job-3183-deabe87f78e6271727d34a103eb80dbd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/data-analytics-engineer-jobs/FEC45B3F217BB4E7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Who We Are\n\nAs Singapore's first institute for lifelong learning, the Singapore University of Social Sciences (SUSS) champions inclusivity to bring education to all and ensure that they are given equal opportunities to develop to their fullest potential in our diverse learning environment.\n\nWe advocate for the same for our people. We believe everyone should have equal opportunities and develop to their fullest potential in their careers.\n\nEmbark on an exciting lifelong journey with us in making a positive difference in your career and serving our society.\n\nFor more information on Singapore University of Social Sciences, please visit www.suss.edu.sg\n\nAbout The Job\n\nWe are seeking a versatile and motivated individual to join our team as a Data Analytics Engineer (Entry Level). In this role, you will build and maintain scalable data pipelines, develop and deploy machine learning (ML) models, and support the university\u2019s data-driven initiatives using tools such as Microsoft Fabric, Azure Machine Learning, and Power BI. In addition to data and ML development, you will also serve as an administrator of Microsoft Fabric, the university\u2019s cloud analytics platform\u2014ensuring efficient operations, governance, and user support. This position is ideal for individuals who enjoy working across the full data and ML lifecycle\u2014from wrangling raw datasets to training models and supporting strategic decisions through analytics. You will be part of a collaborative, in-house data team addressing meaningful challenges in education and operations.\n\nWhat You Will Be Doing\n\nKey Responsibilities:\n\nData Engineering & Machine Learning Development\n\u2022 Design, build, and maintain scalable data pipelines using Microsoft Fabric components (e.g., Data Pipelines, Dataflows) and PySpark.\n\u2022 Ingest, clean, and transform data from diverse sources such as APIs, flat files, and databases.\n\u2022 Operate within the Lakehouse architecture to ensure reliable, well-governed data delivery.\n\u2022 Design, train, and evaluate models for classification, prediction, and segmentation use cases.\n\u2022 Perform feature engineering, model tuning, and validation.\n\u2022 Deploy models in collaboration with analytics specialists and contribute to building reusable ML feature stores.\n\u2022 Ensure data quality, documentation, and lineage are maintained across workflows.\n\nMicrosoft Fabric Administration & Analytics Support\n\u2022 Act as the primary administrator for Microsoft Fabric, overseeing workspace, capacity, and permission management.\n\u2022 Monitor usage, performance, and cost metrics; troubleshoot and optimise resource allocation.\n\u2022 Support onboarding of university teams to Microsoft Fabric and maintain security and compliance standards.\n\u2022 Manage workspace hygiene, archival, and governance enforcement across all Fabric artefacts.\n\u2022 Collaborate with the data warehouse team, analytics specialists, AI engineers, and domain users to deliver structured datasets and model outputs.\n\u2022 Assist in building and refining Power BI datasets and dashboards for decision-making support.\n\nJob Requirements\n\u2022 Possess a degree in Data Science, Business Analytics or related.\n\u2022 Proficiency in SQL and Python, including libraries such as Pandas, Scikit-learn, and PySpark.\n\u2022 Strong interest or hands-on experience in end-to-end ML development (from data prep to deployment).\n\u2022 Familiarity with Microsoft Fabric components (e.g., Lakehouse, Data Pipelines, Dataflows).\n\u2022 Exposure to Power BI or equivalent BI/visualisation tools.\n\u2022 Microsoft Fabric certification (or willingness to obtain one).\n\u2022 Exposure to Azure Machine Learning or similar ML model lifecycle platforms.\n\u2022 Practical experience (via internships, coursework, or projects) in data engineering or ML model development.\n\u2022 Understanding of higher education data or student analytics scenarios.\n\u2022 Curious and self-driven with a growth mindset.\n\u2022 Able to clearly explain technical concepts to both technical and non-technical audiences.\n\u2022 Strong attention to detail and a methodical, structured approach to solving problems.\n\nWhat We Offer\n\nAt SUSS, we advocate the Spirit of Learning and pride ourselves as lifelong learners. You will gain access to various learning platforms and plenty of development opportunities to support your growth in a meaningful career!\n\nBesides that, you will also get:\n\u2022 Competitive Pay Package\n\u2022 Hybrid Work Arrangement (Subject to Job Role)\n\u2022 Medical Benefits\n\u2022 Flex Benefits\n\u2022 Family Care Leaves\n\u2022 Volunteer Service Leaves\n\u2022 Wellness & Recreation Activities\n\u2022 Lifelong Learning Opportunities\n\u2022 Career Development Opportunities through Internal Job Postings and Transfers",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1752796800,
    "job_posted_at_datetime_utc": "2025-07-18T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DKnUoVvpG0KDQSXvFAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "39PqMEyLu4LplxOoAAAAAA==",
    "job_title": "Senior Data Engineer / Data Engineer (Department of Data Science, SGH)",
    "employer_name": "SingHealth",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR38hwyYNc7mfIBQYqofPZttiNCTHStvjzLAS9F&s=0",
    "employer_website": "http://www.singhealth.com.sg/",
    "job_publisher": "SingHealth Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.singhealth.com.sg/SGH/job/Singapore-Senior-Data-Engineer-Data-Engineer-%28Department-of-Data-Science%2C-SGH%29-Sing/12413844/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "SingHealth Careers",
        "apply_link": "https://careers.singhealth.com.sg/SGH/job/Singapore-Senior-Data-Engineer-Data-Engineer-%28Department-of-Data-Science%2C-SGH%29-Sing/12413844/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/senior-data-engineer-data-engineer-department-of-data-science-sgh-singapore-sing-singapore-general-hospital-885?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Description\n\nYou will support the design, implementation and maintenance of data flow channels and data processing systems that support the collection, storage, batch and real-time processing, and analysis of information in a scalable, repeatable and secure manner. You will focus on defining optimal solutions to data collection, processing and warehousing. You will also focus on collecting, parsing, managing, analysing and visualising large sets of data to turn information into insights accessible through multiple platforms. You will design, code and test data systems and work on implementing them into the internal infrastructure, improving data reliability and quality. Most of all, you enjoy reducing complexity and building automated systems.\n\nJob Requirements\n\u2022 Bachelor\u2019s Degree in Computer Science, Electronics or Electrical Engineering, Software Engineering, Information Technology or other related technical disciplines.\n\u2022 Preferably 1-3 (Analyst) or 3-5 (Senior Analyst) years of relevant experience.\n\u2022 Prior experience in a healthcare setting would be an advantage but is not required.\n\u2022 Fluency in command-line shell scripting, Python and/or R.\n\u2022 Experience in several of the following skill clusters:\n\u2022 Relational databases, data marts and data warehouses \u2013 experience with Oracle and/or PostgreSQL platforms; crafting secure and effective SQL queries\n\u2022 NoSQL databases \u2013 key-value, document, graph, column family\n\u2022 Reproducible computing \u2013 version control, software packaging, continuous integration/continuous delivery, process monitoring and containerization\n\u2022 Automated workflows - programmatically author, schedule and monitor workflows using Apache Airflow, Azure Logic Apps or similar\n\u2022 Distributed computing \u2013 distribute jobs for concurrent processing using Apache Spark, Databricks or similar\n\u2022 Streaming data \u2013 process continuous, serial port or micro-batch online data\n\u2022 Ability to work independently as well as in a team.\n\u2022 Knowledge in cloud technologies is preferable.\n\u2022 Strong analytical and problem-solving skills.\n\u2022 Excellent written and verbal communication skills.",
    "job_is_remote": false,
    "job_posted_at": "24 days ago",
    "job_posted_at_timestamp": 1751068800,
    "job_posted_at_datetime_utc": "2025-06-28T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D39PqMEyLu4LplxOoAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5"
  },
  {
    "job_id": "zGqjgzVmtKX6bdYgAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "AvePoint",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRabXGv07cqAupbSEdKtdFTYhyb2BiRYJMfHRm4&s=0",
    "employer_website": "http://www.avepoint.com/",
    "job_publisher": "AvePoint",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.avepoint.com/careers/job-detail?gh_jid=6339041&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "AvePoint",
        "apply_link": "https://www.avepoint.com/careers/job-detail?gh_jid=6339041&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Michael Page Singapore",
        "apply_link": "https://www.michaelpage.com.sg/job-detail/data-engineer/ref/jn-052025-6743618?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "NTT DATA Careers",
        "apply_link": "https://careers.services.global.ntt/global/en/job/R-130902/Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "HealthTech Career | Tech Jobs At Synapxe And Public Healthcare - Synapxe",
        "apply_link": "https://careers-public-healthtech-jobs.synapxe.sg/job/Data-Engineer/43219844/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://sg.indeed.com/viewjob?jk=a0630cf4119b0205&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-at-sedha-consulting-4259967590?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Careers@Gov Singapore",
        "apply_link": "https://jobs.careers.gov.sg/jobs/hrp/11782951/7bb1ff5a-635b-1eef-88e0-0f7a49dc2163?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.sg/job-listing/data-engineer-sembcorp-industries-JV_IC3235921_KO0,13_KE14,33.htm?jl=1009798924099&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Securing the Future with AvePoint\n\nAvePoint is a global leader in data management and governance, trusted by over 21,000 customers worldwide to enhance their digital workplaces across Microsoft, Google, Salesforce, and other collaboration platforms. Our global channel partner program includes more than 3,500 managed service providers, value-added resellers, and systems integrators, with our solutions featured in over 100 cloud marketplaces. To learn more, visit www.avepoint.com.\n\nAt AvePoint, we are dedicated to investing in our people. Our culture, driven by agility, passion, and teamwork, empowers you to shape your career, make a significant impact, and take ownership of your future. Discover how you can unleash your potential with us!\n\nJob Summary\nWe are seeking a skilled Data Engineer to join our dynamic team in the Enterprise Knowledge Department, where you will play a pivotal role in transforming how we work with structured and unstructured data. You will be responsible for developing, enhancing, and maintaining IT systems and applications that support our data management needs.\n\nKey Responsibilities\n\u2022 Analyze data needs and document technical requirements.\n\u2022 Migrate data collection processes to more efficient channels.\n\u2022 Plan, design, and implement data engineering jobs and reporting solutions to meet analytics needs.\n\u2022 Develop test plans and scripts for system testing and support user acceptance testing.\n\u2022 Collaborate with technical teams to ensure smooth deployment and adoption of new solutions.\n\u2022 Ensure the smooth operation and performance of IT solutions, including addressing production issues.\n\nWhat We Are Looking For\n\u2022 Strong understanding of waterfall/Agile methodologies.\n\u2022 Hands-on experience with DevOps deployment and data virtualization tools such as Denodo (preferred).\n\u2022 Proficiency with reporting or visualization tools like SAP BO and Tableau.\n\u2022 Experience working with Hive, Impala, and Cloudera Data Platform is preferred.\n\u2022 Expertise in big data engineering, using Python, Pyspark, Linux, and ETL tools like Informatica.\n\u2022 Strong SQL skills and experience in data modeling and analysis.\n\u2022 Knowledge of analytics and data warehouse implementations.\n\u2022 Ability to troubleshoot complex technical issues.\n\u2022 Experience in high-availability, high-performance systems hosted in data centers or hybrid cloud environments is a plus.\n\nAny personal data you share with us during the application process will be processed strictly in compliance with applicable data protection laws and our Privacy Notice.\n\nAny personal data you share with us during the application process will be processed strictly in compliance with applicable data protection laws and our Privacy Notice.",
    "job_is_remote": false,
    "job_posted_at": "25 days ago",
    "job_posted_at_timestamp": 1750982400,
    "job_posted_at_datetime_utc": "2025-06-27T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DzGqjgzVmtKX6bdYgAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "IVWl75IB-SomxwyMAAAAAA==",
    "job_title": "Data Engineer - Global E-Commerce (Governance Service - Security Platform)",
    "employer_name": "TikTok",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRy6v5l1pXh-OB9BurAw49fhjT5XQCS6iZ_lQSp&s=0",
    "employer_website": "https://www.tiktok.com",
    "job_publisher": "LinkedIn Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-global-e-commerce-governance-service-security-platform-at-tiktok-4267155889?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-global-e-commerce-governance-service-security-platform-at-tiktok-4267155889?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/data-engineer-global-e-commerce-jobs/05B7D51C0309AE17?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs In Singapore. Find Jobs Now, Find New Jobs, Recruit New Jobs 2025",
        "apply_link": "https://singaporejob24h.com/fr/data-security-expert-1084-job121374?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Responsibilities\nGlobal E-Commerce is a content e-commerce business with international short video product as the carrier. It is committed to becoming the first choice for users to discover and purchase good products with affordable prices. Global E-Commerce business team hopes to provide users with more tailored and efficient consumption experience, enabling merchants to receive reliable platform services in different scenarios such as live e-commerce, short video content e-commerce, thereby making more affordable and high-quality products easily accessible and improving lives.\n\nThe Global E-Commerce's Governance and Experience is a global team responsible for ensuring a safe and trustworthy marketplace not only for our buyers but also for our sellers and creators. We continually work on areas such as risk detection abilities, fairness, and sustainability of the E-Commerce ecosystem, content and commodity quality, and friction-free experiences to drive improvement.\n\nResponsibilities:\n- Responsible for the offline data warehouse construction of global e-commerce security, including data layering, model design, ETL process, etc.\n- Establish data warehouse standards for global e-commerce security to ensure data accuracy and consistency.\n- Collaborate with the security team in optimising their offline tasks to ensure system stability and improving resource efficiency.\n- Assist in identifying risk indicators through data analysis to support the development of key security projects.\n- Support real-time data processing pipelines, including tagging systems and real-time ETL infrastructure.\n- Visualise, interpret, and report data findings and may create dynamic data reports as well.\n\nQualifications\nMinimum Qualifications:\n- Bachelor's or higher degree in Computer Science, Information Technology, Programming & System Analysis, Science (Computer Studies) or related discipline.\n- Candidates should have at least 5 years of experience in big data ecosystem development, familiar with technologies such as Spark, Flink, Clickhouse, Hadoop, and practical experience with Lambda/Kappa architectures.\n- Proficient in data warehouse implementation methodologies, with a deep understanding of data warehouse systems, and experience supporting real-world business scenarios.\n- Experience in SQL performance tuning, with an understanding of Hive SQL development.\n\nPreferred Qualifications:\n- Candidates with deep experience in real-time data warehouse construction.\n- Data-sensitive with strong business understanding, excellent logical reasoning skills, and some data analysis capabilities.\n\nAbout TikTok\nTikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.\n\nWhy Join Us\nInspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.\nWe strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.\n\nDiversity & Inclusion\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1752537600,
    "job_posted_at_datetime_utc": "2025-07-15T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DIVWl75IB-SomxwyMAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "13119900",
    "job_onet_job_zone": "3"
  },
  {
    "job_id": "4YUXjAIp5thZRvbmAAAAAA==",
    "job_title": "Software & Data Engineering Lead",
    "employer_name": "Adecco - GS Perm",
    "employer_logo": null,
    "employer_website": "https://www.adecco.com.sg",
    "job_publisher": "Adecco Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.adecco.com.sg/jobview/software-and-data-engineering-lead/67b40a83-5ca6-4ff0-a1d9-b61eac5192c6/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Adecco Singapore",
        "apply_link": "https://www.adecco.com.sg/jobview/software-and-data-engineering-lead/67b40a83-5ca6-4ff0-a1d9-b61eac5192c6/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobstreet",
        "apply_link": "https://sg.jobstreet.com/job/85889779?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "BeBee SG",
        "apply_link": "https://sg.bebee.com/job/85e2fc2c19da3799b76142c4df63e1f3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Our client is a next-generation innovation center established by a leading global automotive group, committed to transforming the future of mobility. Based in Singapore, this state-of-the-art facility brings together advanced manufacturing, R&D, and customer experience under one roof.\n\nServing as a testbed for smart factory innovation, AI-powered vehicle production, and digital twin technology, the center supports the shift toward electrification and autonomous mobility. It's designed to be fully connected, human-centered, and sustainable-uniting cross-functional teams in AI, robotics, data science, and engineering to reimagine the mobility ecosystem.\n\nAs a core pillar of the group's global innovation strategy, the center pioneers smart urban mobility solutions and sets new standards in intelligent manufacturing and customer-centric innovation.\n\nWe're hiring a senior tech leader to drive smart factory innovation for a global automotive group. You'll lead a team building and deploying intelligent systems-IoT, AI, MLOps, and simulations-that improve productivity, logistics, and manufacturing outcomes in a software-defined factory.\n\nWhat You'll Do:\n\u2022 Design and scale IoT and AI systems using cloud (AWS, Azure, GCP) and on-prem solutions\n\u2022 Architect smart solutions using platforms like Thingworx, NVIDIA Omniverse, and MLOps tools\n\u2022 Build enterprise data architecture (data lakes, warehouses) with strong governance and security\n\u2022 Lead cross-functional engineering teams and align with digital transformation goals\n\u2022 Oversee project delivery and continuous improvement\n\nWhat You'll Bring:\n\u2022 13+ years in IT/OT systems and data engineering\n\u2022 Strong in Python, SQL, Java, C++, cloud platforms, and modern dev tools (Docker, CI/CD)\n\u2022 Experience with SCADA, communication protocols, and data pipelines\n\u2022 Strong leadership, architecture, and project management skills\n\nWilson Tay\nDirect Line: 6697 7866\nEA License No: 91C2918\nPersonnel Registration Number: R2091205",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1752969600,
    "job_posted_at_datetime_utc": "2025-07-20T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4YUXjAIp5thZRvbmAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "MONTH",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "HGgV0-Xe7pagLjspAAAAAA==",
    "job_title": "Data Engineer, GovTech Anti Scam Products (GASP)",
    "employer_name": "Government Technology Agency",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRRGxo7lK-qWdmiMqiHEA6h9VBkBdV75ldgjk1i&s=0",
    "employer_website": "http://www.tech.gov.sg/",
    "job_publisher": "Careers - Careers@Gov",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.careers.gov.sg/jobs/hrp/16207454/992bab65-4819-1fe0-98d0-4bc31378c24e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers - Careers@Gov",
        "apply_link": "https://jobs.careers.gov.sg/jobs/hrp/16207454/992bab65-4819-1fe0-98d0-4bc31378c24e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn Singapore",
        "apply_link": "https://sg.linkedin.com/jobs/view/data-engineer-govtech-anti-scam-products-gasp-at-govtech-singapore-4255673317?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit",
        "apply_link": "https://www.foundit.sg/job/data-engineer-govtech-anti-scam-products-gasp-gvt-government-technology-agency-singapore-35393196?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs Trabajo.org",
        "apply_link": "https://sg.trabajo.org/job-3820-e0125065d3bfad0721cf63192b9f3acc?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "What the role is:\n\nThe Government Technology Agency (GovTech) is the lead agency driving Singapore\u2019s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government\u2019s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.\n\nAt GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.\n\nGovTech aims to transform the delivery of Government digital services by taking an \"outside-in\" view, putting citizens and businesses at the heart of everything we do.\n\nPlay a part in Singapore\u2019s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!\n\nLearn more about GovTech at tech.gov.sg.\n\nWhat you will be working on:\n\nThe GovTech Anti Scam Products Team\u2019s mission is to build tech solutions to detect, disrupt and deter scammers at scale. The work spans data infrastructure, developing and deploying AI & ML to scale proactive detection, API connections to partners, as well as front-end systems for enforcement agencies.\n\nThe Data Engineer will be solving abstract problems to tackling scams using data:\u202f\n\u2022 Design and implement ETL processes\n\u2022 Manage data warehousing solutions\n\u2022 Expose and deploy machine learning models to production\n\u2022 Ensure data quality and consistency across various sources\n\u2022 Collaborate with data scientists and software engineers to develop anti-scam tech products\n\n\u202f\n\nMore senior engineers will also be expected to:\n\u2022 Establish best practices for developer operations\n\u2022 Provide technical leadership across multiple product teams\n\u2022 Share your expertise and mentor other engineers\n\u2022 Help with recruiting\n\nWhat we are looking for:\n\u2022 Proficient in Python\u202f\n\u2022 Experience with SQL/NoSQL/Graph databases\n\u2022 Glue, Presto, Apache Spark and/or Hive\n\u2022 Experience working with CI/CD setups\n\u2022 Strong knowledge of algorithms and database structures\n\u2022 Strong knowledge of database integration and migration strategy\n\u2022 Strong knowledge in designing and implementing scalable data infrastructure\n\nPreferred\n\u2022 Experience in AWS Cloud\n\u2022 Experience building high scale, low latency APIs & deploying scaled AI / ML solutions\n\u2022 Experience with data intensive frameworks & data warehousing solutions, bonus if you\u2019re familiar with graph data structures\n\u2022 Understanding of event driven architectures involving SQS / kafka, bonus if you\u2019re familiar with Flink!\n\nWe're here to improve how we live as a society through what we can offer as a government. Because our team focuses on pushing new initiatives, you will also have to:\n\u2022 Identify potential projects that improve the public good\n\u2022 Design novel systems that work around bureaucratic constraints\n\u2022 Advocate and explain these technical ideas to other government agencies\n\n\u202f\n\nYou're not just here to write code, but also to figure out what we should be building and how we should build it. You will work on meaningful projects to reduce scams and make a big impact on people\u2019s lives.\u202f\n\nOur employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. These include leave benefits to meet your work-life needs and employee wellness programmes.\n\nWe champion flexible work arrangements (subject to your job role) and trust that you will manage your own time to deliver your best, wherever you are, and whatever works best for you.\n\nLearn more about life inside GovTech at go.gov.sg/GovTechCareers.\n\nStay connected with us on social media at go.gov.sg/ConnectWithGovTech.\n\nAbout Government Technology Agency\nThe Government Technology Agency (GovTech) is the lead agency driving Singapore\u2019s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government\u2019s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.\n\nAt GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.\n\nGovTech aims to transform the delivery of Government digital services by taking an \"outside-in\" view, putting citizens and businesses at the heart of everything we do.\n\nPlay a part in Singapore\u2019s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!\n\nLearn more about GovTech at tech.gov.sg.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1752710400,
    "job_posted_at_datetime_utc": "2025-07-17T00:00:00.000Z",
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DHGgV0-Xe7pagLjspAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  },
  {
    "job_id": "AHxtw_lLwXgyY9JDAAAAAA==",
    "job_title": "Data Engineer (Hedge fund, high frequency trading)",
    "employer_name": "Hays",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTfMkY6IKILC8qcBX26ZLYdVoWz9udEV5cAAk3i&s=0",
    "employer_website": "http://www.hays.com/",
    "job_publisher": "Hays Singapore",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.hays.com.sg/job-detail/JOB_5141039?jobSource=HaysGCJ&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Hays Singapore",
        "apply_link": "https://www.hays.com.sg/job-detail/JOB_5141039?jobSource=HaysGCJ&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Foundit",
        "apply_link": "https://www.foundit.sg/job/data-engineer-hedge-fund-high-frequency-trading-hays-specialist-recruitment-pte-ltd-singapore-35390866?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GrabJobs",
        "apply_link": "https://grabjobs.co/singapore/job/full-time/technology/data-engineer-hedge-fund-high-frequency-trading-142017890?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Your new company\nA Chinese hedge fund/ high frequency trading firm.\n\nYour new role\n\u2022 Data Acquisition & Integration \u2013 Identify and evaluate both traditional and alternative data sources for firm-wide use while developing robust ETL/ELT pipelines and data warehouses to efficiently manage large-scale datasets.\n\u2022 Research & Strategy Development \u2013 Collaborate closely with investment teams to transform data into actionable insights, supporting quantitative research, factor modelling, portfolio construction, and strategic decision-making.\n\u2022 Data Governance & Accuracy \u2013 Oversee automated data collection and cleansing processes, ensuring accuracy and reliability while maintaining symbol mapping libraries across exchanges, time zones, and asset classes.\n\u2022 Market & Trading Infrastructure Alignment \u2013 Manage global equity datasets from various vendors, track corporate actions, align data with trading venues and broker conventions, and collaborate with PMs, researchers, and engineers to fulfil data requirements\n\nWhat you'll need to succeed\n\u2022 Advanced proficiency in C++ and Python (Pandas, NumPy), along with experience in working with large datasets and live data streaming\n\u2022 Demonstrated success in maintaining the high availability, accuracy, and reliability of data infrastructure within a trading or financial environment\n\u2022 3+ years in data engineering or quantitative data operations, preferably at a hedge fund, proprietary trading firm, or asset manager, with deep familiarity across global equity markets (North America, Europe, Asia-Pacific).\n\u2022 Expertise in security identifiers (ISIN, CUSIP, SEDOL, RIC, Bloomberg Ticker) and prime brokerage data mapping.\n\nWhat you'll get in return\nIn return, you'll be part of an organisation that values its employees. You\u2019ll be rewarded with:\n\u2022 Structured career growth and plenty of developmental opportunities\n\u2022 Stable working environment\n\nWhat you need to do now\nIf you're interested in this role, click 'apply now' or for more information and a confidential discussion on this role or to find out about more opportunities in Technology, contact Yuki Cheung or email yuki.cheung@hays.com.sg. Referrals are welcome.\n\nAt Hays, we value diversity and are passionate about placing people in a role where they can flourish and succeed. We actively encourage people from diverse backgrounds to apply.\n#1276780 - Yuki Cheung",
    "job_is_remote": false,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DAHxtw_lLwXgyY9JDAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "41303100",
    "job_onet_job_zone": "3"
  },
  {
    "job_id": "X1CmXWpmy4SCVfOMAAAAAA==",
    "job_title": "Data Engineer, Product Analytics",
    "employer_name": "Meta",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSrsPV50auya_P5N4t5W3jXXiFwdOQQhbhzNzhn&s=0",
    "employer_website": "http://meta.com/",
    "job_publisher": "Indeed",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://sg.indeed.com/viewjob?jk=e34108a87b33c47a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Indeed",
        "apply_link": "https://sg.indeed.com/viewjob?jk=e34108a87b33c47a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.sg/job-listing/data-engineer-product-analytics-meta-JV_KO0,31_KE32,36.htm?jl=1009774571369&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Diversity Jobs - AbilityJOBS",
        "apply_link": "https://career.abilityjobs.com/job/data-engineer-product-analytics/78969402/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido.com",
        "apply_link": "https://sg.jobrapido.com/jobpreview/1443336915305627648?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Expertini",
        "apply_link": "https://sg.expertini.com/jobs/job/data-engineer-product-analytics--meta-facebook2-1227712915520896nullsingapore/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "The BIG Jobsite | Singapore",
        "apply_link": "https://sg.thebigjobsite.com/details/E397CD28AF66F3310111941599528458/data-engineer--product-analytics?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Top Jobs Today",
        "apply_link": "https://topjobstoday.com/company/meta/job/data-engineer-product-analytics-singapore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "As a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads). Your technical skills and analytical mindset will be utilized designing and building some of the world's most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide.In this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community.You will be at the forefront of identifying and solving some of the most interesting data challenges at a scale few companies can match. By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond.Data Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions. You will refine our systems, design logging solutions, and create scalable data models. Ensuring data security and quality, and with a focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems.Product leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges. You'll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities.Communication and influence: You won't simply present data, but tell data-driven stories. You will convince and influence your partners using clear insights and recommendations. You will build credibility through structure and clarity, and be a trusted strategic partner.\n\nData Engineer, Product Analytics Responsibilities:\n\u2022 Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way\n\u2022 Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains\n\u2022 Define and manage Service Level Agreements for all data sets in allocated areas of ownership\n\u2022 Solve challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources\n\u2022 Improve logging\n\u2022 Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts\n\u2022 Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts\n\u2022 Influence product and cross-functional teams to identify data opportunities to drive impact\n\nMinimum Qualifications:\n\u2022 Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent\n\u2022 2+ years of experience where the primary responsibility involves working with data. This could include roles such as data analyst, data scientist, data engineer, or similar positions\n\u2022 2+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala or others.)\n\nPreferred Qualifications:\n\u2022 Master's or Ph.D degree in a STEM field\n\nAbout Meta:\n\nMeta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today\u2014beyond the constraints of screens, the limits of distance, and even the rules of physics.\n\nIndividual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about benefits at Meta.",
    "job_is_remote": false,
    "job_posted_at": null,
    "job_posted_at_timestamp": null,
    "job_posted_at_datetime_utc": null,
    "job_location": "Singapore",
    "job_city": null,
    "job_state": null,
    "job_country": "SG",
    "job_latitude": 1.352083,
    "job_longitude": 103.819836,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DX1CmXWpmy4SCVfOMAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4"
  }
]